2024-09-26 19:09:46,512 - INFO - Starting configuration setup
2024-09-26 19:09:46,512 - INFO - Environment variables loaded successfully
2024-09-26 19:09:46,512 - INFO - Twitter API client initialized successfully
2024-09-26 19:09:48,407 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-09-26 19:09:54,420 - INFO - Summarization model initialized successfully
2024-09-26 19:09:54,421 - INFO - Starting tweet scheduling
2024-09-26 19:09:54,421 - INFO - Tweeting process started at 2024-09-26 19:09:54.421095!
2024-09-26 19:09:54,421 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Sep26.csv
2024-09-26 19:09:54,421 - INFO - Fetching latest tech news from RSS feeds
2024-09-26 19:10:09,630 - INFO - Total entries found: 587
2024-09-26 19:10:09,633 - INFO - Recent AI-related entries found: 24
2024-09-26 19:10:09,633 - INFO - Input content - content="Preorders are open for Samsung's new tablets, and they'll be available Oct. 3."
2024-09-26 19:10:09,634 - INFO - clean content - clean_content="Preorders are open for Samsung's new tablets, and they'll be available Oct. 3."
2024-09-26 19:10:09,634 - INFO - Generating summary with BART model
2024-09-26 19:10:15,150 - INFO - Generating summary with BART model
2024-09-26 19:10:19,940 - INFO - full_tweet="Samsung's Galaxy S7 and S7 Edge tablets will go on sale Oct. 3. Preorders are open for the tablets, and..[read moreğŸ‘‡ğŸ¼] #tablets #Preorders https://www.cnet.com/tech/computing/new-samsung-galaxy-s10-tablets-include-ai-features-s-pen/#ftag=CAD590a51e"
2024-09-26 19:10:20,171 - INFO - Tweet posted successfully: Response(data={'text': "Samsung's Galaxy S7 and S7 Edge tablets will go on sale Oct. 3. Preorders are open for the tablets, and..[read moreğŸ‘‡ğŸ¼] #tablets #Preorders https://t.co/YAU6zLNjRN", 'edit_history_tweet_ids': ['1839382010562777373'], 'id': '1839382010562777373'}, includes={}, errors=[], meta={})
2024-09-26 19:10:20,171 - INFO - Sleeping for 8 minutes and 21 seconds.
2024-09-26 19:18:41,172 - INFO - Saved posted URL: https://www.cnet.com/tech/computing/new-samsung-galaxy-s10-tablets-include-ai-features-s-pen/#ftag=CAD590a51e at 2024-09-26 19:18:41
2024-09-26 19:18:41,172 - INFO - Input content - content='<h4>Demystifying NLP: From Text to Embeddings</h4><figure><img alt="Tokenization example generated by Llama-3-8B. Each colored subword represents a distinct token." src="https://cdn-images-1.medium.com/max/1024/1*QVXvydRMEWTWiUP42bYBAg.png" /><figcaption>Tokenization example generated by Llama-3-8B. Each colored subword represents a distinct\xa0token.</figcaption></figure><h3><strong>What is tokenization?</strong></h3><p>In computer science, we refer to human languages, like English and Mandarin, as â€œnaturalâ€ languages. In contrast, languages designed to interact with computers, like Assembly and LISP, are called â€œmachineâ€ languages, following strict syntactic rules that leave little room for interpretation. While computers excel at processing their own highly structured languages, they struggle with the messiness of human language.</p><p>Language\u200aâ€”\u200aespecially text\u200aâ€”\u200amakes up most of our communication and knowledge storage. For example, the internet is mostly text. Large language models like <a href="https://openai.com/chatgpt/">ChatGPT</a>, <a href="https://www.anthropic.com/claude">Claude</a>, and <a href="https://www.llama.com/">Llama </a>are trained on enormous amounts of text\u200aâ€”\u200aessentially all the text available online\u200aâ€”\u200ausing sophisticated computational techniques. However, computers operate on numbers, not words or sentences. So, how do we bridge the gap between human language and machine understanding?</p><p>This is where <strong>Natural Language Processing (NLP)</strong> comes into play. NLP is a field that combines linguistics, computer science, and artificial intelligence to enable computers to understand, interpret, and generate human language. Whether translating text from English to French, summarizing articles, or engaging in conversation, NLP allows machines to produce meaningful outputs from textual\xa0inputs.</p><p>The first critical step in NLP is transforming raw text into a format that computers can work with effectively. This process is known as <strong>tokenization</strong>. Tokenization involves breaking down text into smaller, manageable units called <strong><em>tokens</em></strong>, which can be words, subwords, or even individual characters. Hereâ€™s how the process typically works:</p><ul><li><strong>Standardization:</strong> Before tokenizing, the text is standardized to ensure consistency. This may include converting all letters to lowercase, removing punctuation, and applying other normalization techniques.</li><li><strong>Tokenization:</strong> The standardized text is then split into tokens. For example, the sentence â€œThe quick brown fox jumps over the lazy dogâ€ can be tokenized into\xa0words:</li></ul><pre>[&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumps&quot;, &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dog&quot;]</pre><ul><li><strong>Numerical representation:</strong> Since computers operate on numerical data, each token is converted into a numerical representation. This can be as simple as assigning a unique identifier to each token or as complex as creating multi-dimensional vectors that capture the tokenâ€™s meaning and\xa0context.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*mkvzSPMiX5FZcuQjFe2B6w.png" /><figcaption><em>Illustration inspired by â€œFigure 11.1 From text to vectorsâ€ from</em> <a href="https://www.manning.com/books/deep-learning-with-python-second-edition"><strong>Deep Learning with Python</strong> <em>by FranÃ§ois\xa0Chollet</em></a></figcaption></figure><p>Tokenization is more than just splitting text; itâ€™s about preparing language data in a way that preserves meaning and context for computational models. Different tokenization methods can significantly impact how well a model understands and processes language.</p><p>In this article, we focus on text standardization and tokenization, exploring a few techniques and implementations. Weâ€™ll lay the groundwork for converting text into numerical forms that machines can process\u200aâ€”\u200aa crucial step toward advanced topics like word embeddings and language modeling that weâ€™ll tackle in future articles.</p><h3><strong>Text standardization</strong></h3><p>Consider these two sentences:</p><blockquote><em>1. </em>â€œdusk fell, i was gazing at the Sao Paulo skyline. Isnt urban life vibrant??â€</blockquote><blockquote><em>2. </em>â€œDusk fell; I gazed at the SÃ£o Paulo skyline. Isnâ€™t urban life vibrant?â€</blockquote><p>At first glance, these sentences convey a similar meaning. However, when processed by a computer, especially during tasks like tokenization or encoding, they can appear vastly different due to subtle variations:</p><ul><li><strong>Capitalization:</strong> â€œduskâ€ vs.\xa0â€œDuskâ€</li><li><strong>Punctuation:</strong> Comma vs. semicolon; presence of question\xa0marks</li><li><strong>Contractions:</strong> â€œIsntâ€ vs.\xa0â€œIsnâ€™tâ€</li><li><strong>Spelling and Special Characters:</strong> â€œSao Pauloâ€ vs. â€œSÃ£o\xa0Pauloâ€</li></ul><p>These differences can significantly impact how algorithms interpret the text. For example, â€œIsntâ€ without an apostrophe may not be recognized as the contraction of â€œis notâ€, and special characters like â€œÃ£â€ in â€œSÃ£oâ€ may be misinterpreted or cause encoding\xa0issues.</p><p><strong>Text standardization</strong> is a crucial preprocessing step in NLP that addresses these issues. By standardizing text, we reduce irrelevant variability and ensure that the data fed into models is consistent. This process is a form of feature engineering where we eliminate differences that are not meaningful for the task at\xa0hand.</p><p>A simple method for text standardization includes:</p><ul><li><strong>Converting to lowercase</strong>: Reduces discrepancies due to capitalization.</li><li><strong>Removing punctuation</strong>: Simplifies the text by eliminating punctuation marks.</li><li><strong>Normalizing special characters</strong>: Converts characters like â€œÃ£â€ to their standard forms\xa0(â€œaâ€).</li></ul><p>Applying these steps to our sentences, we\xa0get:</p><blockquote><em>1. </em>â€œdusk fell i was gazing at the sao paulo skyline isnt urban life\xa0vibrantâ€</blockquote><blockquote><em>2. </em>â€œdusk fell i gazed at the sao paulo skyline isnt urban life\xa0vibrantâ€</blockquote><p>Now, the sentences are more uniform, highlighting only the meaningful differences in word choice (e.g., â€œwas gazing atâ€ vs. â€œgazed\xa0atâ€).</p><p>While there are more advanced standardization techniques like <a href="https://en.wikipedia.org/wiki/Stemming"><strong>stemming</strong></a> (reducing words to their root forms) and <a href="https://en.wikipedia.org/wiki/Lemmatization"><strong>lemmatization</strong> </a>(reducing words to their dictionary form), this basic approach effectively minimizes superficial differences.</p><h4><strong>Python implementation of text standardization</strong></h4><p>Hereâ€™s how you can implement basic text standardization in\xa0Python:</p><pre>import re<br />import unicodedata<br /><br />def standardize_text(text:str) -&gt; str:<br />    # Convert text to lowercase<br />    text = text.lower()<br />    # Normalize unicode characters to ASCII<br />    text = unicodedata.normalize(\'NFKD\', text).encode(\'ascii\', \'ignore\').decode(\'utf-8\')<br />    # Remove punctuation<br />    text = re.sub(r\'[^\\w\\s]\', \'\', text)<br />    # Remove extra whitespace<br />    text = re.sub(r\'\\s+\', \' \', text).strip()<br />    return text<br /><br /># Example sentences<br />sentence1 = &quot;dusk fell, i was gazing at the Sao Paulo skyline. Isnt urban life vibrant??&quot;<br />sentence2 = &quot;Dusk fell; I gazed at the SÃ£o Paulo skyline. Isn\'t urban life vibrant?&quot;<br /><br /># Standardize sentences<br />std_sentence1 = standardize_text(sentence1)<br />std_sentence2 = standardize_text(sentence2)<br />print(std_sentence1)<br />print(std_sentence2)</pre><p><strong>Output:</strong></p><pre>dusk fell i was gazing at the sao paulo skyline isnt urban life vibrant<br />dusk fell i gazed at the sao paulo skyline isnt urban life vibrant</pre><p>By standardizing the text, weâ€™ve minimized differences that could confuse a computational model. The model can now focus on the variations between the sentences, such as the difference between â€œwas gazing atâ€ and â€œgazed atâ€, rather than discrepancies like punctuation or capitalization.</p><h3>Tokenization</h3><p>After text standardization, the next critical step in natural language processing is <strong>tokenization</strong>. Tokenization involves breaking down the standardized text into smaller units called <strong><em>tokens</em></strong>. These tokens are the building blocks that models use to understand and generate human language. Tokenization prepares the text for vectorization, where each token is converted into numerical representations that machines can\xa0process.</p><p>We aim to convert sentences into a form that computers can efficiently and effectively handle. There are three common methods for tokenization:</p><h4><strong>1. Word-level tokenization</strong></h4><p>Splits text into individual words based on spaces and punctuation. Itâ€™s the most intuitive way to break down\xa0text.</p><pre>text = &quot;dusk fell i gazed at the sao paulo skyline isnt urban life vibrant&quot;<br />tokens = text.split()<br />print(tokens)</pre><p><strong>Output:</strong></p><pre>[\'dusk\', \'fell\', \'i\', \'gazed\', \'at\', \'the\', \'sao\', \'paulo\', \'skyline\', \'isnt\', \'urban\', \'life\', \'vibrant\']</pre><h4><strong>2. Character-level tokenization</strong></h4><p>Breaks text into individual characters, including letters and sometimes punctuation.</p><pre>text = &quot;Dusk fell&quot;<br />tokens = list(text)<br />print(tokens)</pre><p><strong>Output:</strong></p><pre>[\'D\', \'u\', \'s\', \'k\', \' \', \'f\', \'e\', \'l\', \'l\']</pre><h4><strong>3. Subword tokenization</strong></h4><p>Splits words into smaller, meaningful subword units. This method balances the granularity of character-level tokenization with the semantic richness of word-level tokenization. Algorithms like <strong>Byte-Pair Encoding (BPE)</strong> and <strong>WordPiece</strong> fall under this category. For instance, the <a href="https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> tokenizes â€œI have a new GPU!â€ as\xa0follows:</p><pre>from transformers import BertTokenizer<br /><br />text = &quot;I have a new GPU!&quot;<br />tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)<br />tokens = tokenizer.tokenize(text)<br />print(tokens)</pre><p><strong>Output:</strong></p><pre>[\'i\', \'have\', \'a\', \'new\', \'gp\', \'##u\', \'!\']</pre><p>Here, â€œGPUâ€ is split into â€œgpâ€ and â€œ##uâ€, where â€œ##â€ indicates that â€œuâ€ is a continuation of the previous\xa0subword.</p><p>Subword tokenization offers a balanced approach between vocabulary size and semantic representation. By decomposing rare words into common subwords, it maintains a manageable vocabulary size without sacrificing meaning. Subwords carry semantic information that aids models in understanding context more effectively. This means models can process new or rare words by breaking them down into familiar subwords, increasing their ability to handle a wider range of language\xa0inputs.</p><p>For example, consider the word â€œannoyinglyâ€ which might be rare in a training corpus. It can be decomposed into the subwords â€œannoyingâ€ and â€œlyâ€. Both â€œannoyingâ€ and â€œlyâ€ appear more frequently on their own, and their combined meanings retain the essence of â€œannoyinglyâ€. This approach is especially beneficial in <a href="https://en.wikipedia.org/wiki/Agglutinative_language">agglutinative languages</a> like Turkish, where words can become exceedingly long by stringing together subwords to convey complex meanings.</p><p>Notice that the standardization step is often integrated into the tokenizer itself. Large language models use tokens as both inputs and outputs when processing text. Hereâ€™s a visual representation of tokens generated by Llama-3â€“8B on <a href="https://tiktokenizer.vercel.app/">Tiktokenizer</a>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*h5G4o3Nhh0BzvYBX3SsBtw.png" /><figcaption><strong>Tiktokenizer</strong> example using <strong>Llama-3â€“8B</strong>. Each token is represented by a different color.</figcaption></figure><p>Additionally, Hugging Face provides an excellent <a href="https://huggingface.co/docs/transformers/en/tokenizer_summary">summary of the tokenizers</a> guide, in which I use some of its examples in this\xa0article.</p><p>Letâ€™s now explore how different subword tokenization algorithms work. Note that all of those tokenization algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained\xa0on.</p><h3>Byte-Pair Encoding\xa0(BPE)</h3><p><strong>Byte-Pair Encoding</strong> is a subword tokenization method introduced in <a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a> by Sennrich et al. in 2015. BPE starts with a base vocabulary consisting of all unique characters in the training data and iteratively merges the most frequent pairs of symbols\u200aâ€”\u200awhich can be characters or sequences of characters\u200aâ€”\u200ato form new subwords. This process continues until the vocabulary reaches a predefined size, which is a hyperparameter you choose before training.</p><p>Suppose we have the following words with their frequencies:</p><ul><li>â€œhugâ€ (10 occurrences)</li><li>â€œpugâ€ (5 occurrences)</li><li>â€œpunâ€ (12 occurrences)</li><li>â€œbunâ€ (4 occurrences)</li><li>â€œhugsâ€ (5 occurrences)</li></ul><p>Our initial base vocabulary consists of the following characters: [â€œhâ€, â€œuâ€, â€œgâ€, â€œpâ€, â€œnâ€, â€œbâ€,\xa0â€œsâ€].</p><p>We split the words into individual characters:</p><ul><li>â€œhâ€ â€œuâ€ â€œgâ€\xa0(hug)</li><li>â€œpâ€ â€œuâ€ â€œgâ€\xa0(pug)</li><li>â€œpâ€ â€œuâ€ â€œnâ€\xa0(pun)</li><li>â€œbâ€ â€œuâ€ â€œnâ€\xa0(bun)</li><li>â€œhâ€ â€œuâ€ â€œgâ€ â€œsâ€\xa0(hugs)</li></ul><p>Next, we count the frequency of each symbol\xa0pair:</p><ul><li>â€œh uâ€: 15 times (from â€œhugâ€ and\xa0â€œhugsâ€)</li><li>â€œu gâ€: 20 times (from â€œhugâ€, â€œpugâ€,\xa0â€œhugsâ€)</li><li>â€œp uâ€: 17 times (from â€œpugâ€,\xa0â€œpunâ€)</li><li>â€œu nâ€: 16 times (from â€œpunâ€,\xa0â€œbunâ€)</li></ul><p>The most frequent pair is â€œu gâ€ (20 times), so we merge â€œuâ€ and â€œgâ€ to form â€œugâ€ and update our\xa0words:</p><ul><li>â€œhâ€ â€œugâ€\xa0(hug)</li><li>â€œpâ€ â€œugâ€\xa0(pug)</li><li>â€œpâ€ â€œuâ€ â€œnâ€\xa0(pun)</li><li>â€œbâ€ â€œuâ€ â€œnâ€\xa0(bun)</li><li>â€œhâ€ â€œugâ€ â€œsâ€\xa0(hugs)</li></ul><p>We continue this process, merging the next most frequent pairs, such as â€œu nâ€ into â€œunâ€, until we reach our desired vocabulary size.</p><p>BPE controls the vocabulary size by specifying the number of merge operations. Frequent words remain intact, reducing the need for extensive memorization. And, rare or unseen words can be represented through combinations of known subwords. Itâ€™s used in models like <a href="https://openai.com/index/language-unsupervised/"><strong>GPT</strong></a> and\xa0<a href="https://arxiv.org/abs/1907.11692"><strong>RoBERTa</strong></a>.</p><p>The Hugging Face tokenizers library provides a fast and flexible way to train and use tokenizers, including BPE.</p><h4>Training a BPE Tokenizer</h4><p>Hereâ€™s how to train a BPE tokenizer on a sample\xa0dataset:</p><pre>from tokenizers import Tokenizer<br />from tokenizers.models import BPE<br />from tokenizers.trainers import BpeTrainer<br />from tokenizers.pre_tokenizers import Whitespace<br /><br /># Initialize a tokenizer<br />tokenizer = Tokenizer(BPE())<br /><br /># Set the pre-tokenizer to split on whitespace<br />tokenizer.pre_tokenizer = Whitespace()<br /><br /># Initialize a trainer with desired vocabulary size<br />trainer = BpeTrainer(vocab_size=1000, min_frequency=2, special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])<br /><br /># Files to train on<br />files = [&quot;path/to/your/dataset.txt&quot;]<br /><br /># Train the tokenizer<br />tokenizer.train(files, trainer)<br /><br /># Save the tokenizer<br />tokenizer.save(&quot;bpe-tokenizer.json&quot;)</pre><p><strong>Using the trained BPE Tokenizer:</strong></p><pre>from tokenizers import Tokenizer<br /><br /># Load the tokenizer<br />tokenizer = Tokenizer.from_file(&quot;bpe-tokenizer.json&quot;)<br /><br /># Encode a text input<br />encoded = tokenizer.encode(&quot;I have a new GPU!&quot;)<br />print(&quot;Tokens:&quot;, encoded.tokens)<br />print(&quot;IDs:&quot;, encoded.ids)</pre><p><strong>Output:</strong></p><pre>Tokens: [\'I\', \'have\', \'a\', \'new\', \'GP\', \'U\', \'!\']<br />IDs: [12, 45, 7, 89, 342, 210, 5]</pre><h3>WordPiece</h3><p><strong>WordPiece </strong>is another subword tokenization algorithm, introduced by <a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">Schuster and Nakajima in 2012</a> and popularized by models like <a href="https://arxiv.org/abs/1810.04805"><strong>BERT</strong></a>. Similar to BPE, WordPiece starts with all unique characters but differs in how it selects which symbol pairs to\xa0merge.</p><p>Hereâ€™s how WordPiece works:</p><ol><li><strong>Initialization</strong>: Start with a vocabulary of all unique characters.</li><li><strong>Pre-tokenization</strong>: Split the training text into\xa0words.</li><li><strong>Building the Vocabulary</strong>: Iteratively add new symbols (subwords) to the vocabulary.</li><li><strong>Selection Criterion</strong>: Instead of choosing the most frequent symbol pair, WordPiece selects the pair that maximizes the likelihood of the training data when added to the vocabulary.</li></ol><p>Using the same word frequencies as before, WordPiece evaluates which symbol pair, when merged, would most increase the probability of the training data. This involves a more probabilistic approach compared to BPEâ€™s frequency-based method.</p><p>Similar to BPE, we can train a WordPiece tokenizer using the tokenizers library.</p><h4>Training a WordPiece Tokenizer</h4><pre>from tokenizers import Tokenizer<br />from tokenizers.models import WordPiece<br />from tokenizers.trainers import WordPieceTrainer<br />from tokenizers.pre_tokenizers import Whitespace<br /><br /># Initialize a tokenizer<br />tokenizer = Tokenizer(WordPiece(unk_token=&quot;[UNK]&quot;))<br /><br /># Set the pre-tokenizer<br />tokenizer.pre_tokenizer = Whitespace()<br /><br /># Initialize a trainer<br />trainer = WordPieceTrainer(vocab_size=1000, min_frequency=2, special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])<br /><br /># Train the tokenizer<br />tokenizer.train(files, trainer)<br /><br /># Save the tokenizer<br />tokenizer.save(&quot;wordpiece-tokenizer.json&quot;)</pre><p><strong>Using the trained WordPiece tokenizer:</strong></p><pre>from tokenizers import Tokenizer<br /><br /># Load the tokenizer<br />tokenizer = Tokenizer.from_file(&quot;wordpiece-tokenizer.json&quot;)<br /><br /># Encode a text input<br />encoded = tokenizer.encode(&quot;I have a new GPU!&quot;)<br />print(&quot;Tokens:&quot;, encoded.tokens)<br />print(&quot;IDs:&quot;, encoded.ids)</pre><p><strong>Output:</strong></p><pre>Tokens: [\'I\', \'have\', \'a\', \'new\', \'G\', \'##PU\', \'!\']<br />IDs: [10, 34, 5, 78, 301, 502, 8]</pre><h3><strong>Conclusion</strong></h3><p>Tokenization is a foundational step in NLP that prepares text data for computational models. By understanding and implementing appropriate tokenization strategies, we enable models to process and generate human language more effectively, setting the stage for advanced topics like word embeddings and language modeling.</p><blockquote>All the code in this article is also available on my GitHub repo: <a href="https://github.com/murilogustineli/nlp-medium"><strong>github.com/murilogustineli/nlp-medium</strong></a></blockquote><h4>Other Resources</h4><ul><li><a href="https://www.youtube.com/watch?v=zduSFxRajkE">Letâ€™s build the GPT Tokenizer | Andrej Karpathy on\xa0YouTube</a></li><li><a href="https://docs.mistral.ai/guides/tokenization/">Tokenization | Mistral AI Large Language\xa0Models</a></li><li><a href="https://huggingface.co/docs/transformers/en/tokenizer_summary">Summary of the tokenizers | Hugging\xa0Face</a></li><li><a href="https://huggingface.co/learn/nlp-course/en/chapter6/8">Building a tokenizer, block by block | Hugging\xa0Face</a></li></ul><blockquote>Unless otherwise noted, all images are created by the\xa0author.</blockquote><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=43c7bccaed25" width="1" /><hr /><p><a href="https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25">The Art of Tokenization: Breaking Down Text for AI</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>'
2024-09-26 19:18:41,175 - INFO - clean content - clean_content="Demystifying NLP: From Text to EmbeddingsTokenization example generated by Llama-3-8B. Each colored subword represents a distinct token.What is tokenization?In computer science, we refer to human languages, like English and Mandarin, as â€œnaturalâ€ languages. In contrast, languages designed to interact with computers, like Assembly and LISP, are called â€œmachineâ€ languages, following strict syntactic rules that leave little room for interpretation. While computers excel at processing their own highly structured languages, they struggle with the messiness of human language.Language â€” especially text â€” makes up most of our communication and knowledge storage. For example, the internet is mostly text. Large language models like ChatGPT, Claude, and Llama are trained on enormous amounts of text â€” essentially all the text available online â€” using sophisticated computational techniques. However, computers operate on numbers, not words or sentences. So, how do we bridge the gap between human language and machine understanding?This is where Natural Language Processing (NLP) comes into play. NLP is a field that combines linguistics, computer science, and artificial intelligence to enable computers to understand, interpret, and generate human language. Whether translating text from English to French, summarizing articles, or engaging in conversation, NLP allows machines to produce meaningful outputs from textual inputs.The first critical step in NLP is transforming raw text into a format that computers can work with effectively. This process is known as tokenization. Tokenization involves breaking down text into smaller, manageable units called tokens, which can be words, subwords, or even individual characters. Hereâ€™s how the process typically works:Standardization: Before tokenizing, the text is standardized to ensure consistency. This may include converting all letters to lowercase, removing punctuation, and applying other normalization techniques.Tokenization: The standardized text is then split into tokens. For example, the sentence â€œThe quick brown fox jumps over the lazy dogâ€ can be tokenized into words:[&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumps&quot;, &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dog&quot;]Numerical representation: Since computers operate on numerical data, each token is converted into a numerical representation. This can be as simple as assigning a unique identifier to each token or as complex as creating multi-dimensional vectors that capture the tokenâ€™s meaning and context.Illustration inspired by â€œFigure 11.1 From text to vectorsâ€ from Deep Learning with Python by FranÃ§ois CholletTokenization is more than just splitting text; itâ€™s about preparing language data in a way that preserves meaning and context for computational models. Different tokenization methods can significantly impact how well a model understands and processes language.In this article, we focus on text standardization and tokenization, exploring a few techniques and implementations. Weâ€™ll lay the groundwork for converting text into numerical forms that machines can process â€” a crucial step toward advanced topics like word embeddings and language modeling that weâ€™ll tackle in future articles.Text standardizationConsider these two sentences:1. â€œdusk fell, i was gazing at the Sao Paulo skyline. Isnt urban life vibrant??â€2. â€œDusk fell; I gazed at the SÃ£o Paulo skyline. Isnâ€™t urban life vibrant?â€At first glance, these sentences convey a similar meaning. However, when processed by a computer, especially during tasks like tokenization or encoding, they can appear vastly different due to subtle variations:Capitalization: â€œduskâ€ vs. â€œDuskâ€Punctuation: Comma vs. semicolon; presence of question marksContractions: â€œIsntâ€ vs. â€œIsnâ€™tâ€Spelling and Special Characters: â€œSao Pauloâ€ vs. â€œSÃ£o Pauloâ€These differences can significantly impact how algorithms interpret the text. For example, â€œIsntâ€ without an apostrophe may not be recognized as the contraction of â€œis notâ€, and special characters like â€œÃ£â€ in â€œSÃ£oâ€ may be misinterpreted or cause encoding issues.Text standardization is a crucial preprocessing step in NLP that addresses these issues. By standardizing text, we reduce irrelevant variability and ensure that the data fed into models is consistent. This process is a form of feature engineering where we eliminate differences that are not meaningful for the task at hand.A simple method for text standardization includes:Converting to lowercase: Reduces discrepancies due to capitalization.Removing punctuation: Simplifies the text by eliminating punctuation marks.Normalizing special characters: Converts characters like â€œÃ£â€ to their standard forms (â€œaâ€).Applying these steps to our sentences, we get:1. â€œdusk fell i was gazing at the sao paulo skyline isnt urban life vibrantâ€2. â€œdusk fell i gazed at the sao paulo skyline isnt urban life vibrantâ€Now, the sentences are more uniform, highlighting only the meaningful differences in word choice (e.g., â€œwas gazing atâ€ vs. â€œgazed atâ€).While there are more advanced standardization techniques like stemming (reducing words to their root forms) and lemmatization (reducing words to their dictionary form), this basic approach effectively minimizes superficial differences.Python implementation of text standardizationHereâ€™s how you can implement basic text standardization in Python:import reimport unicodedatadef standardize_text(text:str) -&gt; str: # Convert text to lowercase text = text.lower() # Normalize unicode characters to ASCII text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8') # Remove punctuation text = re.sub(r'[^\\w\\s]', '', text) # Remove extra whitespace text = re.sub(r'\\s+', ' ', text).strip() return text# Example sentencessentence1 = &quot;dusk fell, i was gazing at the Sao Paulo skyline. Isnt urban life vibrant??&quot;sentence2 = &quot;Dusk fell; I gazed at the SÃ£o Paulo skyline. Isn't urban life vibrant?&quot;# Standardize sentencesstd_sentence1 = standardize_text(sentence1)std_sentence2 = standardize_text(sentence2)print(std_sentence1)print(std_sentence2)Output:dusk fell i was gazing at the sao paulo skyline isnt urban life vibrantdusk fell i gazed at the sao paulo skyline isnt urban life vibrantBy standardizing the text, weâ€™ve minimized differences that could confuse a computational model. The model can now focus on the variations between the sentences, such as the difference between â€œwas gazing atâ€ and â€œgazed atâ€, rather than discrepancies like punctuation or capitalization.TokenizationAfter text standardization, the next critical step in natural language processing is tokenization. Tokenization involves breaking down the standardized text into smaller units called tokens. These tokens are the building blocks that models use to understand and generate human language. Tokenization prepares the text for vectorization, where each token is converted into numerical representations that machines can process.We aim to convert sentences into a form that computers can efficiently and effectively handle. There are three common methods for tokenization:1. Word-level tokenizationSplits text into individual words based on spaces and punctuation. Itâ€™s the most intuitive way to break down text.text = &quot;dusk fell i gazed at the sao paulo skyline isnt urban life vibrant&quot;tokens = text.split()print(tokens)Output:['dusk', 'fell', 'i', 'gazed', 'at', 'the', 'sao', 'paulo', 'skyline', 'isnt', 'urban', 'life', 'vibrant']2. Character-level tokenizationBreaks text into individual characters, including letters and sometimes punctuation.text = &quot;Dusk fell&quot;tokens = list(text)print(tokens)Output:['D', 'u', 's', 'k', ' ', 'f', 'e', 'l', 'l']3. Subword tokenizationSplits words into smaller, meaningful subword units. This method balances the granularity of character-level tokenization with the semantic richness of word-level tokenization. Algorithms like Byte-Pair Encoding (BPE) and WordPiece fall under this category. For instance, the BertTokenizer tokenizes â€œI have a new GPU!â€ as follows:from transformers import BertTokenizertext = &quot;I have a new GPU!&quot;tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)tokens = tokenizer.tokenize(text)print(tokens)Output:['i', 'have', 'a', 'new', 'gp', '##u', '!']Here, â€œGPUâ€ is split into â€œgpâ€ and â€œ##uâ€, where â€œ##â€ indicates that â€œuâ€ is a continuation of the previous subword.Subword tokenization offers a balanced approach between vocabulary size and semantic representation. By decomposing rare words into common subwords, it maintains a manageable vocabulary size without sacrificing meaning. Subwords carry semantic information that aids models in understanding context more effectively. This means models can process new or rare words by breaking them down into familiar subwords, increasing their ability to handle a wider range of language inputs.For example, consider the word â€œannoyinglyâ€ which might be rare in a training corpus. It can be decomposed into the subwords â€œannoyingâ€ and â€œlyâ€. Both â€œannoyingâ€ and â€œlyâ€ appear more frequently on their own, and their combined meanings retain the essence of â€œannoyinglyâ€. This approach is especially beneficial in agglutinative languages like Turkish, where words can become exceedingly long by stringing together subwords to convey complex meanings.Notice that the standardization step is often integrated into the tokenizer itself. Large language models use tokens as both inputs and outputs when processing text. Hereâ€™s a visual representation of tokens generated by Llama-3â€“8B on Tiktokenizer:Tiktokenizer example using Llama-3â€“8B. Each token is represented by a different color.Additionally, Hugging Face provides an excellent summary of the tokenizers guide, in which I use some of its examples in this article.Letâ€™s now explore how different subword tokenization algorithms work. Note that all of those tokenization algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained on.Byte-Pair Encoding (BPE)Byte-Pair Encoding is a subword tokenization method introduced in Neural Machine Translation of Rare Words with Subword Units by Sennrich et al. in 2015. BPE starts with a base vocabulary consisting of all unique characters in the training data and iteratively merges the most frequent pairs of symbols â€” which can be characters or sequences of characters â€” to form new subwords. This process continues until the vocabulary reaches a predefined size, which is a hyperparameter you choose before training.Suppose we have the following words with their frequencies:â€œhugâ€ (10 occurrences)â€œpugâ€ (5 occurrences)â€œpunâ€ (12 occurrences)â€œbunâ€ (4 occurrences)â€œhugsâ€ (5 occurrences)Our initial base vocabulary consists of the following characters: [â€œhâ€, â€œuâ€, â€œgâ€, â€œpâ€, â€œnâ€, â€œbâ€, â€œsâ€].We split the words into individual characters:â€œhâ€ â€œuâ€ â€œgâ€ (hug)â€œpâ€ â€œuâ€ â€œgâ€ (pug)â€œpâ€ â€œuâ€ â€œnâ€ (pun)â€œbâ€ â€œuâ€ â€œnâ€ (bun)â€œhâ€ â€œuâ€ â€œgâ€ â€œsâ€ (hugs)Next, we count the frequency of each symbol pair:â€œh uâ€: 15 times (from â€œhugâ€ and â€œhugsâ€)â€œu gâ€: 20 times (from â€œhugâ€, â€œpugâ€, â€œhugsâ€)â€œp uâ€: 17 times (from â€œpugâ€, â€œpunâ€)â€œu nâ€: 16 times (from â€œpunâ€, â€œbunâ€)The most frequent pair is â€œu gâ€ (20 times), so we merge â€œuâ€ and â€œgâ€ to form â€œugâ€ and update our words:â€œhâ€ â€œugâ€ (hug)â€œpâ€ â€œugâ€ (pug)â€œpâ€ â€œuâ€ â€œnâ€ (pun)â€œbâ€ â€œuâ€ â€œnâ€ (bun)â€œhâ€ â€œugâ€ â€œsâ€ (hugs)We continue this process, merging the next most frequent pairs, such as â€œu nâ€ into â€œunâ€, until we reach our desired vocabulary size.BPE controls the vocabulary size by specifying the number of merge operations. Frequent words remain intact, reducing the need for extensive memorization. And, rare or unseen words can be represented through combinations of known subwords. Itâ€™s used in models like GPT and RoBERTa.The Hugging Face tokenizers library provides a fast and flexible way to train and use tokenizers, including BPE.Training a BPE TokenizerHereâ€™s how to train a BPE tokenizer on a sample dataset:from tokenizers import Tokenizerfrom tokenizers.models import BPEfrom tokenizers.trainers import BpeTrainerfrom tokenizers.pre_tokenizers import Whitespace# Initialize a tokenizertokenizer = Tokenizer(BPE())# Set the pre-tokenizer to split on whitespacetokenizer.pre_tokenizer = Whitespace()# Initialize a trainer with desired vocabulary sizetrainer = BpeTrainer(vocab_size=1000, min_frequency=2, special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])# Files to train onfiles = [&quot;path/to/your/dataset.txt&quot;]# Train the tokenizertokenizer.train(files, trainer)# Save the tokenizertokenizer.save(&quot;bpe-tokenizer.json&quot;)Using the trained BPE Tokenizer:from tokenizers import Tokenizer# Load the tokenizertokenizer = Tokenizer.from_file(&quot;bpe-tokenizer.json&quot;)# Encode a text inputencoded = tokenizer.encode(&quot;I have a new GPU!&quot;)print(&quot;Tokens:&quot;, encoded.tokens)print(&quot;IDs:&quot;, encoded.ids)Output:Tokens: ['I', 'have', 'a', 'new', 'GP', 'U', '!']IDs: [12, 45, 7, 89, 342, 210, 5]WordPieceWordPiece is another subword tokenization algorithm, introduced by Schuster and Nakajima in 2012 and popularized by models like BERT. Similar to BPE, WordPiece starts with all unique characters but differs in how it selects which symbol pairs to merge.Hereâ€™s how WordPiece works:Initialization: Start with a vocabulary of all unique characters.Pre-tokenization: Split the training text into words.Building the Vocabulary: Iteratively add new symbols (subwords) to the vocabulary.Selection Criterion: Instead of choosing the most frequent symbol pair, WordPiece selects the pair that maximizes the likelihood of the training data when added to the vocabulary.Using the same word frequencies as before, WordPiece evaluates which symbol pair, when merged, would most increase the probability of the training data. This involves a more probabilistic approach compared to BPEâ€™s frequency-based method.Similar to BPE, we can train a WordPiece tokenizer using the tokenizers library.Training a WordPiece Tokenizerfrom tokenizers import Tokenizerfrom tokenizers.models import WordPiecefrom tokenizers.trainers import WordPieceTrainerfrom tokenizers.pre_tokenizers import Whitespace# Initialize a tokenizertokenizer = Tokenizer(WordPiece(unk_token=&quot;[UNK]&quot;))# Set the pre-tokenizertokenizer.pre_tokenizer = Whitespace()# Initialize a trainertrainer = WordPieceTrainer(vocab_size=1000, min_frequency=2, special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])# Train the tokenizertokenizer.train(files, trainer)# Save the tokenizertokenizer.save(&quot;wordpiece-tokenizer.json&quot;)Using the trained WordPiece tokenizer:from tokenizers import Tokenizer# Load the tokenizertokenizer = Tokenizer.from_file(&quot;wordpiece-tokenizer.json&quot;)# Encode a text inputencoded = tokenizer.encode(&quot;I have a new GPU!&quot;)print(&quot;Tokens:&quot;, encoded.tokens)print(&quot;IDs:&quot;, encoded.ids)Output:Tokens: ['I', 'have', 'a', 'new', 'G', '##PU', '!']IDs: [10, 34, 5, 78, 301, 502, 8]ConclusionTokenization is a foundational step in NLP that prepares text data for computational models. By understanding and implementing appropriate tokenization strategies, we enable models to process and generate human language more effectively, setting the stage for advanced topics like word embeddings and language modeling.All the code in this article is also available on my GitHub repo: github.com/murilogustineli/nlp-mediumOther ResourcesLetâ€™s build the GPT Tokenizer | Andrej Karpathy on YouTubeTokenization | Mistral AI Large Language ModelsSummary of the tokenizers | Hugging FaceBuilding a tokenizer, block by block | Hugging FaceUnless otherwise noted, all images are created by the author.The Art of Tokenization: Breaking Down Text for AI was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story."
2024-09-26 19:18:41,175 - INFO - Generating summary with BART model
2024-09-26 19:18:51,270 - INFO - Generating summary with BART model
2024-09-26 19:19:01,362 - INFO - full_tweet='In computer science, we refer to human languages, like English and Mandarin, as â€œnaturalâ€ languages. In..[read moreğŸ‘‡ğŸ¼] #computer #languages https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25?source=rss----7f60cf5620c9---4'
2024-09-26 19:19:01,452 - ERROR - Failed to post tweet: 429 Too Many Requests
Too Many Requests

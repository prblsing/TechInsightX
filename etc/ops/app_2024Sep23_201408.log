2024-09-23 20:14:08,683 - INFO - Starting configuration setup
2024-09-23 20:14:08,683 - INFO - Environment variables loaded successfully
2024-09-23 20:14:08,684 - INFO - Twitter API client initialized successfully
2024-09-23 20:14:10,581 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-09-23 20:14:26,533 - INFO - Summarization model initialized successfully
2024-09-23 20:14:26,533 - INFO - Starting tweet scheduling
2024-09-23 20:14:26,533 - INFO - Tweeting process started at 2024-09-23 20:14:26.533860!
2024-09-23 20:14:26,534 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Sep23.csv
2024-09-23 20:14:26,534 - INFO - Fetching latest tech news from RSS feeds
2024-09-23 20:14:40,238 - INFO - Total entries found: 617
2024-09-23 20:14:40,241 - INFO - Recent AI-related entries found: 21
2024-09-23 20:14:40,241 - INFO - Input content - content='<h4>Using Gemini + Text to Speech + MoviePy to create a video, and what this says about what GenAI is becoming rapidly useful\xa0for</h4><p>Like most everyone, I was flabbergasted by <a href="https://blog.google/technology/ai/notebooklm-audio-overviews/">NotebookLM and its ability to generate a podcast</a> from a set of documents. And then, I got to thinking: “how do they do that, and where can I get some of that magic?” How easy would it be to replicate?</p><h4>Goal: Create a video talk from an\xa0article</h4><p>I don’t want to create a podcast, but I’ve often wished I could generate slides and a video talk from my blog posts —some people prefer paging through slides, and others prefer to watch videos, and this would be a good way to meet them where they are. In this article, I’ll show you how to do\xa0this.</p><p>The <a href="https://github.com/lakshmanok/lakblogs/blob/main/genai_seminar/create_lecture.ipynb">full code for this article</a> is on GitHub\u200a—\u200ain case you want to follow along with me. And the goal is to create this video from <a href="https://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc">this\xa0article</a>:</p><a href="https://medium.com/media/9e21a6965b4d638eebc39df3b37777c6/href">https://medium.com/media/9e21a6965b4d638eebc39df3b37777c6/href</a><h4>1. Initialize the\xa0LLM</h4><p>I am going to use Google Gemini Flash because (a) it is the least expensive frontier LLM today, (b) it’s multimodal in that it can read and understand images also, and (c) it supports controlled generation, meaning that we can make sure the output of the LLM matches a desired structure.</p><pre>import pdfkit<br />import os<br />import google.generativeai as genai<br />from dotenv import load_dotenv<br /><br />load_dotenv(&quot;../genai_agents/keys.env&quot;)<br />genai.configure(api_key=os.environ[&quot;GOOGLE_API_KEY&quot;])</pre><p>Note that I’m using Google Generative AI and not Google Cloud Vertex AI. The two packages are different. The Google one supports Pydantic objects for controlled generation; the Vertex AI one only supports JSON for\xa0now.</p><h4>2. Get a PDF of the\xa0article</h4><p>I used Python to download the article as a PDF, and upload it to a temporary storage location that Gemini can\xa0read:</p><pre>ARTICLE_URL = &quot;https://lakshmanok.medium....&quot;<br />pdfkit.from_url(ARTICLE_URL, &quot;article.pdf&quot;)<br />pdf_file = genai.upload_file(&quot;article.pdf&quot;)</pre><p>Unfortunately, something about medium prevents pdfkit from getting the images in the article (perhaps because they are webm and not png\xa0…). So, my slides are going to be based on just the text of the article and not the\xa0images.</p><h4>3. Create lecture notes in\xa0JSON</h4><p>Here, the data format I want is a set of slides each of which has a title, key points, and a set of lecture notes. The lecture as a whole has a title and an attribution also.</p><pre>class Slide(BaseModel):<br />    title: str<br />    key_points: List[str]<br />    lecture_notes: str<br /><br />class Lecture(BaseModel):<br />    slides: List[Slide]<br />    lecture_title: str<br />    based_on_article_by: str</pre><p>Let’s tell Gemini what we want it to\xa0do:</p><pre>lecture_prompt = &quot;&quot;&quot;<br />You are a university professor who needs to create a lecture to<br />a class of undergraduate students.<br /><br />* Create a 10-slide lecture based on the following article.<br />* Each slide should contain the following information:<br />  - title: a single sentence that summarizes the main point<br />  - key_points: a list of between 2 and 5 bullet points. Use phrases, not full sentences.<br />  - lecture_notes: 3-10 sentences explaining the key points in easy-to-understand language. Expand on the points using other information from the article.<br />* Also, create a title for the lecture and attribute the original article\'s author.<br />&quot;&quot;&quot;</pre><p>The prompt is pretty straightforward\u200a—\u200aask Gemini to read the article, extract key points and create lecture\xa0notes.</p><p>Now, invoke the model, passing in the PDF file and asking it to populate the desired structure:</p><pre><br />model = genai.GenerativeModel(<br />    &quot;gemini-1.5-flash-001&quot;,<br />    system_instruction=[lecture_prompt]<br />)<br />generation_config={<br />    &quot;temperature&quot;: 0.7,<br />    &quot;response_mime_type&quot;: &quot;application/json&quot;,<br />    &quot;response_schema&quot;: Lecture<br />}<br />response = model.generate_content(<br />    [pdf_file],<br />    generation_config=generation_config,<br />    stream=False<br />)</pre><p>A few things to note about the code\xa0above:</p><ul><li>We pass in the prompt as the system prompt, so that we don’t need to keep sending in the prompt with new\xa0inputs.</li><li>We specify the desired response type as JSON, and the schema to be a Pydantic\xa0object</li><li>We send the PDF file to the model and tell it generate a response. We’ll wait for it to complete (no need to\xa0stream)</li></ul><p>The result is JSON, so extract it into a Python\xa0object:</p><pre>lecture = json.loads(response.text)</pre><p>For example, this is what the 3rd slide looks\xa0like:</p><pre>{\'key_points\': [<br />    \'Silver layer cleans, structures, and prepares data for self-service analytics.\',<br />    \'Data is denormalized and organized for easier use.\',<br />    \'Type 2 slowly changing dimensions are handled in this layer.\',<br />    \'Governance responsibility lies with the source team.\'<br />  ],<br /> \'lecture_notes\': \'The silver layer takes data from the bronze layer and transforms it into a usable format for self-service analytics. This involves cleaning, structuring, and organizing the data. Type 2 slowly changing dimensions, which track changes over time, are also handled in this layer. The governance of the silver layer rests with the source team, which is typically the data engineering team responsible for the source system.\',<br /> \'title\': \'The Silver Layer: Data Transformation and Preparation\'<br />}</pre><h4>4. Convert to PowerPoint</h4><p>We can use the Python package pptx to create a Presentation with notes and bullet points. The code to create a slide looks like\xa0this:</p><pre>for slidejson in lecture[\'slides\']:<br />    slide = presentation.slides.add_slide(presentation.slide_layouts[1])<br />    title = slide.shapes.title<br />    title.text = slidejson[\'title\']<br />    # bullets<br />    textframe = slide.placeholders[1].text_frame<br />    for key_point in slidejson[\'key_points\']:<br />        p = textframe.add_paragraph()<br />        p.text = key_point<br />        p.level = 1<br />    # notes<br />    notes_frame = slide.notes_slide.notes_text_frame<br />    notes_frame.text = slidejson[\'lecture_notes\']</pre><p>The result is a PowerPoint presentation that looks like\xa0this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NDLoGhJb-THBvAOgRFQ5tQ.jpeg" /><figcaption>The PowerPoint file that was generated from the keypoints and lecture notes. Screenshot by\xa0author.</figcaption></figure><p>Not very fancy, but definitely a great starting point for editing if you are going to give a\xa0talk.</p><h4>5. Read the notes aloud and save\xa0audio</h4><p>Well, we were inspired by a podcast, so let’s see how to create just an audio of someone summarizing the\xa0article.</p><p>We already have the lecture notes, so let’s create audio files of each of the\xa0slides.</p><p>Here’s the code to take some text, and have an AI voice read it out. We save the resulting audio into an mp3\xa0file:</p><pre>from google.cloud import texttospeech<br /><br />def convert_text_audio(text, audio_mp3file):<br />    &quot;&quot;&quot;Synthesizes speech from the input string of text.&quot;&quot;&quot;<br />    tts_client = texttospeech.TextToSpeechClient()    <br />    input_text = texttospeech.SynthesisInput(text=text)<br />    <br />    voice = texttospeech.VoiceSelectionParams(<br />        language_code=&quot;en-US&quot;,<br />        name=&quot;en-US-Standard-C&quot;,<br />        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,<br />    )<br />    audio_config = texttospeech.AudioConfig(<br />        audio_encoding=texttospeech.AudioEncoding.MP3<br />    )<br /><br />    response = tts_client.synthesize_speech(<br />        request={&quot;input&quot;: input_text, &quot;voice&quot;: voice, &quot;audio_config&quot;: audio_config}<br />    )<br /><br />    # The response\'s audio_content is binary.<br />    with open(audio_mp3file, &quot;wb&quot;) as out:<br />        out.write(response.audio_content)<br />        print(f&quot;{audio_mp3file} written.&quot;)</pre><p>What’s happening in the code\xa0above?</p><ul><li>We are using Google Cloud’s text to speech\xa0API</li><li>Asking it to use a standard US accent female voice. If you were doing a podcast, you’d pass in a “speaker map” here, one voice for each\xa0speaker.</li><li>We then give it in the input text, ask it generate\xa0audio</li><li>Save the audio as an mp3 file. Note that this has to match the audio encoding.</li></ul><p>Now, create audio by iterating through the slides, and passing in the lecture\xa0notes:</p><pre>for slideno, slide in enumerate(lecture[\'slides\']):<br />        text = f&quot;On to {slide[\'title\']} \\n&quot;<br />        text += slide[\'lecture_notes\'] + &quot;\\n\\n&quot;<br />        filename = os.path.join(outdir, f&quot;audio_{slideno+1:02}.mp3&quot;)<br />        convert_text_audio(text, filename)<br />        filenames.append(filename)</pre><p>The result is a bunch of audio files. You can concatenate them if you wish using\xa0pydub:</p><pre>combined = pydub.AudioSegment.empty()<br />for audio_file in audio_files:<br />    audio = pydub.AudioSegment.from_file(audio_file)<br />    combined += audio<br />    # pause for 4 seconds<br />    silence = pydub.AudioSegment.silent(duration=4000)<br />    combined += silence<br />combined.export(&quot;lecture.wav&quot;, format=&quot;wav&quot;)</pre><p>But it turned out that I didn’t need to. The individual audio files, one for each slide, were what I needed to create a video. For a podcast, of course, you’d want a single mp3 or wav\xa0file.</p><h4>6. Create images of the\xa0slides</h4><p>Rather annoyingly, there’s no easy way to render PowerPoint slides as images using Python. You need a machine with Office software installed to do that\u200a—\u200anot the kind of thing that’s easily automatable. Maybe I should have used Google Slides\xa0… Anyway, a simple way to render images is to use the Python Image Library\xa0(PIL):</p><pre>def text_to_image(output_path, title, keypoints):<br />    image = Image.new(&quot;RGB&quot;, (1000, 750), &quot;black&quot;)<br />    draw = ImageDraw.Draw(image)<br />    title_font = ImageFont.truetype(&quot;Coval-Black.ttf&quot;, size=42)<br />    draw.multiline_text((10, 25), wrap(title, 50), font=title_font)<br />    text_font = ImageFont.truetype(&quot;Coval-Light.ttf&quot;, size=36)<br />    for ptno, keypoint in enumerate(keypoints):<br />        draw.multiline_text((10, (ptno+2)*100), wrap(keypoint, 60), font=text_font) <br />    image.save(output_path)</pre><p>The resulting image is not great, but it is serviceable (you can tell no one pays me to write production code anymore):</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1000/1*SHY-JxSGXkzP7Ko4Xr_d5w.jpeg" /><figcaption>The images used along with the audio clips look like this. Image generated by\xa0author.</figcaption></figure><h4>7. Create a\xa0Video</h4><p>Now that we have a set of audio files and a set of image files, we can use a Python package moviepy to create a video\xa0clip:</p><pre>clips = []<br />for slide, audio in zip(slide_files, audio_files):<br />    audio_clip = AudioFileClip(f&quot;article_audio/{audio}&quot;)<br />    slide_clip = ImageClip(f&quot;article_slides/{slide}&quot;).set_duration(audio_clip.duration)<br />    slide_clip = slide_clip.set_audio(audio_clip)<br />    clips.append(slide_clip)<br />full_video = concatenate_videoclips(clips)</pre><p>And we can now write it\xa0out:</p><pre>full_video.write_videofile(&quot;lecture.mp4&quot;, fps=24, codec=&quot;mpeg4&quot;, <br />                           temp_audiofile=\'temp-audio.mp4\', remove_temp=True)</pre><p>End result? We have four artifacts, all created automatically from the article.pdf:</p><pre>lecture.json  lecture.mp4  lecture.pptx  lecture.wav</pre><p>There’s:</p><ul><li>a JSON file with keypoints, lecture notes,\xa0etc.</li><li>A PowerPoint file that you can modify. The slides have the key points, and the notes section of the slides has the “lecture\xa0notes”</li><li>An audio file consisting of an AI voice reading out the lecture\xa0notes</li><li>A mp4 movie (that I uploaded to YouTube) of the audio + images. This is the video talk that I set out to\xa0create.</li></ul><p>Pretty cool,\xa0eh?</p><h4>8. What this says about the future of\xa0software</h4><p>We are all, as a community, probing around to find what this really cool technology (generative AI) can be used for. Obviously, you can use it to create content, but the content that it creates is good for brainstorming, but not to use as-is. Three years of improvements in the tech have not solved the problem that GenAI generates blah content, and not-ready-to-use code.</p><p>That brings us to some of the ancillary capabilities that GenAI has opened up. And these turn out to be extremely useful. There are four capabilities of GenAI that this post illustrates.</p><p><strong>(1) Translating unstructured data to structured data</strong></p><p>The Attention paper was written to solve the translation problem, and it turns out transformer-based models are really good at translation. We keep discovering use cases of this. But not just <a href="https://mse238blog.stanford.edu/2017/08/jchoi8/machine-learning-transforms-google-translate-overnight/">Japanese to English</a>, but also <a href="https://digiday.com/media/how-amazons-genai-tool-for-developers-is-saving-4500-years-of-work-260-million-annually/">Java 11 to Java 17</a>, of <a href="https://paperswithcode.com/task/text-to-sql">text to SQL</a>, of text to speech, between database dialects,\xa0…, and now of articles to audio-scripts. This, it turns out is the stepping point of using GenAI to create podcasts, lectures, videos,\xa0etc.</p><p>All I had to do was to prompt the LLM to construct a series of slide contents (keypoints, title, etc.) from the article, and it did. It even returned the data to me in structured format, conducive to using it from a computer program. Specifically, <em>GenAI is really good at translating unstructured data to structured data</em>.</p><p><strong>(2) Code search and coding assistance are now dramatically better</strong></p><p>The other thing that GenAI turns out to be really good at is at adapting code samples dynamically. I don’t write code to create presentations or text-to-speech or moviepy everyday. Two years ago, I’d have been using Google search and getting Stack Overflow pages and adapting the code by hand. Now, Google search is giving me ready-to-incorporate code:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/687/1*7Tumb0Rc8co0cYZAlpgFug.png" /><figcaption>Google Search returning code samples, adapated to my specific query. Screenshot by\xa0author.</figcaption></figure><p>Of course, had I been using a Python IDE (rather than a Jupyter notebook), I could have avoided the search step completely\u200a—\u200aI could have written a comment and gotten the code generated for me. This is hugely helpful, and speeds up development using general purpose\xa0APIs.</p><p><strong>(3) GenAI web services are robust and easy-to-consume</strong></p><p>Let’s not lose track of the fact that I used the Google Cloud Text-to-Speech service to turn my audio script into actual audio files. Text-to-speech is itself a generative AI model (and another example of the translation superpower). The Google TTS service which was <a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-text-to-speech-powered-by-deepmind-wavenet-technology">introduced in 2018</a> (and presumably improved since then) was one of the first generative AI services in production and made available through an\xa0API.</p><p>In this article, I used two generative AI models\u200a—\u200aTTS and Gemini\u200a—\u200athat are made available as web services. All I had to do was to call their\xa0APIs.</p><p><strong>(4) It’s easier than ever to provide end-user customizability</strong></p><p>I didn’t do this, but you can squint a little and see where things are headed. If I’d wrapped up the presentation creation, audio creation, and movie creation code in services, I could have had a prompt create the function call to invoke these services as well. And put a request-handling agent that would allow you to use text to change the look-and-feel of the slides or the voice of the person reading the\xa0video.</p><p>It becomes extremely easy to add open-ended customizability to the software you\xa0build.</p><h4>Summary</h4><p>Inspired by the NotebookLM podcast feature, I set out to build an application that would convert my articles to video talks. The key step is to prompt an LLM to produce slide contents from the article, another GenAI model to convert the audio script into audio files, and use existing Python APIs to put them together into a\xa0video.</p><p>This article illustrates four capabilities that GenAI is unlocking: translation of all kinds, coding assistance, robust web services, and end-user customizability.</p><p>I loved being able to easily and quickly create video lectures from my articles. But I’m even more excited about the potential that we keep discovering in this new tool we have in our\xa0hands.</p><h4>Further Reading</h4><ol><li>Full code for this article: <a href="https://github.com/lakshmanok/lakblogs/blob/main/genai_seminar/create_lecture.ipynb">https://github.com/lakshmanok/lakblogs/blob/main/genai_seminar/create_lecture.ipynb</a></li><li>The source article that I converted to a video: <a href="https://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc">https://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc</a></li><li>The resulting video: <a href="https://youtu.be/jKzmj8-1Y9Q">https://youtu.be/jKzmj8-1Y9Q</a></li><li>Turns out <a href="https://medium.com/google-cloud/building-a-dynamic-podcast-generator-inspired-by-googles-notebooklm-and-illuminate-e585cfcd0af1">Sascha Heyer wrote up how to use GenAI to generate a podcast</a>, which is the exact Notebook LM usecase. His approach is somewhat similar to mine, except that there is no video, just audio. In a cool twist, he uses his own voice as one of the podcast speakers!</li><li>Of course, here’s the video talk of this article created using the technique shown in this video. Ideally, we are pulling out code snippets and images from the article, but this is a start\xa0…</li></ol><a href="https://medium.com/media/217a648d4c29d7ce479909172d22893e/href">https://medium.com/media/217a648d4c29d7ce479909172d22893e/href</a><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6381c44c5fe0" width="1" /><hr /><p><a href="https://towardsdatascience.com/using-generative-ai-to-automatically-create-a-video-talk-from-an-article-6381c44c5fe0">Using Generative AI to Automatically Create a Video Talk from an Article</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>'
2024-09-23 20:14:40,246 - INFO - clean content - clean_content="Using Gemini + Text to Speech + MoviePy to create a video, and what this says about what GenAI is becoming rapidly useful forLike most everyone, I was flabbergasted by NotebookLM and its ability to generate a podcast from a set of documents. And then, I got to thinking: “how do they do that, and where can I get some of that magic?” How easy would it be to replicate?Goal: Create a video talk from an articleI don’t want to create a podcast, but I’ve often wished I could generate slides and a video talk from my blog posts —some people prefer paging through slides, and others prefer to watch videos, and this would be a good way to meet them where they are. In this article, I’ll show you how to do this.The full code for this article is on GitHub — in case you want to follow along with me. And the goal is to create this video from this article: Initialize the LLMI am going to use Google Gemini Flash because (a) it is the least expensive frontier LLM today, (b) it’s multimodal in that it can read and understand images also, and (c) it supports controlled generation, meaning that we can make sure the output of the LLM matches a desired structure.import pdfkitimport osimport google.generativeai as genaifrom dotenv import load_dotenvload_dotenv(&quot;../genai_agents/keys.env&quot;)genai.configure(api_key=os.environ[&quot;GOOGLE_API_KEY&quot;])Note that I’m using Google Generative AI and not Google Cloud Vertex AI. The two packages are different. The Google one supports Pydantic objects for controlled generation; the Vertex AI one only supports JSON for now.2. Get a PDF of the articleI used Python to download the article as a PDF, and upload it to a temporary storage location that Gemini can read:ARTICLE_URL = &quot; &quot;article.pdf&quot;)pdf_file = genai.upload_file(&quot;article.pdf&quot;)Unfortunately, something about medium prevents pdfkit from getting the images in the article (perhaps because they are webm and not png …). So, my slides are going to be based on just the text of the article and not the images.3. Create lecture notes in JSONHere, the data format I want is a set of slides each of which has a title, key points, and a set of lecture notes. The lecture as a whole has a title and an attribution also.class Slide(BaseModel): title: str key_points: List[str] lecture_notes: strclass Lecture(BaseModel): slides: List[Slide] lecture_title: str based_on_article_by: strLet’s tell Gemini what we want it to do:lecture_prompt = &quot;&quot;&quot;You are a university professor who needs to create a lecture toa class of undergraduate students.* Create a 10-slide lecture based on the following article.* Each slide should contain the following information: - title: a single sentence that summarizes the main point - key_points: a list of between 2 and 5 bullet points. Use phrases, not full sentences. - lecture_notes: 3-10 sentences explaining the key points in easy-to-understand language. Expand on the points using other information from the article.* Also, create a title for the lecture and attribute the original article's author.&quot;&quot;&quot;The prompt is pretty straightforward — ask Gemini to read the article, extract key points and create lecture notes.Now, invoke the model, passing in the PDF file and asking it to populate the desired structure:model = genai.GenerativeModel( &quot;gemini-1.5-flash-001&quot;, system_instruction=[lecture_prompt])generation_config={ &quot;temperature&quot;: 0.7, &quot;response_mime_type&quot;: &quot;application/json&quot;, &quot;response_schema&quot;: Lecture}response = model.generate_content( [pdf_file], generation_config=generation_config, stream=False)A few things to note about the code above:We pass in the prompt as the system prompt, so that we don’t need to keep sending in the prompt with new inputs.We specify the desired response type as JSON, and the schema to be a Pydantic objectWe send the PDF file to the model and tell it generate a response. We’ll wait for it to complete (no need to stream)The result is JSON, so extract it into a Python object:lecture = json.loads(response.text)For example, this is what the 3rd slide looks like:{'key_points': [ 'Silver layer cleans, structures, and prepares data for self-service analytics.', 'Data is denormalized and organized for easier use.', 'Type 2 slowly changing dimensions are handled in this layer.', 'Governance responsibility lies with the source team.' ], 'lecture_notes': 'The silver layer takes data from the bronze layer and transforms it into a usable format for self-service analytics. This involves cleaning, structuring, and organizing the data. Type 2 slowly changing dimensions, which track changes over time, are also handled in this layer. The governance of the silver layer rests with the source team, which is typically the data engineering team responsible for the source system.', 'title': 'The Silver Layer: Data Transformation and Preparation'}4. Convert to PowerPointWe can use the Python package pptx to create a Presentation with notes and bullet points. The code to create a slide looks like this:for slidejson in lecture['slides']: slide = presentation.slides.add_slide(presentation.slide_layouts[1]) title = slide.shapes.title title.text = slidejson['title'] # bullets textframe = slide.placeholders[1].text_frame for key_point in slidejson['key_points']: p = textframe.add_paragraph() p.text = key_point p.level = 1 # notes notes_frame = slide.notes_slide.notes_text_frame notes_frame.text = slidejson['lecture_notes']The result is a PowerPoint presentation that looks like this:The PowerPoint file that was generated from the keypoints and lecture notes. Screenshot by author.Not very fancy, but definitely a great starting point for editing if you are going to give a talk.5. Read the notes aloud and save audioWell, we were inspired by a podcast, so let’s see how to create just an audio of someone summarizing the article.We already have the lecture notes, so let’s create audio files of each of the slides.Here’s the code to take some text, and have an AI voice read it out. We save the resulting audio into an mp3 file:from google.cloud import texttospeechdef convert_text_audio(text, audio_mp3file): &quot;&quot;&quot;Synthesizes speech from the input string of text.&quot;&quot;&quot; tts_client = texttospeech.TextToSpeechClient() input_text = texttospeech.SynthesisInput(text=text) voice = texttospeech.VoiceSelectionParams( language_code=&quot;en-US&quot;, name=&quot;en-US-Standard-C&quot;, ssml_gender=texttospeech.SsmlVoiceGender.FEMALE, ) audio_config = texttospeech.AudioConfig( audio_encoding=texttospeech.AudioEncoding.MP3 ) response = tts_client.synthesize_speech( request={&quot;input&quot;: input_text, &quot;voice&quot;: voice, &quot;audio_config&quot;: audio_config} ) # The response's audio_content is binary. with open(audio_mp3file, &quot;wb&quot;) as out: out.write(response.audio_content) print(f&quot;{audio_mp3file} written.&quot;)What’s happening in the code above?We are using Google Cloud’s text to speech APIAsking it to use a standard US accent female voice. If you were doing a podcast, you’d pass in a “speaker map” here, one voice for each speaker.We then give it in the input text, ask it generate audioSave the audio as an mp3 file. Note that this has to match the audio encoding.Now, create audio by iterating through the slides, and passing in the lecture notes:for slideno, slide in enumerate(lecture['slides']): text = f&quot;On to {slide['title']} \\n&quot; text += slide['lecture_notes'] + &quot;\\n\\n&quot; filename = os.path.join(outdir, f&quot;audio_{slideno+1:02}.mp3&quot;) convert_text_audio(text, filename) filenames.append(filename)The result is a bunch of audio files. You can concatenate them if you wish using pydub:combined = pydub.AudioSegment.empty()for audio_file in audio_files: audio = pydub.AudioSegment.from_file(audio_file) combined += audio # pause for 4 seconds silence = pydub.AudioSegment.silent(duration=4000) combined += silencecombined.export(&quot;lecture.wav&quot;, format=&quot;wav&quot;)But it turned out that I didn’t need to. The individual audio files, one for each slide, were what I needed to create a video. For a podcast, of course, you’d want a single mp3 or wav file.6. Create images of the slidesRather annoyingly, there’s no easy way to render PowerPoint slides as images using Python. You need a machine with Office software installed to do that — not the kind of thing that’s easily automatable. Maybe I should have used Google Slides … Anyway, a simple way to render images is to use the Python Image Library (PIL):def text_to_image(output_path, title, keypoints): image = Image.new(&quot;RGB&quot;, (1000, 750), &quot;black&quot;) draw = ImageDraw.Draw(image) title_font = ImageFont.truetype(&quot;Coval-Black.ttf&quot;, size=42) draw.multiline_text((10, 25), wrap(title, 50), font=title_font) text_font = ImageFont.truetype(&quot;Coval-Light.ttf&quot;, size=36) for ptno, keypoint in enumerate(keypoints): draw.multiline_text((10, (ptno+2)*100), wrap(keypoint, 60), font=text_font) image.save(output_path)The resulting image is not great, but it is serviceable (you can tell no one pays me to write production code anymore):The images used along with the audio clips look like this. Image generated by author.7. Create a VideoNow that we have a set of audio files and a set of image files, we can use a Python package moviepy to create a video clip:clips = []for slide, audio in zip(slide_files, audio_files): audio_clip = AudioFileClip(f&quot;article_audio/{audio}&quot;) slide_clip = ImageClip(f&quot;article_slides/{slide}&quot;).set_duration(audio_clip.duration) slide_clip = slide_clip.set_audio(audio_clip) clips.append(slide_clip)full_video = concatenate_videoclips(clips)And we can now write it out:full_video.write_videofile(&quot;lecture.mp4&quot;, fps=24, codec=&quot;mpeg4&quot;, temp_audiofile='temp-audio.mp4', remove_temp=True)End result? We have four artifacts, all created automatically from the article.pdf:lecture.json lecture.mp4 lecture.pptx lecture.wavThere’s:a JSON file with keypoints, lecture notes, etc.A PowerPoint file that you can modify. The slides have the key points, and the notes section of the slides has the “lecture notes”An audio file consisting of an AI voice reading out the lecture notesA mp4 movie (that I uploaded to YouTube) of the audio + images. This is the video talk that I set out to create.Pretty cool, eh?8. What this says about the future of softwareWe are all, as a community, probing around to find what this really cool technology (generative AI) can be used for. Obviously, you can use it to create content, but the content that it creates is good for brainstorming, but not to use as-is. Three years of improvements in the tech have not solved the problem that GenAI generates blah content, and not-ready-to-use code.That brings us to some of the ancillary capabilities that GenAI has opened up. And these turn out to be extremely useful. There are four capabilities of GenAI that this post illustrates.(1) Translating unstructured data to structured dataThe Attention paper was written to solve the translation problem, and it turns out transformer-based models are really good at translation. We keep discovering use cases of this. But not just Japanese to English, but also Java 11 to Java 17, of text to SQL, of text to speech, between database dialects, …, and now of articles to audio-scripts. This, it turns out is the stepping point of using GenAI to create podcasts, lectures, videos, etc.All I had to do was to prompt the LLM to construct a series of slide contents (keypoints, title, etc.) from the article, and it did. It even returned the data to me in structured format, conducive to using it from a computer program. Specifically, GenAI is really good at translating unstructured data to structured data.(2) Code search and coding assistance are now dramatically betterThe other thing that GenAI turns out to be really good at is at adapting code samples dynamically. I don’t write code to create presentations or text-to-speech or moviepy everyday. Two years ago, I’d have been using Google search and getting Stack Overflow pages and adapting the code by hand. Now, Google search is giving me ready-to-incorporate code:Google Search returning code samples, adapated to my specific query. Screenshot by author.Of course, had I been using a Python IDE (rather than a Jupyter notebook), I could have avoided the search step completely — I could have written a comment and gotten the code generated for me. This is hugely helpful, and speeds up development using general purpose APIs.(3) GenAI web services are robust and easy-to-consumeLet’s not lose track of the fact that I used the Google Cloud Text-to-Speech service to turn my audio script into actual audio files. Text-to-speech is itself a generative AI model (and another example of the translation superpower). The Google TTS service which was introduced in 2018 (and presumably improved since then) was one of the first generative AI services in production and made available through an API.In this article, I used two generative AI models — TTS and Gemini — that are made available as web services. All I had to do was to call their APIs.(4) It’s easier than ever to provide end-user customizabilityI didn’t do this, but you can squint a little and see where things are headed. If I’d wrapped up the presentation creation, audio creation, and movie creation code in services, I could have had a prompt create the function call to invoke these services as well. And put a request-handling agent that would allow you to use text to change the look-and-feel of the slides or the voice of the person reading the video.It becomes extremely easy to add open-ended customizability to the software you build.SummaryInspired by the NotebookLM podcast feature, I set out to build an application that would convert my articles to video talks. The key step is to prompt an LLM to produce slide contents from the article, another GenAI model to convert the audio script into audio files, and use existing Python APIs to put them together into a video.This article illustrates four capabilities that GenAI is unlocking: translation of all kinds, coding assistance, robust web services, and end-user customizability.I loved being able to easily and quickly create video lectures from my articles. But I’m even more excited about the potential that we keep discovering in this new tool we have in our hands.Further ReadingFull code for this article: source article that I converted to a video: resulting video: out Sascha Heyer wrote up how to use GenAI to generate a podcast, which is the exact Notebook LM usecase. His approach is somewhat similar to mine, except that there is no video, just audio. In a cool twist, he uses his own voice as one of the podcast speakers!Of course, here’s the video talk of this article created using the technique shown in this video. Ideally, we are pulling out code snippets and images from the article, but this is a start … Generative AI to Automatically Create a Video Talk from an Article was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story."
2024-09-23 20:14:40,246 - INFO - Generating summary with BART model
2024-09-23 20:14:46,396 - INFO - Generating summary with BART model
2024-09-23 20:14:51,842 - INFO - full_tweet='Using Gemini + Text to Speech + MoviePy to create a video, and what this says about what GenAI is..[read more👇🏼] #what #becoming https://towardsdatascience.com/using-generative-ai-to-automatically-create-a-video-talk-from-an-article-6381c44c5fe0?source=rss----7f60cf5620c9---4'
2024-09-23 20:14:52,244 - INFO - Tweet posted successfully: Response(data={'id': '1838311087550218494', 'text': 'Using Gemini + Text to Speech + MoviePy to create a video, and what this says about what GenAI is..[read more👇🏼] #what #becoming https://t.co/ClGTxHbM5i', 'edit_history_tweet_ids': ['1838311087550218494']}, includes={}, errors=[], meta={})
2024-09-23 20:14:52,244 - INFO - Sleeping for 3 minutes and 59 seconds.
2024-09-23 20:18:51,245 - INFO - Saved posted URL: https://towardsdatascience.com/using-generative-ai-to-automatically-create-a-video-talk-from-an-article-6381c44c5fe0?source=rss----7f60cf5620c9---4 at 2024-09-23 20:18:51
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://www.cnet.com/tech/mobile/jony-ive-and-openais-sam-altman-are-working-on-a-top-secret-ai-device/#ftag=CAD590a51e
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1fnry1x/p_i_built_a_live_ai_sports_commentator_that_can/
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://techcrunch.com/2024/09/23/ephos-wants-to-shatter-the-market-for-ai-and-quantum-chips-with-a-new-design-based-on-glass/
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://www.pcmag.com/how-to/ai-on-the-go-how-to-use-chatgpt-from-your-mobile-device
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://www.techradar.com/pro/i-want-one-msi-quietly-debuts-sub-1kg-laptop-with-a-killer-price-tag-and-exceptional-performance-prestige-13-ai-will-appeal-to-businesses-and-no-don-t-get-put-off-by-its-lack-of-ram-upgradability
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://venturebeat.com/ai/together-ai-promises-faster-inference-and-lower-costs-with-enterprise-ai-platform-for-private-cloud/
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://towardsdatascience.com/programming-an-arduino-with-crewai-agents-3ac5ad200fdf?source=rss----7f60cf5620c9---4
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://www.pcmag.com/news/openai-is-developing-a-hardware-device-with-ex-apple-designer-jony-ive
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://techcrunch.com/2024/09/23/letta-one-of-uc-berkeleys-most-anticipated-ai-startups-has-just-come-out-of-stealth/
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://venturebeat.com/ai/altera-brings-more-ai-to-the-edge-and-cloud-with-new-programmable-chips/
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://techcrunch.com/2024/09/23/meta-connect-2024-how-to-watch-the-metaverse-and-generative-ai-event/
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://www.techradar.com/pro/security-pros-are-stressed-by-c-suite-ai-expectations
2024-09-23 20:18:51,245 - INFO - This link is already posted: https://www.techradar.com/pro/ai-is-no-silver-bullet-for-workforce-transformation-without-planning
2024-09-23 20:18:51,246 - INFO - This link is already posted: https://www.pcmag.com/news/dont-need-ai-microsoft-will-let-you-customize-the-copilot-key
2024-09-23 20:18:51,246 - INFO - This link is already posted: https://venturebeat.com/ai/openai-academy-launches-with-1m-in-developer-credits-for-devs-in-low-and-middle-income-countries/
2024-09-23 20:18:51,246 - INFO - This link is already posted: https://www.techradar.com/pro/visual-ai-a-future-proof-solution-for-better-meetings-today
2024-09-23 20:18:51,246 - INFO - This link is already posted: https://techcrunch.com/2024/09/23/cloudflares-new-marketplace-will-let-websites-charge-ai-bots-for-scraping/
2024-09-23 20:18:51,246 - INFO - This link is already posted: https://www.wired.com/story/cloudflare-tools-detect-block-ai-bots/
2024-09-23 20:18:51,246 - INFO - This link is already posted: https://venturebeat.com/ai/how-agentic-ai-could-improve-enterprise-data-operations/
2024-09-23 20:18:51,246 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1fnefsr/how_ai_companions_alleviate_loneliness_insights/

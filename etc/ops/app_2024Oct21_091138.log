2024-10-21 09:11:38,268 - INFO - Starting configuration setup
2024-10-21 09:11:38,269 - INFO - Environment variables loaded successfully
2024-10-21 09:11:38,269 - INFO - Twitter API client initialized successfully
2024-10-21 09:11:40,170 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-10-21 09:12:14,859 - INFO - Summarization model initialized successfully
2024-10-21 09:12:14,860 - INFO - Starting tweet scheduling
2024-10-21 09:12:14,860 - INFO - Tweeting process started at 2024-10-21 09:12:14.860210!
2024-10-21 09:12:14,860 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Oct21.csv
2024-10-21 09:12:14,860 - INFO - Fetching latest tech news from RSS feeds
2024-10-21 09:12:29,589 - INFO - Total entries found: 615
2024-10-21 09:12:29,592 - INFO - Recent AI-related entries found: 12
2024-10-21 09:12:29,592 - INFO - Input content - content='<p>A fledgling startup is setting out to become one of Europe&#8217;s first &#8220;AI compute&#8221; hyperscalers, with renewable energy playing a pivotal part in its pitch to prospective customers. The AI goldrush has spurred unprecedented demand for &#8220;compute,&#8221; which refers to the processing power, infrastructure and resources needed for tasks such as running algorithms, executing machine [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>'
2024-10-21 09:12:29,593 - INFO - clean content - clean_content='A fledgling startup is setting out to become one of Europe’s first “AI compute” hyperscalers, with renewable energy playing a pivotal part in its pitch to prospective customers. The AI goldrush has spurred unprecedented demand for “compute,” which refers to the processing power, infrastructure and resources needed for tasks such as running algorithms, executing machine […] © 2024 TechCrunch. All rights reserved. For personal use only.'
2024-10-21 09:12:29,593 - INFO - Generating summary with BART model
2024-10-21 09:12:37,005 - INFO - Generating summary with BART model
2024-10-21 09:12:43,737 - INFO - full_tweet='A fledgling startup is setting out to become one of Europe’s first ‘AI compute’ hyperscalers. Renewable..[read more👇🏼] #AI #startup https://techcrunch.com/2024/10/21/datacrunch-wants-to-be-europes-first-ai-cloud-hyperscaler-powered-by-renewable-energy/'
2024-10-21 09:12:44,060 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1848291315701743679'], 'text': 'A fledgling startup is setting out to become one of Europe’s first ‘AI compute’ hyperscalers. Renewable..[read more👇🏼] #AI #startup https://t.co/RwnPwSrAfT', 'id': '1848291315701743679'}, includes={}, errors=[], meta={})
2024-10-21 09:12:44,060 - INFO - Saved posted URL: https://techcrunch.com/2024/10/21/datacrunch-wants-to-be-europes-first-ai-cloud-hyperscaler-powered-by-renewable-energy/ at 2024-10-21 09:12:44
2024-10-21 09:12:44,060 - INFO - Tweet link saved successfully.
2024-10-21 09:12:44,060 - INFO - Sleeping for 7 minutes and 24 seconds.
2024-10-21 09:20:08,061 - INFO - Input content - content='<!-- SC_OFF --><div class="md"><p>I have a quick question regarding the AAAI Phase 2 review process. Specifically, when do the reviewers need to submit their reviews?</p> <p>I’m considering submitting my paper to arXiv, but I’ve heard concerns that some reviewers can be biased toward specific research groups or first-time authors. I want to ensure that my paper\'s chances of acceptance aren\'t negatively affected by any potential biases. Once I upload my paper to arXiv, I understand that my anonymity is lost, which adds to my concerns.</p> <p>Any insights or advice on this topic would be greatly appreciated! Thanks in advance!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/morphinejunkie"> /u/morphinejunkie </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/">[comments]</a></span>'
2024-10-21 09:20:08,061 - INFO - clean content - clean_content="I have a quick question regarding the AAAI Phase 2 review process. Specifically, when do the reviewers need to submit their reviews? I’m considering submitting my paper to arXiv, but I’ve heard concerns that some reviewers can be biased toward specific research groups or first-time authors. I want to ensure that my paper's chances of acceptance aren't negatively affected by any potential biases. Once I upload my paper to arXiv, I understand that my anonymity is lost, which adds to my concerns. Any insights or advice on this topic would be greatly appreciated! Thanks in advance!   submitted by   /u/morphinejunkie [link]   [comments]"
2024-10-21 09:20:08,061 - INFO - Generating summary with BART model
2024-10-21 09:20:16,455 - INFO - Generating summary with BART model
2024-10-21 09:20:24,879 - INFO - full_tweet="I'm considering submitting my paper to arXiv, but I’ve heard concerns that some reviewers can be biased..[read more👇🏼] #I #paper https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/"
2024-10-21 09:20:25,148 - INFO - Tweet posted successfully: Response(data={'id': '1848293249686835518', 'text': "I'm considering submitting my paper to arXiv, but I’ve heard concerns that some reviewers can be biased..[read more👇🏼] #I #paper https://t.co/JNNK7AXJdK", 'edit_history_tweet_ids': ['1848293249686835518']}, includes={}, errors=[], meta={})
2024-10-21 09:20:25,149 - INFO - Saved posted URL: https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/ at 2024-10-21 09:20:25
2024-10-21 09:20:25,149 - INFO - Tweet link saved successfully.
2024-10-21 09:20:25,149 - INFO - Sleeping for 5 minutes and 5 seconds.
2024-10-21 09:25:30,149 - INFO - This link is already posted: https://www.techradar.com/pro/ai-and-creativity-the-dynamic-duo-fueling-business-growth
2024-10-21 09:25:30,149 - INFO - This link is already posted: https://venturebeat.com/ai/ibm-debuts-open-source-granite-3-0-llms-for-enterprise-ai/
2024-10-21 09:25:30,149 - INFO - Input content - content='<p>This is part of TechCrunch’s ongoing Women in AI series, which seeks to give AI-focused women academics and others their well-deserved time in the spotlight.</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>'
2024-10-21 09:25:30,149 - INFO - clean content - clean_content='This is part of TechCrunch’s ongoing Women in AI series, which seeks to give AI-focused women academics and others their well-deserved time in the spotlight. © 2024 TechCrunch. All rights reserved. For personal use only.'
2024-10-21 09:25:30,150 - INFO - Generating summary with BART model
2024-10-21 09:25:34,716 - INFO - Generating summary with BART model
2024-10-21 09:25:39,292 - INFO - full_tweet='This is part of TechCrunch’s ongoing Women in AI series. The series seeks to give AI-focused women..[read more👇🏼] #AI #series https://techcrunch.com/2024/10/20/women-in-ai-marissa-hummon-thinks-ai-will-help-make-the-power-grid-greener/'
2024-10-21 09:25:39,519 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1848294568309002503'], 'text': 'This is part of TechCrunch’s ongoing Women in AI series. The series seeks to give AI-focused women..[read more👇🏼] #AI #series https://t.co/rxN9lD1Njc', 'id': '1848294568309002503'}, includes={}, errors=[], meta={})
2024-10-21 09:25:39,520 - INFO - Saved posted URL: https://techcrunch.com/2024/10/20/women-in-ai-marissa-hummon-thinks-ai-will-help-make-the-power-grid-greener/ at 2024-10-21 09:25:39
2024-10-21 09:25:39,520 - INFO - Tweet link saved successfully.
2024-10-21 09:25:39,520 - INFO - Sleeping for 6 minutes and 43 seconds.
2024-10-21 09:32:22,520 - INFO - Input content - content="AI's ability to sift through massive amounts of data, identify patterns and constantly learn makes it invaluable in cybersecurity."
2024-10-21 09:32:22,520 - INFO - clean content - clean_content="AI's ability to sift through massive amounts of data, identify patterns and constantly learn makes it invaluable in cybersecurity."
2024-10-21 09:32:22,520 - INFO - Generating summary with BART model
2024-10-21 09:32:26,936 - INFO - Generating summary with BART model
2024-10-21 09:32:31,395 - INFO - full_tweet="AI's ability to sift through massive amounts of data, identify patterns and constantly learn makes it..[read more👇🏼] #data #AI https://venturebeat.com/security/the-ai-edge-in-cybersecurity-predictive-tools-aim-to-slash-response-times/"
2024-10-21 09:32:31,652 - INFO - Tweet posted successfully: Response(data={'id': '1848296296915824807', 'edit_history_tweet_ids': ['1848296296915824807'], 'text': "AI's ability to sift through massive amounts of data, identify patterns and constantly learn makes it..[read more👇🏼] #data #AI https://t.co/56vY8ubCRf"}, includes={}, errors=[], meta={})
2024-10-21 09:32:31,653 - INFO - Saved posted URL: https://venturebeat.com/security/the-ai-edge-in-cybersecurity-predictive-tools-aim-to-slash-response-times/ at 2024-10-21 09:32:31
2024-10-21 09:32:31,653 - INFO - Tweet link saved successfully.
2024-10-21 09:32:31,653 - INFO - Sleeping for 8 minutes and 56 seconds.
2024-10-21 09:41:27,653 - INFO - Input content - content='<p>AI companies claim to have robust safety checks in place that ensure that models don&#8217;t say or do weird, illegal, or unsafe stuff. But what if the models were capable of evading those checks and, for some reason, trying to sabotage or mislead users? Turns out they can do this, according to Anthropic researchers. Just [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>'
2024-10-21 09:41:27,653 - INFO - clean content - clean_content='AI companies claim to have robust safety checks in place that ensure that models don’t say or do weird, illegal, or unsafe stuff. But what if the models were capable of evading those checks and, for some reason, trying to sabotage or mislead users? Turns out they can do this, according to Anthropic researchers. Just […] © 2024 TechCrunch. All rights reserved. For personal use only.'
2024-10-21 09:41:27,654 - INFO - Generating summary with BART model
2024-10-21 09:41:33,710 - INFO - Generating summary with BART model
2024-10-21 09:41:39,784 - INFO - full_tweet='AI companies claim to have robust safety checks in place. But what if the models were capable of evading..[read more👇🏼] #AI #checks https://techcrunch.com/2024/10/20/can-ai-sandbag-safety-checks-to-sabotage-users-yes-but-not-very-well-for-now/'
2024-10-21 09:41:40,048 - INFO - Tweet posted successfully: Response(data={'id': '1848298596988240289', 'edit_history_tweet_ids': ['1848298596988240289'], 'text': 'AI companies claim to have robust safety checks in place. But what if the models were capable of evading..[read more👇🏼] #AI #checks https://t.co/aXNYsXNzx1'}, includes={}, errors=[], meta={})
2024-10-21 09:41:40,049 - INFO - Saved posted URL: https://techcrunch.com/2024/10/20/can-ai-sandbag-safety-checks-to-sabotage-users-yes-but-not-very-well-for-now/ at 2024-10-21 09:41:40
2024-10-21 09:41:40,049 - INFO - Tweet link saved successfully.
2024-10-21 09:41:40,049 - INFO - Sleeping for 7 minutes and 0 seconds.
2024-10-21 09:48:40,049 - INFO - Input content - content='<p>As founders plan for an increasingly AI-centric future, Gusto co-founder and head of technology Edward Kim said that cutting existing teams and hiring a bunch of specially trained AI engineers is &#8220;the wrong way to go.&#8221; Instead, he argued that non-technical team members can “actually have a much deeper understanding than an average engineer on [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>'
2024-10-21 09:48:40,050 - INFO - clean content - clean_content='As founders plan for an increasingly AI-centric future, Gusto co-founder and head of technology Edward Kim said that cutting existing teams and hiring a bunch of specially trained AI engineers is “the wrong way to go.” Instead, he argued that non-technical team members can “actually have a much deeper understanding than an average engineer on […] © 2024 TechCrunch. All rights reserved. For personal use only.'
2024-10-21 09:48:40,050 - INFO - Generating summary with BART model
2024-10-21 09:48:47,535 - INFO - Generating summary with BART model
2024-10-21 09:48:54,941 - INFO - full_tweet='Gusto co-founder and head of technology Edward Kim said that cutting existing teams and hiring a bunch of..[read more👇🏼] #technology #AI https://techcrunch.com/2024/10/20/gustos-head-of-technology-says-hiring-an-army-of-specialists-is-the-wrong-approach-to-ai/'
2024-10-21 09:48:55,239 - INFO - Tweet posted successfully: Response(data={'text': 'Gusto co-founder and head of technology Edward Kim said that cutting existing teams and hiring a bunch of..[read more👇🏼] #technology #AI https://t.co/m4ZjKAUY8o', 'edit_history_tweet_ids': ['1848300422206804077'], 'id': '1848300422206804077'}, includes={}, errors=[], meta={})
2024-10-21 09:48:55,240 - INFO - Saved posted URL: https://techcrunch.com/2024/10/20/gustos-head-of-technology-says-hiring-an-army-of-specialists-is-the-wrong-approach-to-ai/ at 2024-10-21 09:48:55
2024-10-21 09:48:55,240 - INFO - Tweet link saved successfully.
2024-10-21 09:48:55,240 - INFO - Sleeping for 8 minutes and 28 seconds.
2024-10-21 09:57:23,240 - INFO - Input content - content='<p>Not everyone is convinced of generative AI&#8217;s return on investment. But many investors are, judging by the latest figures from funding tracker PitchBook. In Q3 2024, VCs invested $3.9 billion in generative AI startups across 206 deals, per PitchBook. (That&#8217;s not counting OpenAI&#8216;s $6.6 billion round.) And $2.9 billion of that funding went to U.S.-based [&#8230;]</p>\n<p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>'
2024-10-21 09:57:23,240 - INFO - clean content - clean_content='Not everyone is convinced of generative AI’s return on investment. But many investors are, judging by the latest figures from funding tracker PitchBook. In Q3 2024, VCs invested $3.9 billion in generative AI startups across 206 deals, per PitchBook. (That’s not counting OpenAI‘s $6.6 billion round.) And $2.9 billion of that funding went to U.S.-based […] © 2024 TechCrunch. All rights reserved. For personal use only.'
2024-10-21 09:57:23,240 - INFO - Generating summary with BART model
2024-10-21 09:57:30,760 - INFO - Generating summary with BART model
2024-10-21 09:57:38,242 - INFO - full_tweet="In Q3 2024, VCs invested $3.9 billion in generative AI startups across 206 deals, per PitchBook. That's..[read more👇🏼] #AI #startups https://techcrunch.com/2024/10/20/investments-in-generative-ai-startups-topped-3-9b-in-q3-2024/"
2024-10-21 09:57:38,546 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1848302617245442201'], 'text': "In Q3 2024, VCs invested $3.9 billion in generative AI startups across 206 deals, per PitchBook. That's..[read more👇🏼] #AI #startups https://t.co/SBPutfexg3", 'id': '1848302617245442201'}, includes={}, errors=[], meta={})
2024-10-21 09:57:38,546 - INFO - Saved posted URL: https://techcrunch.com/2024/10/20/investments-in-generative-ai-startups-topped-3-9b-in-q3-2024/ at 2024-10-21 09:57:38
2024-10-21 09:57:38,547 - INFO - Tweet link saved successfully.
2024-10-21 09:57:38,547 - INFO - Sleeping for 9 minutes and 52 seconds.
2024-10-21 10:07:30,547 - INFO - Input content - content='<table> <tr><td> <a href="https://www.reddit.com/r/MachineLearning/comments/1g7yzh8/d_last_week_in_medical_ai_top_llm_research/"> <img alt="[D] Last Week in Medical AI: Top LLM Research Papers/Models (October 12 - October 19) " src="https://a.thumbs.redditmedia.com/hyfY2SIaaxgn0EDiJWa08cDMMLSOkKJGkZKey-UI8J8.jpg" title="[D] Last Week in Medical AI: Top LLM Research Papers/Models (October 12 - October 19) " /> </a> </td><td> <!-- SC_OFF --><div class="md"><p><a href="https://preview.redd.it/u2p81gvo0xvd1.jpg?width=1386&amp;format=pjpg&amp;auto=webp&amp;s=22b0099b069104750a4c7bac0f99e4c60dc72912">Last Week in Medical AI: Top LLM Research Papers/Models (October 12 - October 19)</a></p> <p><strong>Medical LLM &amp; Other Model</strong></p> <ul> <li><strong>OLAPH: Factual Biomedical LLM QA</strong> <ul> <li>This paper introduces MedLFQA, a benchmark dataset for evaluating the factuality of long-for answers generated by large language models (LLMs) in the medical domain.</li> </ul></li> <li><strong>LLMD: Interpreting Longitudinal Medical Records</strong> <ul> <li>This paper introduces LLMD, a large language model designed to analyze patient medical history.</li> </ul></li> <li><strong>LifeGPT: Generative Transformer for Cells</strong> <ul> <li>This paper introduces LifeGPT, a decoder-only generative pretrained transformer (GPT) model trained to simulate Conway\'s Game of Life on a toroidal grid without prior knowledge of grid size or boundary conditions.</li> </ul></li> <li><strong>MedCare: Decoupled Clinical LLM Alignment</strong> <ul> <li>This paper introduces MedCare, a Medical LLM that leverages a progressive fine-tuning pipeline to address knowledge-intensive and alignment-required tasks in medical NLP.</li> </ul></li> <li>Y-Mol: Biomedical LLM for Drug Development <ul> <li>This paper introduces Y-Mol, a multiscale biomedical knowledge-guided large language model (LLM) designed for drug development tasks spanning lead compound discovery, pre-clinic, and clinic prediction.</li> </ul></li> </ul> <p><strong>Frameworks and Methodologies:</strong></p> <ul> <li>MedINST: Biomedical Instructions Meta Dataset</li> <li>Democratizing Medical LLMs via Language Experts</li> <li>MCQG-SRefine: Iterative Question Generation</li> <li>Adaptive Medical Language Agents</li> <li>MeNTi: Medical LLM with Nested Tools</li> </ul> <p><strong>Medical LLM Applications:</strong></p> <ul> <li>AGENTiGraph: LLM Chatbots with Private Data</li> <li>MMed-RAG: Multimodal Medical RAG System</li> <li>Medical Graph RAG: Safe LLM via Retrieval</li> <li>MedAide: Multi-Agent Medical LLM Collaboration</li> <li>Synthetic Clinical Trial Generation</li> </ul> <p><strong>Medical LLMs &amp; Benchmarks:</strong></p> <ul> <li>WorldMedQA-V: Multimodal Medical LLM Dataset</li> <li>HEALTH-PARIKSHA: RAG Models Evaluation</li> <li>Synthetic Data for Medical Vision-Language</li> <li>....</li> </ul> <p>...</p> <p>Full thread in detail: <a href="https://x.com/OpenlifesciAI/status/1847686504837202263">https://x.com/OpenlifesciAI/status/1847686504837202263</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/aadityaura"> /u/aadityaura </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1g7yzh8/d_last_week_in_medical_ai_top_llm_research/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1g7yzh8/d_last_week_in_medical_ai_top_llm_research/">[comments]</a></span> </td></tr></table>'
2024-10-21 10:07:30,547 - INFO - clean content - clean_content="Last Week in Medical AI: Top LLM Research Papers/Models (October 12 - October 19) Medical LLM & Other Model OLAPH: Factual Biomedical LLM QA This paper introduces MedLFQA, a benchmark dataset for evaluating the factuality of long-for answers generated by large language models (LLMs) in the medical domain. LLMD: Interpreting Longitudinal Medical Records This paper introduces LLMD, a large language model designed to analyze patient medical history. LifeGPT: Generative Transformer for Cells This paper introduces LifeGPT, a decoder-only generative pretrained transformer (GPT) model trained to simulate Conway's Game of Life on a toroidal grid without prior knowledge of grid size or boundary conditions. MedCare: Decoupled Clinical LLM Alignment This paper introduces MedCare, a Medical LLM that leverages a progressive fine-tuning pipeline to address knowledge-intensive and alignment-required tasks in medical NLP. Y-Mol: Biomedical LLM for Drug Development This paper introduces Y-Mol, a multiscale biomedical knowledge-guided large language model (LLM) designed for drug development tasks spanning lead compound discovery, pre-clinic, and clinic prediction. Frameworks and Methodologies: MedINST: Biomedical Instructions Meta Dataset Democratizing Medical LLMs via Language Experts MCQG-SRefine: Iterative Question Generation Adaptive Medical Language Agents MeNTi: Medical LLM with Nested Tools Medical LLM Applications: AGENTiGraph: LLM Chatbots with Private Data MMed-RAG: Multimodal Medical RAG System Medical Graph RAG: Safe LLM via Retrieval MedAide: Multi-Agent Medical LLM Collaboration Synthetic Clinical Trial Generation Medical LLMs & Benchmarks: WorldMedQA-V: Multimodal Medical LLM Dataset HEALTH-PARIKSHA: RAG Models Evaluation Synthetic Data for Medical Vision-Language .... ... Full thread in detail:   submitted by   /u/aadityaura [link]   [comments]"
2024-10-21 10:07:30,548 - INFO - Generating summary with BART model
2024-10-21 10:07:38,373 - INFO - Generating summary with BART model
2024-10-21 10:07:46,136 - INFO - full_tweet='Last Week in Medical AI: Top LLM Research Papers/Models (October 12 - October 19) Medical LLM & Other..[read more👇🏼] #AI #Medical https://www.reddit.com/r/MachineLearning/comments/1g7yzh8/d_last_week_in_medical_ai_top_llm_research/'
2024-10-21 10:07:46,418 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1848305166878970160'], 'text': 'Last Week in Medical AI: Top LLM Research Papers/Models (October 12 - October 19) Medical LLM &amp; Other..[read more👇🏼] #AI #Medical https://t.co/soqmTf1n8n', 'id': '1848305166878970160'}, includes={}, errors=[], meta={})
2024-10-21 10:07:46,419 - INFO - Saved posted URL: https://www.reddit.com/r/MachineLearning/comments/1g7yzh8/d_last_week_in_medical_ai_top_llm_research/ at 2024-10-21 10:07:46
2024-10-21 10:07:46,419 - INFO - Tweet link saved successfully.
2024-10-21 10:07:46,419 - INFO - Sleeping for 9 minutes and 14 seconds.
2024-10-21 10:17:00,419 - INFO - Input content - content='<h4>Not because we’re curious. Because we need to get shit\xa0done.</h4><p>Are explanations important to AI model outputs important?</p><p>My first answer to this is: <strong>not\xa0really</strong>.</p><p>When an explanation is a rhetorical exercise to impress me you had your reasons for a decision, I’d call that bells and whistles with no impact. If I’m waiting for a cancer diagnosis based on my MRI, I’m much more interested in improving accuracy from 80% to 99% than in seeing a compelling image which shows where the evidence lies. After all, it may take an expert to even recognize the evidence the evidence. Or worse, the evidence might be diffuse, spread across millions of pixels, so no human mind can comprehend it. Chasing explanations just to feel good about trusting the AI is pointless. We should measure correctness, and if the math shows the results exceed human performance, explanations are unnecessary.</p><p>But, sometimes an explanation are more than a rhetorical exercise. Here’s when explanations matter:</p><ol><li>When accuracy is crucial, and a human can <em>verify</em> the results. Then expalnations can let you bring down the error levels, e.g. from 1% to\xa00.01%.</li><li>When the raw prediction isn’t really all you care about, and the explanation generates useful actions. For example, saying “somewhere in this contract there’s an unfair clause”, isn’t useful as showing exactly where this unfair clause shows up. Hihglighting the unfair clause lets us take action, like propose an edit to the contract.</li></ol><h3>When the explanation is more important than the\xa0answer</h3><p>Let’s double click on a concrete example from <a href="https://www.docupanda.io/">DocuPanda</a>, a service I’ve cofounded. In a nutshell, what we do is let users map complex documents into a JSON payload that contains a consistent, correct\xa0output</p><p>So maybe we scan an entire rental lease, and emit a short JSON: {“monthlyRentAmount”: 2000, “dogsAllowed”\xa0:\xa0true}.</p><p>To make it very concrete, <a href="https://docupanda-marketing-assets.s3.amazonaws.com/lease2.pdf">here’s all 51 pages of my lease</a> from my time in Berkeley, California.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DgZovohDnYupZ_KfjSEb2g.png" /><figcaption>Yeah, rent in Bay Area is insane, thanks for\xa0asking</figcaption></figure><p>If you’re not from the US, you might be shocked it takes 51 pages to spell out “You’re gonna pay $3700 a month, you get to live here in exchange”. I think it might not be necessary <a href="https://scholarship.law.tamu.edu/facscholar/302/">legally</a>, but I\xa0digress.</p><p>Now, using Docupanda, we can get to bottom line answers like\u200a—\u200awhat’s the rental amount, and can I take my dog to live there, what’s the start date,\xa0etc.</p><p>Let’s take a look at the JSON we\xa0extract</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Vamp-5IGGVLuDNly9kvKSw.png" /><figcaption>So apparently Roxy can’t come live with\xa0me</figcaption></figure><p>If you look all the way at the bottom, we have a flag to indicate that pets are disallowed, along with a description of the exception spelled out in the\xa0lease.</p><p>There are two reasons explainability would be useful\xa0here:</p><ol><li>Maybe it’s crucial that we get this right. By reviewing the paragraph I can make sure that we understand the policy correctly.</li><li>Maybe I want to propose an edit. Just knowing that somewhere in these 51 pages there’s a pet prohibition doesn’t really help\u200a—\u200aI’ll still have to go over all pages to propose an\xa0edit.</li></ol><p>So here’s how we solve for this. Rather than just giving you a black box with a dollar amount, a true/false result, etc\u200a—\u200awe’ve designed DocuPanda to ground its prediction in precise pixels. You can click on a result, and scroll to the exact page and section that justifies our prediction.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RbFXHTTm0dYXR0eHqPuzLw.gif" /><figcaption>Clicking on “pets allowed = false” immediately scrolls to the relevant page where it says “no mammal pets\xa0etc”</figcaption></figure><h3>Explanation-Driven Workflows</h3><p>At DocuPanda, we’ve observed three overall paradigms for how explainability is\xa0used.</p><h4>Explanations Drive\xa0Accuracy</h4><p>The first paradigm, which we expected from the outset, is that explainability can reduce errors and validate predictions. When you have an invoice for $12,000, you really want a human to ensure the number is valid and not taken out of context, because the stakes are too high if this figure feeds into accounting automation software.</p><p>It’s a great property of document processing, that humans are very good at it. We cost a lot, but we know what we’re doing. This puts us in the happy band where humans can <strong>verify results</strong> very efficiently, often reducing error rates signifcantly.</p><h4>Explanations drive high-knowledge worker productivity</h4><p>This paradigm arose naturally from our user base, and we didn’t entirely anticipate it at first. It turns out that Sometimes, more than we want the raw answer to a question, we want to leverage AI to get the right information in front of our eyes. I already hinted at this use case if you consider an output like {“unfair payment terms”: true}\u200a—\u200athis is hardly useful compared to showing what language of the contract make it\xa0unfair.</p><p>As a more complete example, consider a bio research company that wants to scour every biological publication to identify processes that increase sugar production in potatoes. They use DocuPanda to answer fields\xa0like:</p><p>{sugarProductionLowered: true, sugarProductionGenes: [“AP2a”,”TAGL1&quot;]}</p><p>Their goal is <strong>not </strong>to blindly trust DocuPanda and count how many papers mention a gene or something like that. The thing that makes this result useful is that researcher can click around to get right to the gist of the paper. By clicking on the gene names, a researcher can immediately jump in to context where the gene got mentioned\u200a—\u200aand reason about whether the process described in this paper involving this gene and sugar is relevant to their research goal or not. This is an example where the explanation is more important than the raw answer, and can boost the productivity of very high knowledge workers.</p><h4>Explanations for liability purposes</h4><p>There’s another reason to use explanations and leverage them to put a human in the loop. In addition to reducing error rates (often), they let you demonstrate that you have a <strong>reasonable, legally compliant process</strong> in\xa0place.</p><p>Regulators care about process. A black box that emits mistakes is not a sound process. The ability to trace every extracted data point back to the original source lets you put a human in the loop to review and approve results. Even if the human doesn’t reduce errors, having that person involved can be legally useful. It shifts the process from being blind automation, for which your company is responsible, to one driven by humans, who have an acceptable rate of clerical errors. A related example is that it looks like regulators and public opinion tolerate a far lower rate of fatal car crashes, measured per-mile, when discussing a fully automated system, vs human driving-assistance tools. I personally find this to be morally unjustifiable, but I don’t make the rules, and we have to play by\xa0them.</p><p>By giving you the ability to put a human in the loop, you move from a legally tricky minefield of full automation, with the legal exposure it entails, to the more familiar legal territory of a human analyst using a 10x speed and productivity tool (and making occasional mistakes like the rest of us sinners).</p><blockquote>all images are owned by the\xa0author</blockquote><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=840144df418e" width="1" /><hr /><p><a href="https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e">Why Explainability Matters in AI</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>'
2024-10-21 10:17:00,421 - INFO - clean content - clean_content='Not because we’re curious. Because we need to get **** done.Are explanations important to AI model outputs important?My first answer to this is: not really.When an explanation is a rhetorical exercise to impress me you had your reasons for a decision, I’d call that bells and whistles with no impact. If I’m waiting for a cancer diagnosis based on my MRI, I’m much more interested in improving accuracy from 80% to 99% than in seeing a compelling image which shows where the evidence lies. After all, it may take an expert to even recognize the evidence the evidence. Or worse, the evidence might be diffuse, spread across millions of pixels, so no human mind can comprehend it. Chasing explanations just to feel good about trusting the AI is pointless. We should measure correctness, and if the math shows the results exceed human performance, explanations are unnecessary.But, sometimes an explanation are more than a rhetorical exercise. Here’s when explanations matter:When accuracy is crucial, and a human can verify the results. Then expalnations can let you bring down the error levels, e.g. from 1% to 0.01%.When the raw prediction isn’t really all you care about, and the explanation generates useful actions. For example, saying “somewhere in this contract there’s an unfair clause”, isn’t useful as showing exactly where this unfair clause shows up. Hihglighting the unfair clause lets us take action, like propose an edit to the contract.When the explanation is more important than the answerLet’s double click on a concrete example from DocuPanda, a service I’ve cofounded. In a nutshell, what we do is let users map complex documents into a JSON payload that contains a consistent, correct outputSo maybe we scan an entire rental lease, and emit a short JSON: {“monthlyRentAmount”: 2000, “dogsAllowed” : true}.To make it very concrete, here’s all 51 pages of my lease from my time in Berkeley, California.Yeah, rent in Bay Area is insane, thanks for askingIf you’re not from the US, you might be shocked it takes 51 pages to spell out “You’re gonna pay $3700 a month, you get to live here in exchange”. I think it might not be necessary legally, but I digress.Now, using Docupanda, we can get to bottom line answers like — what’s the rental amount, and can I take my dog to live there, what’s the start date, etc.Let’s take a look at the JSON we extractSo apparently Roxy can’t come live with meIf you look all the way at the bottom, we have a flag to indicate that pets are disallowed, along with a description of the exception spelled out in the lease.There are two reasons explainability would be useful here:Maybe it’s crucial that we get this right. By reviewing the paragraph I can make sure that we understand the policy correctly.Maybe I want to propose an edit. Just knowing that somewhere in these 51 pages there’s a pet prohibition doesn’t really help — I’ll still have to go over all pages to propose an edit.So here’s how we solve for this. Rather than just giving you a black box with a dollar amount, a true/false result, etc — we’ve designed DocuPanda to ground its prediction in precise pixels. You can click on a result, and scroll to the exact page and section that justifies our prediction.Clicking on “pets allowed = false” immediately scrolls to the relevant page where it says “no mammal pets etc”Explanation-Driven WorkflowsAt DocuPanda, we’ve observed three overall paradigms for how explainability is used.Explanations Drive AccuracyThe first paradigm, which we expected from the outset, is that explainability can reduce errors and validate predictions. When you have an invoice for $12,000, you really want a human to ensure the number is valid and not taken out of context, because the stakes are too high if this figure feeds into accounting automation software.It’s a great property of document processing, that humans are very good at it. We cost a lot, but we know what we’re doing. This puts us in the happy band where humans can verify results very efficiently, often reducing error rates signifcantly.Explanations drive high-knowledge worker productivityThis paradigm arose naturally from our user base, and we didn’t entirely anticipate it at first. It turns out that Sometimes, more than we want the raw answer to a question, we want to leverage AI to get the right information in front of our eyes. I already hinted at this use case if you consider an output like {“unfair payment terms”: true} — this is hardly useful compared to showing what language of the contract make it unfair.As a more complete example, consider a bio research company that wants to scour every biological publication to identify processes that increase sugar production in potatoes. They use DocuPanda to answer fields like:{sugarProductionLowered: true, sugarProductionGenes: [“AP2a”,”TAGL1"]}Their goal is not to blindly trust DocuPanda and count how many papers mention a gene or something like that. The thing that makes this result useful is that researcher can click around to get right to the gist of the paper. By clicking on the gene names, a researcher can immediately jump in to context where the gene got mentioned — and reason about whether the process described in this paper involving this gene and sugar is relevant to their research goal or not. This is an example where the explanation is more important than the raw answer, and can boost the productivity of very high knowledge workers.Explanations for liability purposesThere’s another reason to use explanations and leverage them to put a human in the loop. In addition to reducing error rates (often), they let you demonstrate that you have a reasonable, legally compliant process in place.Regulators care about process. A black box that emits mistakes is not a sound process. The ability to trace every extracted data point back to the original source lets you put a human in the loop to review and approve results. Even if the human doesn’t reduce errors, having that person involved can be legally useful. It shifts the process from being blind automation, for which your company is responsible, to one driven by humans, who have an acceptable rate of clerical errors. A related example is that it looks like regulators and public opinion tolerate a far lower rate of fatal car crashes, measured per-mile, when discussing a fully automated system, vs human driving-assistance tools. I personally find this to be morally unjustifiable, but I don’t make the rules, and we have to play by them.By giving you the ability to put a human in the loop, you move from a legally tricky minefield of full automation, with the legal exposure it entails, to the more familiar legal territory of a human analyst using a 10x speed and productivity tool (and making occasional mistakes like the rest of us sinners).all images are owned by the authorWhy Explainability Matters in AI was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.'
2024-10-21 10:17:00,421 - INFO - Generating summary with BART model
2024-10-21 10:17:08,646 - INFO - Generating summary with BART model
2024-10-21 10:17:16,839 - INFO - full_tweet='When an explanation is a rhetorical exercise to impress me you had your reasons for a decision, I’d call..[read more👇🏼] #AI #explanations https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e?source=rss----7f60cf5620c9---4'
2024-10-21 10:17:17,117 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1848307560501174444'], 'id': '1848307560501174444', 'text': 'When an explanation is a rhetorical exercise to impress me you had your reasons for a decision, I’d call..[read more👇🏼] #AI #explanations https://t.co/h0IcMRPB6b'}, includes={}, errors=[], meta={})
2024-10-21 10:17:17,117 - INFO - Saved posted URL: https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e?source=rss----7f60cf5620c9---4 at 2024-10-21 10:17:17
2024-10-21 10:17:17,117 - INFO - Tweet link saved successfully.
2024-10-21 10:17:17,117 - INFO - Sleeping for 9 minutes and 4 seconds.
2024-10-21 10:26:21,117 - INFO - Input content - content='Look we get it, it’s the new buzz word – but you don’t need to strap it to everything'
2024-10-21 10:26:21,117 - INFO - clean content - clean_content='Look we get it, it’s the new buzz word – but you don’t need to strap it to everything'
2024-10-21 10:26:21,118 - INFO - Generating summary with BART model
2024-10-21 10:26:27,866 - INFO - Generating summary with BART model
2024-10-21 10:26:34,624 - INFO - full_tweet='We get it, it’s the new buzz word – but you don’t need to strap it to everything. We get it we get it...[read more👇🏼] #get #everything https://www.techradar.com/computing/artificial-intelligence/ai-marketing-is-a-con-especially-when-it-comes-to-cpus'
2024-10-21 10:26:34,892 - INFO - Tweet posted successfully: Response(data={'id': '1848309900050325920', 'text': 'We get it, it’s the new buzz word – but you don’t need to strap it to everything. We get it we get it...[read more👇🏼] #get #everything https://t.co/2geRWepQib', 'edit_history_tweet_ids': ['1848309900050325920']}, includes={}, errors=[], meta={})
2024-10-21 10:26:34,893 - INFO - Saved posted URL: https://www.techradar.com/computing/artificial-intelligence/ai-marketing-is-a-con-especially-when-it-comes-to-cpus at 2024-10-21 10:26:34
2024-10-21 10:26:34,893 - INFO - Tweet link saved successfully.
2024-10-21 10:26:34,893 - INFO - Sleeping for 7 minutes and 12 seconds.

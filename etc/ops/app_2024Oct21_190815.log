2024-10-21 19:08:15,867 - INFO - Starting configuration setup
2024-10-21 19:08:15,868 - INFO - Environment variables loaded successfully
2024-10-21 19:08:15,868 - INFO - Twitter API client initialized successfully
2024-10-21 19:08:17,731 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-10-21 19:08:25,091 - INFO - Summarization model initialized successfully
2024-10-21 19:08:25,091 - INFO - Starting tweet scheduling
2024-10-21 19:08:25,091 - INFO - Tweeting process started at 2024-10-21 19:08:25.091475!
2024-10-21 19:08:25,091 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Oct21.csv
2024-10-21 19:08:25,091 - INFO - Fetching latest tech news from RSS feeds
2024-10-21 19:08:40,509 - INFO - Total entries found: 614
2024-10-21 19:08:40,514 - INFO - Recent AI-related entries found: 23
2024-10-21 19:08:40,514 - INFO - Input content - content='Apple Intelligence launches next week bringing AI features to your iPhone, iPad, and Mac.'
2024-10-21 19:08:40,515 - INFO - clean content - clean_content='Apple Intelligence launches next week bringing AI features to your iPhone, iPad, and Mac.'
2024-10-21 19:08:40,515 - INFO - Generating summary with BART model
2024-10-21 19:08:45,489 - INFO - Generating summary with BART model
2024-10-21 19:08:49,822 - INFO - full_tweet='Apple Intelligence launches next week bringing AI features to your iPhone, iPad, and Mac. Apple..[read moreüëáüèº] #AI #Intelligence https://www.techradar.com/phones/iphone/apple-intelligence-releases-next-week-with-ios-18-1-here-are-all-the-ai-features-you-can-try-at-launch'
2024-10-21 19:08:50,079 - INFO - Tweet posted successfully: Response(data={'id': '1848441329279193452', 'text': 'Apple Intelligence launches next week bringing AI features to your iPhone, iPad, and Mac. Apple..[read moreüëáüèº] #AI #Intelligence https://t.co/FLXOxkGOru', 'edit_history_tweet_ids': ['1848441329279193452']}, includes={}, errors=[], meta={})
2024-10-21 19:08:50,080 - INFO - Saved posted URL: https://www.techradar.com/phones/iphone/apple-intelligence-releases-next-week-with-ios-18-1-here-are-all-the-ai-features-you-can-try-at-launch at 2024-10-21 19:08:50
2024-10-21 19:08:50,080 - INFO - Tweet link saved successfully.
2024-10-21 19:08:50,080 - INFO - Sleeping for 4 minutes and 42 seconds.
2024-10-21 19:13:32,080 - INFO - Input content - content='<!-- SC_OFF --><div class="md"><p>I am in my 5th sem.please help me </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Glittering-Tell-8963"> /u/Glittering-Tell-8963 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1g8x4go/discussioni_am_a_btech_student_cse_and_i_wanted/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1g8x4go/discussioni_am_a_btech_student_cse_and_i_wanted/">[comments]</a></span>'
2024-10-21 19:13:32,080 - INFO - clean content - clean_content='I am in my 5th sem.please help me   submitted by   /u/Glittering-Tell-8963 [link]   [comments]'
2024-10-21 19:13:32,080 - INFO - Generating summary with BART model
2024-10-21 19:13:36,509 - INFO - Generating summary with BART model
2024-10-21 19:13:40,913 - INFO - full_tweet='please help me   submitted by   /u/Glittering-Tell-8963 [link]   [comments] I am in my 5th sem. #Glittering #submitted https://www.reddit.com/r/MachineLearning/comments/1g8x4go/discussioni_am_a_btech_student_cse_and_i_wanted/'
2024-10-21 19:13:41,162 - INFO - Tweet posted successfully: Response(data={'id': '1848442550190801268', 'text': 'please help me   submitted by   /u/Glittering-Tell-8963 [link]   [comments] I am in my 5th sem. #Glittering #submitted https://t.co/VyZBsYLgqv', 'edit_history_tweet_ids': ['1848442550190801268']}, includes={}, errors=[], meta={})
2024-10-21 19:13:41,162 - INFO - Saved posted URL: https://www.reddit.com/r/MachineLearning/comments/1g8x4go/discussioni_am_a_btech_student_cse_and_i_wanted/ at 2024-10-21 19:13:41
2024-10-21 19:13:41,162 - INFO - Tweet link saved successfully.
2024-10-21 19:13:41,163 - INFO - Sleeping for 7 minutes and 27 seconds.
2024-10-21 19:21:08,163 - INFO - Input content - content='<h4>Tips for accelerating ML with AWS Neuron\xa0SDK</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2Rv2SBFg0AgINmyy" /><figcaption>Photo by <a href="https://unsplash.com/@julientromeur?utm_source=medium&amp;utm_medium=referral">julien Tromeur</a> on\xa0<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>We are in a golden age of AI, with cutting-edge models disrupting industries and poised to transform life as we know it. Powering these advancements are increasingly powerful AI accelerators, such as <a href="https://www.nvidia.com/en-eu/data-center/h100/">NVIDIA H100 GPUs</a>, <a href="https://cloud.google.com/tpu">Google Cloud TPUs</a>, <a href="https://aws.amazon.com/machine-learning/trainium/">AWS‚Äôs Trainium</a> and <a href="https://aws.amazon.com/machine-learning/inferentia/">Inferentia</a> chips, and more. With the growing number of options comes the challenge of <a href="https://towardsdatascience.com/instance-selection-for-deep-learning-7463d774cff0">selecting the most optimal platform</a> for our machine learning (ML) workloads\u200a‚Äî\u200aa crucial decision considering the high costs associated with AI computation. Importantly, a comprehensive assessment of each option necessitates ensuring that we are maximizing its utilization to fully leverage its capabilities.</p><p>In this post, we will review several techniques for optimizing an ML workload on AWS‚Äôs custom-built AI chips using the <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html">AWS Neuron SDK</a>. This continues our ongoing series of posts focused on ML model performance analysis and optimization across various platforms and environments (e.g., see <a href="https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-10c3c5822869">here</a> and <a href="https://towardsdatascience.com/training-ai-models-on-cpu-3903adc9f388">here</a>). While our primary focus will be on an ML training workload and AWS Inferentia2, the techniques discussed are also applicable to AWS Trainium. (Recall that although AWS Inferentia is primarily designed as an AI inference chip, we have <a href="https://towardsdatascience.com/dl-training-on-aws-inferentia-53e103597a03">previously demonstrated</a> its effectiveness in training tasks as\xa0well.)</p><p>Generally speaking, performance optimization is an iterative process that includes a performance analysis step to appropriately identify performance bottlenecks and resource under-utilization (e.g., see <a href="https://towardsdatascience.com/cloud-ml-performance-checklist-caa51e798002">here</a>). However, since the techniques we will discuss are general purpose (i.e., they are potentially applicable to any model, regardless of their performance profile), we defer the discussion on <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/torch-neuronx-profiling-dev-guide.html">performance analysis with the Neuron SDK</a> to a future\xa0post.</p><h4>Disclaimers</h4><p>The code we will share is intended for demonstrative purposes only\u200a‚Äî\u200awe make no claims regarding its accuracy, optimality, or robustness. Please do not view this post as a substitute for the official <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html">Neuron SDK documentation</a>. Please do not interpret our mention of any platforms, libraries, or optimization techniques as an endorsement for their use. The best options for you will depend greatly on the specifics of your use-case and will require your own in-depth investigation and analysis.</p><p>The experiments described below were run on an <a href="https://aws.amazon.com/ec2/instance-types/inf2/">Amazon EC2 inf2.xlarge</a> instance (containing two <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch">Neuron cores</a> and four vCPUs). We used the most recent version of the <a href="https://aws.amazon.com/releasenotes/aws-deep-learning-ami-neuron-ubuntu-22-04/">Deep Learning AMI for Neuron</a> available at the time of this writing, ‚ÄúDeep Learning AMI Neuron (Ubuntu 22.04) 20240927‚Äù, with <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html">AWS Neuron 2.20</a> and <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuronx/introducing-pytorch-2-1.html">PyTorch 2.1</a>. See the <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/multiframework/multi-framework-ubuntu22-neuron-dlami.html#setup-ubuntu22-multi-framework-dlami">SDK documentation</a> for more details on setup and installation. Keep in mind that the Neuron SDK is under active development and that the APIs we refer to, as well as the runtime measurements we report, may become outdated by the time you read this. Please be sure to stay up-to-date with the latest SDK and documentation available.</p><h3>Toy Model</h3><p>To facilitate our discussion, we introduce the following simple <a href="https://en.wikipedia.org/wiki/Vision_transformer">Vision Transformer</a> (ViT)-backed classification model (based on <a href="https://pypi.org/project/timm/">timm</a> version\xa01.0.10):</p><pre>from torch.utils.data import Dataset<br />import time, os<br />import torch<br />import torch_xla.core.xla_model as xm<br />import torch_xla.distributed.parallel_loader as pl<br />from timm.models.vision_transformer import VisionTransformer<br /><br /># use random data<br />class FakeDataset(Dataset):<br />  def __len__(self):<br />    return 1000000<br />  <br />  def __getitem__(self, index):<br />    rand_image = torch.randn([3, 224, 224], dtype=torch.float32)<br />    label = torch.tensor(data=index % 1000, dtype=torch.int64)<br />    return rand_image, label<br /><br />def train(batch_size=16, num_workers=0):<br />  # Initialize XLA process group for torchrun<br />  import torch_xla.distributed.xla_backend<br />  torch.distributed.init_process_group(\'xla\')<br /><br />  # multi-processing: ensure each worker has same initial weights<br />  torch.manual_seed(0)<br />  dataset = FakeDataset()<br />  model = VisionTransformer()<br /><br />  # load model to XLA device<br />  device = xm.xla_device()<br />  model = model.to(device)<br />  optimizer = torch.optim.Adam(model.parameters())<br />  data_loader = torch.utils.data.DataLoader(dataset,<br />                                            batch_size=batch_size,<br />                                            num_workers=num_workers)<br /><br />  data_loader = pl.MpDeviceLoader(data_loader, device)<br />  loss_function = torch.nn.CrossEntropyLoss()<br />  summ = 0<br />  count = 0<br />  t0 = time.perf_counter()<br /><br />  for step, (inputs, targets) in enumerate(data_loader, start=1):<br />    optimizer.zero_grad()<br />    outputs = model(inputs)<br />    loss = loss_function(outputs, targets)<br />    loss.backward()<br />    xm.optimizer_step(optimizer)<br />    batch_time = time.perf_counter() - t0<br />    if step &gt; 10:  # skip first steps<br />      summ += batch_time<br />      count += 1<br />    t0 = time.perf_counter()<br />    if step &gt; 500:<br />      break<br />  print(f\'average step time: {summ/count}\')<br /><br />if __name__ == \'__main__\':<br />  train()<br /><br /># Initialization command:<br /># torchrun --nproc_per_node=2 train.py</pre><p>Running our baseline model on the two cores of our AWS Inferentia instance, results in a training speed of 251.98 samples per\xa0second.</p><p>In the next sections, we will iteratively apply a number of potential optimization techniques and assess their impact on step time performance. While we won‚Äôt go into the full details of each method, we will provide references for further reading (e.g., <a href="https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-10c3c5822869">here</a>). Importantly, the list we will present is not all-inclusive\u200a‚Äî\u200athere are many techniques beyond what we will cover. We will organize the methods into three categories: PyTorch optimizations, OpenXLA optimizations, and Neuron-specific optimizations. However, the order of presentation is not binding. In fact, some of the techniques are interdependent\u200a‚Äî\u200afor example, applying the mixed precision optimization may free up enough device memory to enable increasing the batch\xa0size.</p><h3>PyTorch Performance Optimizations</h3><p>In previous posts (e.g., <a href="https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-10c3c5822869">here</a>) we have covered the topic of PyTorch model performance analysis and optimization on GPU, extensively. Many of the techniques we discussed are relevant to other AI accelerators. In this section we will revisit few of these techniques and apply them to AWS Inferentia.</p><h4>Multi-process Data\xa0Loading</h4><p>In <a href="https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading">multi process data loading</a> the input data is prepared in one or more dedicated CPU processes rather than in the same process that runs the training step. This allows for overlapping the data loading and training which can increase system utilization and lead to a significant speed-up. The number of processes is controlled by the <em>num_workers</em> parameter of the <a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">PyTorch DataLoader</a>. In the following block we run our script with <em>num_workers </em>set to\xa0one:</p><pre>train(num_workers=1)</pre><p>This change results in a training speed of 253.56 samples per second for a boost of less than\xa01%.</p><h4>Batch Size Optimization</h4><p>Another important hyperparameter that can influence training speed is the training batch size. Often, we have found that increasing the batch size improves system utilization and results in better performance. However, the effects can vary based on the model and platform. In the case of our toy model on AWS Inferentia, we find that running with a batch size of 8 samples per neuron core results in a speed of 265.68 samples per second\u200a‚Äî\u200aroughly 5% faster than a batch size of 16 samples per\xa0core.</p><pre>train(batch_size=8, num_workers=1)</pre><h4>PyTorch Automatic Mixed Precision</h4><p>Another common method for boosting performance is to use lower precision floats such as the 16-bit BFloat16. Importantly, some model components might not be compatible with reduced precision floats. PyTorch‚Äôs <a href="https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html">Automatic Mixed Precision (AMP)</a> mode attempts to match the most appropriate floating point type to each model operation automatically. Although, the Neuron compiler offers different options for employing <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#mixed-precision-and-performance-accuracy-tuning-neuronx-cc">mixed precision</a>, it also <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#automatic-mixed-precision">supports the option of using PyTorch AMP</a>. In the code block below we include the modifications required to use PyTorch\xa0AMP.</p><pre>def train(batch_size=16, num_workers=0):<br />  # Initialize XLA process group for torchrun<br />  import torch_xla.distributed.xla_backend<br />  torch.distributed.init_process_group(\'xla\')<br /><br />  # multi-processing: ensure each worker has same initial weights<br />  torch.manual_seed(0)<br />  dataset = FakeDataset()<br />  model = VisionTransformer()<br /><br />  # load model to XLA device<br />  device = xm.xla_device()<br />  model = model.to(device)<br />  optimizer = torch.optim.Adam(model.parameters())<br />  data_loader = torch.utils.data.DataLoader(dataset,<br />                                            batch_size=batch_size,<br />                                            num_workers=num_workers)<br /><br />  data_loader = pl.MpDeviceLoader(data_loader, device)<br />  loss_function = torch.nn.CrossEntropyLoss()<br />  summ = 0<br />  count = 0<br />  t0 = time.perf_counter()<br /><br />  for step, (inputs, targets) in enumerate(data_loader, start=1):<br />    optimizer.zero_grad()<br /><br />    # use PyTorch AMP<br />    with torch.autocast(dtype=torch.bfloat16, device_type=\'cuda\'):<br />      outputs = model(inputs)<br />      loss = loss_function(outputs, targets)<br />    loss.backward()<br />    xm.optimizer_step(optimizer)<br />    batch_time = time.perf_counter() - t0<br />    if step &gt; 10:  # skip first steps<br />      summ += batch_time<br />      count += 1<br />    t0 = time.perf_counter()<br />    if step &gt; 500:<br />      break<br />  print(f\'average step time: {summ/count}\')<br /><br />if __name__ == \'__main__\':<br />  # disable neuron compilar casting<br />  os.environ[&quot;NEURON_CC_FLAGS&quot;] = &quot;--auto-cast=none&quot;<br />  torch.cuda.is_bf16_supported = lambda: True<br />  train(batch_size=8, num_workers=1)</pre><p>The resultant training speed is 196.64 samples per second, about 26% lower than the <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision">default mixed precision</a> setting of the Neuron compiler. It‚Äôs important to note that while this post focuses on performance, in real-world scenarios, we would also need to evaluate the effect of the mixed precision policy we choose on <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#performance-accuracy-tradeoffs">model accuracy</a>.</p><h3>OpenXLA Optimizations</h3><p>As discussed in a <a href="https://towardsdatascience.com/a-first-look-at-aws-trainium-1e0605071970">previous post</a>, Neuron Cores are treated as <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#neuron-xla-device">XLA devices</a> and the <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/quick-start/torch-neuron.html">torch-neuronx</a> Python package implements the <a href="https://github.com/pytorch/xla/">PyTorch/XLA</a> API. Consequently, any optimization opportunities provided by the OpenXLA framework, and specifically those offered by the PyTorch/XLA API, can be leveraged on AWS Inferentia and Trainium. In this section we consider a few of these opportunities.</p><h4>BFloat16 Precision</h4><p>OpenXLA supports the option of <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#automatic-casting-of-float-tensors-to-bfloat16">casting all floats to BFloat16</a> via the XLA_USE_BF16 environment variable, as shown in the code block\xa0below:</p><pre>if __name__ == \'__main__\':<br />  os.environ[\'XLA_USE_BF16\'] = \'1\'<br />  train(batch_size=8, num_workers=1)</pre><p>The resultant training speed is 394.51 samples per second, nearly 50% faster than the speed of the <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision">default mixed precision</a> option.</p><h4>Multi-process Device\xa0Loading</h4><p>The PyTorch/XLA <a href="https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html#MpDeviceLoader">MpDeviceLoader</a> and its internal <a href="https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html">ParallelLoader</a>, which are responsible for loading input data on to the accelerator, include a number of parameters for controlling the transfer of data from the host to the device. In the code block below we tune <a href="https://github.com/pytorch/xla/blob/v2.1.0/torch_xla/distributed/parallel_loader.py#L86"><em>batches_per_execution</em></a> setting which determines the number of batches copied to the device for each execution cycle of the <a href="https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html">ParallelLoader</a>. By increasing this setting, we aim to reduce the overhead of the host-to-device communication:</p><pre>data_loader = torch.utils.data.DataLoader(dataset,<br />                                          batch_size=batch_size,<br />                                          num_workers=num_workers<br />                                          )<br />data_loader = pl.MpDeviceLoader(data_loader, <br />                                device, batches_per_execution=10)</pre><p>As a result of this optimization, the training speed increased to 1,027.39 samples per second, representing an additional 260% speed-up.</p><h4>Torch Compilation with OpenXLA\xa0Backend</h4><p>In previous posts (e.g., <a href="https://towardsdatascience.com/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d">here</a>), we have demonstrated the potential performance gains from using <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">PyTorch‚Äôs graph compilation</a> offering. Although <a href="https://openxla.org/xla">OpenXLA</a> includes its own graph creation and Just-In-Time (JIT) compilation mechanisms, <a href="https://pytorch.org/xla/master/torch_compile.html">torch.compile</a> can provide additional acceleration by eliminating the need for tracing the model operations at every step. The following code snippet demonstrates the use of the dedicated <em>openxla</em> backend for compiling the\xa0model:</p><pre>model = model.to(device)<br />model = torch.compile(backend=\'openxla\')</pre><p>Although torch.compile is currently <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/torch/torch-neuronx/index.html#known-limitations">not yet supported</a> by the Neuron SDK, we include its mention in anticipation of its future\xa0release.</p><h3>Neuron SDK Optimizations</h3><p>In this section we consider some of the optimization opportunities offered by the AWS Neuron SDK and, more specifically, by the Neuron compiler.</p><h4>Mixed Precision</h4><p>The Neuron SDK supports a variety of <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#mixed-precision-and-performance-accuracy-tuning-neuronx-cc">mixed precision</a> settings. In the code block below we program the compiler to cast all floats to BFloat16 via the <em>NEURON_CC_FLAGS</em> environment variable.</p><pre>if __name__ == \'__main__\':<br />  os.environ[&quot;NEURON_CC_FLAGS&quot;] = &quot;--auto-cast all --auto-cast-type bf16&quot;<br />  train(batch_size=8, num_workers=1)</pre><p>This results (unsurprisingly) in a similar training speed to the OpenXLA BFloat16 experiment described above.</p><h4>FP8</h4><p>One of the unique features of NeuronCoreV2 is its support of the eight-bit floating point type, fp8_e4m3. The code block below demonstrates how to configure the Neuron compiler to automatically cast all floating-point operations to\xa0FP8:</p><pre>if __name__ == \'__main__\':<br /> os.environ[&quot;NEURON_CC_FLAGS&quot;] = &quot;--auto-cast all --auto-cast-type fp8_e4m3&quot;<br /> train(batch_size=8, num_workers=1)</pre><p>While FP8 can accelerate training in some cases, maintaining stable convergence can be more challenging than when using BFloat16 due its reduced precision and dynamic range. Please see our <a href="https://towardsdatascience.com/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7">previous post</a> for more on the potential benefits and challenges of FP8 training.</p><p>In the case of our model, using FP8 actually harms runtime performance compared to BFloat16, reducing the training speed to 940.36 samples per\xa0second.</p><h4>Compiler Optimizations</h4><p>The <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/neuronx-cc/api-reference-guide/neuron-compiler-cli-reference-guide.html">Neuron compiler</a> includes a number of controls for optimizing the runtime performance of the compiled graph. Two key settings are <em>model-type</em> and <em>opt-level</em>. The <em>model-type </em>setting applies optimizations tailored to specific model architectures, such as transformers, while the <em>opt-level </em>setting allows for balancing compilation time against runtime performance. In the code block below, we program the <em>model-type</em> setting to <em>tranformer</em> and the <em>opt-level</em> setting to the highest performance option. We further specify the <em>target</em> runtime device, <em>inf2</em>, to ensure that the model is optimized for the target\xa0device.</p><pre>if __name__ == \'__main__\':<br />  os.environ[\'XLA_USE_BF16\'] = \'1\'<br />  os.environ[&quot;NEURON_CC_FLAGS&quot;] = &quot;--model-type transformer &quot; \\<br />                                  &quot;--optlevel 3&quot; \\<br />                                  &quot; --target inf2&quot;<br />  train(batch_size=8, num_workers=1)</pre><p>The above configuration resulted in a training speed of 1093.25 samples per second, amounting to a modest 6% improvement.</p><h3>Results</h3><p>We summarize the results of our experiments in the table below. Keep in mind that the effect of each of the optimization methods we discussed will depend greatly on the model and the runtime environment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/736/1*efMoE_0TpTUVD3EtQR5cww.png" /><figcaption>Experiment Results (by\xa0Author)</figcaption></figure><p>The techniques we employed resulted in a 435% performance boost compared to our baseline experiment. It is likely that additional acceleration could be achieved by revisiting and fine-tuning some of the methods we discussed, or by applying other optimization techniques not covered in this\xa0post.</p><p>Our goal has been demonstrate some of the available optimization strategies and demonstrate their potential impact on runtime performance. However, in a real-world scenario, we would need to assess the manner in which each of these optimizations impact our model convergence. In some cases, adjustments to the model configuration may be necessary to ensure optimal performance without sacrificing accuracy. Additionally, using a performance profiler to identify bottlenecks and measure system resource utilization is essential for guiding and informing our optimization activities.</p><h3>Summary</h3><p>Nowadays, we are fortunate to have a wide variety of systems on which to run our ML workloads. No matter which platform we choose, our goal is to maximize its capabilities. In this post, we focused on AWS Inferentia and reviewed several techniques for accelerating ML workloads running on it. Be sure to check out our <a href="https://chaimrand.medium.com/">other posts</a> for more optimization strategies across various AI accelerators.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cfd48e85d5ac" width="1" /><hr /><p><a href="https://towardsdatascience.com/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac">AI Model Optimization on AWS Inferentia and Trainium</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>'
2024-10-21 19:21:08,168 - INFO - clean content - clean_content='Tips for accelerating ML with AWS Neuron SDKPhoto by julien Tromeur on UnsplashWe are in a golden age of AI, with cutting-edge models disrupting industries and poised to transform life as we know it. Powering these advancements are increasingly powerful AI accelerators, such as NVIDIA H100 GPUs, Google Cloud TPUs, AWS‚Äôs Trainium and Inferentia chips, and more. With the growing number of options comes the challenge of selecting the most optimal platform for our machine learning (ML) workloads ‚Äî a crucial decision considering the high costs associated with AI computation. Importantly, a comprehensive assessment of each option necessitates ensuring that we are maximizing its utilization to fully leverage its capabilities.In this post, we will review several techniques for optimizing an ML workload on AWS‚Äôs custom-built AI chips using the AWS Neuron SDK. This continues our ongoing series of posts focused on ML model performance analysis and optimization across various platforms and environments (e.g., see here and here). While our primary focus will be on an ML training workload and AWS Inferentia2, the techniques discussed are also applicable to AWS Trainium. (Recall that although AWS Inferentia is primarily designed as an AI inference chip, we have previously demonstrated its effectiveness in training tasks as well.)Generally speaking, performance optimization is an iterative process that includes a performance analysis step to appropriately identify performance bottlenecks and resource under-utilization (e.g., see here). However, since the techniques we will discuss are general purpose (i.e., they are potentially applicable to any model, regardless of their performance profile), we defer the discussion on performance analysis with the Neuron SDK to a future post.DisclaimersThe code we will share is intended for demonstrative purposes only ‚Äî we make no claims regarding its accuracy, optimality, or robustness. Please do not view this post as a substitute for the official Neuron SDK documentation. Please do not interpret our mention of any platforms, libraries, or optimization techniques as an endorsement for their use. The best options for you will depend greatly on the specifics of your use-case and will require your own in-depth investigation and analysis.The experiments described below were run on an Amazon EC2 inf2.xlarge instance (containing two Neuron cores and four vCPUs). We used the most recent version of the Deep Learning AMI for Neuron available at the time of this writing, ‚ÄúDeep Learning AMI Neuron (Ubuntu 22.04) 20240927‚Äù, with AWS Neuron 2.20 and PyTorch 2.1. See the SDK documentation for more details on setup and installation. Keep in mind that the Neuron SDK is under active development and that the APIs we refer to, as well as the runtime measurements we report, may become outdated by the time you read this. Please be sure to stay up-to-date with the latest SDK and documentation available.Toy ModelTo facilitate our discussion, we introduce the following simple Vision Transformer (ViT)-backed classification model (based on timm version 1.0.10):from torch.utils.data import Datasetimport time, osimport torchimport torch_xla.core.xla_model as xmimport torch_xla.distributed.parallel_loader as plfrom timm.models.vision_transformer import VisionTransformer# use random dataclass FakeDataset(Dataset): def __len__(self): return 1000000 def __getitem__(self, index): rand_image = torch.randn([3, 224, 224], dtype=torch.float32) label = torch.tensor(data=index % 1000, dtype=torch.int64) return rand_image, labeldef train(batch_size=16, num_workers=0): # Initialize XLA process group for torchrun import torch_xla.distributed.xla_backend torch.distributed.init_process_group(\'xla\') # multi-processing: ensure each worker has same initial weights torch.manual_seed(0) dataset = FakeDataset() model = VisionTransformer() # load model to XLA device device = xm.xla_device() model = model.to(device) optimizer = torch.optim.Adam(model.parameters()) data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers) data_loader = pl.MpDeviceLoader(data_loader, device) loss_function = torch.nn.CrossEntropyLoss() summ = 0 count = 0 t0 = time.perf_counter() for step, (inputs, targets) in enumerate(data_loader, start=1): optimizer.zero_grad() outputs = model(inputs) loss = loss_function(outputs, targets) loss.backward() xm.optimizer_step(optimizer) batch_time = time.perf_counter() - t0 if step > 10: # skip first steps summ += batch_time count += 1 t0 = time.perf_counter() if step > 500: break print(f\'average step time: {summ/count}\')if __name__ == \'__main__\': train()# Initialization command:# torchrun --nproc_per_node=2 train.pyRunning our baseline model on the two cores of our AWS Inferentia instance, results in a training speed of 251.98 samples per second.In the next sections, we will iteratively apply a number of potential optimization techniques and assess their impact on step time performance. While we won‚Äôt go into the full details of each method, we will provide references for further reading (e.g., here). Importantly, the list we will present is not all-inclusive ‚Äî there are many techniques beyond what we will cover. We will organize the methods into three categories: PyTorch optimizations, OpenXLA optimizations, and Neuron-specific optimizations. However, the order of presentation is not binding. In fact, some of the techniques are interdependent ‚Äî for example, applying the mixed precision optimization may free up enough device memory to enable increasing the batch size.PyTorch Performance OptimizationsIn previous posts (e.g., here) we have covered the topic of PyTorch model performance analysis and optimization on GPU, extensively. Many of the techniques we discussed are relevant to other AI accelerators. In this section we will revisit few of these techniques and apply them to AWS Inferentia.Multi-process Data LoadingIn multi process data loading the input data is prepared in one or more dedicated CPU processes rather than in the same process that runs the training step. This allows for overlapping the data loading and training which can increase system utilization and lead to a significant speed-up. The number of processes is controlled by the num_workers parameter of the PyTorch DataLoader. In the following block we run our script with num_workers set to one:train(num_workers=1)This change results in a training speed of 253.56 samples per second for a boost of less than 1%.Batch Size OptimizationAnother important hyperparameter that can influence training speed is the training batch size. Often, we have found that increasing the batch size improves system utilization and results in better performance. However, the effects can vary based on the model and platform. In the case of our toy model on AWS Inferentia, we find that running with a batch size of 8 samples per neuron core results in a speed of 265.68 samples per second ‚Äî roughly 5% faster than a batch size of 16 samples per core.train(batch_size=8, num_workers=1)PyTorch Automatic Mixed PrecisionAnother common method for boosting performance is to use lower precision floats such as the 16-bit BFloat16. Importantly, some model components might not be compatible with reduced precision floats. PyTorch‚Äôs Automatic Mixed Precision (AMP) mode attempts to match the most appropriate floating point type to each model operation automatically. Although, the Neuron compiler offers different options for employing mixed precision, it also supports the option of using PyTorch AMP. In the code block below we include the modifications required to use PyTorch AMP.def train(batch_size=16, num_workers=0): # Initialize XLA process group for torchrun import torch_xla.distributed.xla_backend torch.distributed.init_process_group(\'xla\') # multi-processing: ensure each worker has same initial weights torch.manual_seed(0) dataset = FakeDataset() model = VisionTransformer() # load model to XLA device device = xm.xla_device() model = model.to(device) optimizer = torch.optim.Adam(model.parameters()) data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers) data_loader = pl.MpDeviceLoader(data_loader, device) loss_function = torch.nn.CrossEntropyLoss() summ = 0 count = 0 t0 = time.perf_counter() for step, (inputs, targets) in enumerate(data_loader, start=1): optimizer.zero_grad() # use PyTorch AMP with torch.autocast(dtype=torch.bfloat16, device_type=\'cuda\'): outputs = model(inputs) loss = loss_function(outputs, targets) loss.backward() xm.optimizer_step(optimizer) batch_time = time.perf_counter() - t0 if step > 10: # skip first steps summ += batch_time count += 1 t0 = time.perf_counter() if step > 500: break print(f\'average step time: {summ/count}\')if __name__ == \'__main__\': # disable neuron compilar casting os.environ["NEURON_CC_FLAGS"] = "--auto-cast=none" torch.cuda.is_bf16_supported = lambda: True train(batch_size=8, num_workers=1)The resultant training speed is 196.64 samples per second, about 26% lower than the default mixed precision setting of the Neuron compiler. It‚Äôs important to note that while this post focuses on performance, in real-world scenarios, we would also need to evaluate the effect of the mixed precision policy we choose on model accuracy.OpenXLA OptimizationsAs discussed in a previous post, Neuron Cores are treated as XLA devices and the torch-neuronx Python package implements the PyTorch/XLA API. Consequently, any optimization opportunities provided by the OpenXLA framework, and specifically those offered by the PyTorch/XLA API, can be leveraged on AWS Inferentia and Trainium. In this section we consider a few of these opportunities.BFloat16 PrecisionOpenXLA supports the option of casting all floats to BFloat16 via the XLA_USE_BF16 environment variable, as shown in the code block below:if __name__ == \'__main__\': os.environ[\'XLA_USE_BF16\'] = \'1\' train(batch_size=8, num_workers=1)The resultant training speed is 394.51 samples per second, nearly 50% faster than the speed of the default mixed precision option.Multi-process Device LoadingThe PyTorch/XLA MpDeviceLoader and its internal ParallelLoader, which are responsible for loading input data on to the accelerator, include a number of parameters for controlling the transfer of data from the host to the device. In the code block below we tune batches_per_execution setting which determines the number of batches copied to the device for each execution cycle of the ParallelLoader. By increasing this setting, we aim to reduce the overhead of the host-to-device communication:data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers )data_loader = pl.MpDeviceLoader(data_loader, device, batches_per_execution=10)As a result of this optimization, the training speed increased to 1,027.39 samples per second, representing an additional 260% speed-up.Torch Compilation with OpenXLA BackendIn previous posts (e.g., here), we have demonstrated the potential performance gains from using PyTorch‚Äôs graph compilation offering. Although OpenXLA includes its own graph creation and Just-In-Time (JIT) compilation mechanisms, torch.compile can provide additional acceleration by eliminating the need for tracing the model operations at every step. The following code snippet demonstrates the use of the dedicated openxla backend for compiling the model:model = model.to(device)model = torch.compile(backend=\'openxla\')Although torch.compile is currently not yet supported by the Neuron SDK, we include its mention in anticipation of its future release.Neuron SDK OptimizationsIn this section we consider some of the optimization opportunities offered by the AWS Neuron SDK and, more specifically, by the Neuron compiler.Mixed PrecisionThe Neuron SDK supports a variety of mixed precision settings. In the code block below we program the compiler to cast all floats to BFloat16 via the NEURON_CC_FLAGS environment variable.if __name__ == \'__main__\': os.environ["NEURON_CC_FLAGS"] = "--auto-cast all --auto-cast-type bf16" train(batch_size=8, num_workers=1)This results (unsurprisingly) in a similar training speed to the OpenXLA BFloat16 experiment described above.FP8One of the unique features of NeuronCoreV2 is its support of the eight-bit floating point type, fp8_e4m3. The code block below demonstrates how to configure the Neuron compiler to automatically cast all floating-point operations to FP8:if __name__ == \'__main__\': os.environ["NEURON_CC_FLAGS"] = "--auto-cast all --auto-cast-type fp8_e4m3" train(batch_size=8, num_workers=1)While FP8 can accelerate training in some cases, maintaining stable convergence can be more challenging than when using BFloat16 due its reduced precision and dynamic range. Please see our previous post for more on the potential benefits and challenges of FP8 training.In the case of our model, using FP8 actually harms runtime performance compared to BFloat16, reducing the training speed to 940.36 samples per second.Compiler OptimizationsThe Neuron compiler includes a number of controls for optimizing the runtime performance of the compiled graph. Two key settings are model-type and opt-level. The model-type setting applies optimizations tailored to specific model architectures, such as transformers, while the opt-level setting allows for balancing compilation time against runtime performance. In the code block below, we program the model-type setting to tranformer and the opt-level setting to the highest performance option. We further specify the target runtime device, inf2, to ensure that the model is optimized for the target device.if __name__ == \'__main__\': os.environ[\'XLA_USE_BF16\'] = \'1\' os.environ["NEURON_CC_FLAGS"] = "--model-type transformer " \\ "--optlevel 3" \\ " --target inf2" train(batch_size=8, num_workers=1)The above configuration resulted in a training speed of 1093.25 samples per second, amounting to a modest 6% improvement.ResultsWe summarize the results of our experiments in the table below. Keep in mind that the effect of each of the optimization methods we discussed will depend greatly on the model and the runtime environment.Experiment Results (by Author)The techniques we employed resulted in a 435% performance boost compared to our baseline experiment. It is likely that additional acceleration could be achieved by revisiting and fine-tuning some of the methods we discussed, or by applying other optimization techniques not covered in this post.Our goal has been demonstrate some of the available optimization strategies and demonstrate their potential impact on runtime performance. However, in a real-world scenario, we would need to assess the manner in which each of these optimizations impact our model convergence. In some cases, adjustments to the model configuration may be necessary to ensure optimal performance without sacrificing accuracy. Additionally, using a performance profiler to identify bottlenecks and measure system resource utilization is essential for guiding and informing our optimization activities.SummaryNowadays, we are fortunate to have a wide variety of systems on which to run our ML workloads. No matter which platform we choose, our goal is to maximize its capabilities. In this post, we focused on AWS Inferentia and reviewed several techniques for accelerating ML workloads running on it. Be sure to check out our other posts for more optimization strategies across various AI accelerators.AI Model Optimization on AWS Inferentia and Trainium was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.'
2024-10-21 19:21:08,168 - INFO - Generating summary with BART model
2024-10-21 19:21:15,904 - INFO - Generating summary with BART model
2024-10-21 19:21:23,737 - INFO - full_tweet='We are in a golden age of AI, with cutting-edge models disrupting industries and poised to transform life..[read moreüëáüèº] #AI #advancements https://towardsdatascience.com/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac?source=rss----7f60cf5620c9---4'
2024-10-21 19:21:23,995 - INFO - Tweet posted successfully: Response(data={'id': '1848444491444740173', 'text': 'We are in a golden age of AI, with cutting-edge models disrupting industries and poised to transform life..[read moreüëáüèº] #AI #advancements https://t.co/lW4V6hKBjn', 'edit_history_tweet_ids': ['1848444491444740173']}, includes={}, errors=[], meta={})
2024-10-21 19:21:23,996 - INFO - Saved posted URL: https://towardsdatascience.com/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac?source=rss----7f60cf5620c9---4 at 2024-10-21 19:21:23
2024-10-21 19:21:23,996 - INFO - Tweet link saved successfully.
2024-10-21 19:21:23,996 - INFO - Sleeping for 6 minutes and 4 seconds.
2024-10-21 19:27:27,996 - INFO - This link is already posted: https://www.pcmag.com/news/news-corp-sues-perplexity-ai-wants-it-to-pay-up-like-openai
2024-10-21 19:27:27,996 - INFO - This link is already posted: https://www.kdnuggets.com/2025/10/gartner/discover-the-top-ways-to-scale-your-ai-for-data-analytics-initiative
2024-10-21 19:27:27,996 - INFO - This link is already posted: https://techcrunch.com/2024/10/21/xai-elon-musks-ai-startup-launches-an-api/
2024-10-21 19:27:27,996 - INFO - This link is already posted: https://venturebeat.com/ai/microsofts-new-ai-agents-set-to-shake-up-enterprise-software-sparking-new-battle-with-salesforce/
2024-10-21 19:27:27,996 - INFO - This link is already posted: https://www.techradar.com/pro/us-treasury-claims-to-have-won-back-billions-of-dollars-lost-to-fraud-using-ai
2024-10-21 19:27:27,996 - INFO - This link is already posted: https://venturebeat.com/security/gartner-2025-will-see-the-rise-of-ai-agents-and-other-top-trends/
2024-10-21 19:27:27,996 - INFO - This link is already posted: https://techcrunch.com/2024/10/21/daze-a-creative-ai-powered-messaging-app-for-gen-z-is-blowing-up-prelaunch/
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://news.mit.edu/2024/making-it-easier-verify-ai-models-responses-1021
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://www.techradar.com/pro/if-you-dont-trust-it-youre-not-going-to-use-it-microsoft-ceo-says-ai-has-to-be-trustworthy-to-be-useful
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://www.techradar.com/pro/investment-in-ai-and-ai-pcs-is-now-critical-for-talent-retention
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1g8qn5s/r_does_aistats_do_acceptance_with_revisions/
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://www.techradar.com/pro/workers-really-arent-sure-their-bosses-know-enough-about-ai
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://www.pcmag.com/news/microsoft-launching-autonomous-ai-agents-copilot-studio
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://www.pcmag.com/news/tiktok-parent-bytedance-confirms-intern-sabotaged-ai-training
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://www.wired.com/story/donald-trump-ai-safety-regulation/
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://techcrunch.com/2024/10/21/datacrunch-wants-to-be-europes-first-ai-cloud-hyperscaler-powered-by-renewable-energy/
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1g8l0vq/research_when_do_aaai_phase_2_reviewers_need_to/
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://venturebeat.com/ai/ibm-debuts-open-source-granite-3-0-llms-for-enterprise-ai/
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://techcrunch.com/2024/10/20/women-in-ai-marissa-hummon-thinks-ai-will-help-make-the-power-grid-greener/
2024-10-21 19:27:27,997 - INFO - This link is already posted: https://venturebeat.com/security/the-ai-edge-in-cybersecurity-predictive-tools-aim-to-slash-response-times/

2024-10-09 06:14:19,195 - INFO - Starting configuration setup
2024-10-09 06:14:19,195 - INFO - Environment variables loaded successfully
2024-10-09 06:14:19,195 - INFO - Twitter API client initialized successfully
2024-10-09 06:14:21,073 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-10-09 06:14:28,821 - INFO - Summarization model initialized successfully
2024-10-09 06:14:28,821 - INFO - Starting tweet scheduling
2024-10-09 06:14:28,821 - INFO - Tweeting process started at 2024-10-09 06:14:28.821655!
2024-10-09 06:14:28,821 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Oct09.csv
2024-10-09 06:14:28,821 - INFO - Fetching latest tech news from RSS feeds
2024-10-09 06:14:43,036 - INFO - Total entries found: 617
2024-10-09 06:14:43,039 - INFO - Recent AI-related entries found: 27
2024-10-09 06:14:43,039 - INFO - Input content - content='<!-- SC_OFF --><div class="md"><p>I\'m not a researcher or working in AI or anything, but I have a hobby interest in it, and I\'ve been reading a bunch of research papers lately that are related to an idea I have. The problem I\'m trying to solve is what objective function to give a reinforcement learning AI so that it will be empathetic towards conscious agents and help them fulfill their goals. I\'m basically trying to solve Collaborative Inverse Reinforcement Learning, but where the AI isn\'t told who it\'s supposed to help, so it helps anything that acts like an agent. My assumption is that something that acts like an agent is likely to be conscious, and cases where this isn\'t true aren\'t big enough to matter. The reason I\'m interested in this reward function is that I want the AI to learn friendliness without the bottleneck of labelled examples, and I want it to generalize to helping non-humans such as animals. One primary challenge whose solution would make it so much easier is how to do Inverse Reinforcement Learning without access to action labels. For example, inferring goals just from a video. I\'m basically trying to come up with an objective function for benevolence in general.</p> <p>The thread I\'m currently thinking along is empowerment (mutual information between actions and world states). I think you can measure the alignment between 2 agents by subtracting their individual empowerment from their joint empowerment. When they\'re collaborating, this measure would presumably be positive. When they\'re working independently it\'d be zero and when they\'re competing it\'d be negative. By maximizing this, the AI can account for wasted effort when helping one agent causes a disadvantage to the other. Once this is corrected for, if you then help the agents by maximizing their individual empowerment, the terms cancel and you\'re just left maximizing their joint empowerment. Maximizing joint empowerment is the same as maximizing individual empowerment plus alignment. The logical conclusion is that to empower everyone, you\'d want to maximize the empowerment of a joint agent whose actions are equal to the world states themselves.</p> <p>Empowerment is only an instrumental goal though. Once you empower everyone jointly, you\'d want to measure what they\'re empowered towards and help with that, even if it reduces empowerment. Maybe that means that you just directly do Collaborative Inverse Reinforcement Learning on this overall agent whose actions are the world states. However, that just seems like accelerationism which could have problems. I also don\'t know how to factor in the idea that this &quot;world agent&quot; might not be rational while it\'s still unempowered.</p> <p>If this idea of a &quot;world agent&quot; doesn\'t pan out, I\'m back to the problem of how to detect agent-like behavior and adding up the preferences of all these agents. I\'d definitely like more options of how to do so, but I haven\'t been able to think of anything else. Please show me any research that\'s relevant to this.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/NeuroPyrox"> /u/NeuroPyrox </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fzkh4y/d_please_point_me_in_the_right_direction_for/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fzkh4y/d_please_point_me_in_the_right_direction_for/">[comments]</a></span>'
2024-10-09 06:14:43,041 - INFO - clean content - clean_content="I'm not a researcher or working in AI or anything, but I have a hobby interest in it, and I've been reading a bunch of research papers lately that are related to an idea I have. The problem I'm trying to solve is what objective function to give a reinforcement learning AI so that it will be empathetic towards conscious agents and help them fulfill their goals. I'm basically trying to solve Collaborative Inverse Reinforcement Learning, but where the AI isn't told who it's supposed to help, so it helps anything that acts like an agent. My assumption is that something that acts like an agent is likely to be conscious, and cases where this isn't true aren't big enough to matter. The reason I'm interested in this reward function is that I want the AI to learn friendliness without the bottleneck of labelled examples, and I want it to generalize to helping non-humans such as animals. One primary challenge whose solution would make it so much easier is how to do Inverse Reinforcement Learning without access to action labels. For example, inferring goals just from a video. I'm basically trying to come up with an objective function for benevolence in general. The thread I'm currently thinking along is empowerment (mutual information between actions and world states). I think you can measure the alignment between 2 agents by subtracting their individual empowerment from their joint empowerment. When they're collaborating, this measure would presumably be positive. When they're working independently it'd be zero and when they're competing it'd be negative. By maximizing this, the AI can account for wasted effort when helping one agent causes a disadvantage to the other. Once this is corrected for, if you then help the agents by maximizing their individual empowerment, the terms cancel and you're just left maximizing their joint empowerment. Maximizing joint empowerment is the same as maximizing individual empowerment plus alignment. The logical conclusion is that to empower everyone, you'd want to maximize the empowerment of a joint agent whose actions are equal to the world states themselves. Empowerment is only an instrumental goal though. Once you empower everyone jointly, you'd want to measure what they're empowered towards and help with that, even if it reduces empowerment. Maybe that means that you just directly do Collaborative Inverse Reinforcement Learning on this overall agent whose actions are the world states. However, that just seems like accelerationism which could have problems. I also don't know how to factor in the idea that this &quot;world agent&quot; might not be rational while it's still unempowered. If this idea of a &quot;world agent&quot; doesn't pan out, I'm back to the problem of how to detect agent-like behavior and adding up the preferences of all these agents. I'd definitely like more options of how to do so, but I haven't been able to think of anything else. Please show me any research that's relevant to this. &#32; submitted by &#32; /u/NeuroPyrox [link] &#32; [comments]"
2024-10-09 06:14:43,041 - INFO - Generating summary with BART model
2024-10-09 06:14:53,660 - INFO - Generating summary with BART model
2024-10-09 06:15:03,641 - INFO - full_tweet="I'm not a researcher or working in AI or anything, but I have a hobby interest in it, and I've been..[read moreüëáüèº] #AI #I https://www.reddit.com/r/MachineLearning/comments/1fzkh4y/d_please_point_me_in_the_right_direction_for/"
2024-10-09 06:15:03,747 - ERROR - Failed to post tweet: 401 Unauthorized
Unauthorized

2024-09-22 14:09:59,882 - INFO - Starting configuration setup
2024-09-22 14:09:59,883 - INFO - Environment variables loaded successfully
2024-09-22 14:09:59,883 - INFO - Twitter API client initialized successfully
2024-09-22 14:10:01,732 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-09-22 14:10:08,978 - INFO - Summarization model initialized successfully
2024-09-22 14:10:08,978 - INFO - Starting tweet scheduling
2024-09-22 14:10:08,978 - INFO - Tweeting process started at 2024-09-22 14:10:08.978876!
2024-09-22 14:10:08,979 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Sep22.csv
2024-09-22 14:10:08,979 - INFO - Fetching latest tech news from RSS feeds
2024-09-22 14:10:22,246 - INFO - Total entries found: 599
2024-09-22 14:10:22,249 - INFO - Recent AI-related entries found: 7
2024-09-22 14:10:22,249 - INFO - Input content - content='<h4>And the difference between weak vs strong grounding</h4><figure><img alt="A stylised flowchart showing a user request turning into: (Input + Context) forming a request to a Large Language Model (LLM). The LLM is styled as a digitised brain with equations behind it. The LLM generates a response with “Possible Hallucinations”, some of which are labelled with “Where did you get this from? PROVE IT”. The response is connected back to portions of the input and context with “DON’T TRUST\u200a—\u200aVERIFY”. The verified result is sent back to the user." src="https://cdn-images-1.medium.com/max/1024/1*tcjsLBh_rGDsZ1uav0AGMA.png" /><figcaption>Image by the\xa0author</figcaption></figure><p>I work as an AI Engineer in a particular niche: document automation and information extraction. In my industry using Large Language Models has presented a number of challenges when it comes to hallucinations. Imagine an AI misreading an invoice amount as $100,000 instead of $1,000, leading to a 100x overpayment. When faced with such risks, preventing hallucinations becomes a critical aspect of building robust AI solutions. These are some of the key principles I focus on when designing solutions that may be prone to hallucinations.</p><h3>Using validation rules and “human in the\xa0loop”</h3><p>There are various ways to incorporate human oversight in AI systems. Sometimes, extracted information is always presented to a human for review. For instance, a parsed resume might be shown to a user before submission to an Applicant Tracking System (ATS). More often, the extracted information is automatically added to a system and only flagged for human review if potential issues\xa0arise.</p><p>A crucial part of any AI platform is determining when to include human oversight. This often involves different types of validation rules:</p><p>1. Simple rules, such as ensuring line-item totals match the invoice\xa0total.</p><p>2. Lookups and integrations, like validating the total amount against a purchase order in an accounting system or verifying payment details against a supplier’s previous\xa0records.</p><figure><img alt="Validation popup for an invoice. Text “30,000” is highlighted with the following overlaid text: Payment Amount Total | #xpected line item totals to equal document total | Confirm anyway? | Remove?" src="https://cdn-images-1.medium.com/max/480/0*f7c4p2ECbMQu3IUc.png" /><figcaption>An example validation error when there needs to be a human in the loop. Source:\xa0Affinda</figcaption></figure><p>These processes are a good thing. But we also don’t want an AI that constantly triggers safeguards and forces manual human intervention. Hallucinations can defeat the purpose of using AI if it’s constantly triggering these safeguards.</p><h3>Small Language\xa0Models</h3><p>One solution to preventing hallucinations is to use Small Language Models (SLMs) which are “extractive”. This means that the model labels parts of the document and we collect these labels into structured outputs. I recommend trying to use a SLMs where possible rather than defaulting to LLMs for every problem. For example, in resume parsing for job boards, waiting 30+ seconds for an LLM to process a resume is often unacceptable. For this use case we’ve found an SLM can provide results in 2–3 seconds with higher accuracy than larger models like\xa0GPT-4o.</p><h4>An example from our\xa0pipeline</h4><p>In our startup a document can be processed by up to 7 different models\u200a—\u200aonly 2 of which might be an LLM. That’s because an LLM isn’t always the best tool for the job. Some steps such as Retrieval Augmented Generation rely on a small multimodal model to create useful embeddings for retrieval. The first step\u200a—\u200adetecting whether something is even a document\u200a—\u200auses a small and super-fast model that achieves 99.9% accuracy. It’s vital to break a problem down into small chunks and then work out which parts LLMs are best suited for. This way, you reduce the chances of hallucinations occurring.</p><h4>Distinguishing Hallucinations from\xa0Mistakes</h4><p>I make a point to differentiate between hallucinations (the model inventing information) and mistakes (the model misinterpreting existing information). For instance, selecting the wrong dollar amount as a receipt total is a mistake, while generating a non-existent amount is a hallucination. Extractive models can only make mistakes, while generative models can make <strong>both </strong>mistakes and hallucinations.</p><h3>Risk tolerance and Grounding</h3><p>When using generative models we need some way of eliminating hallucinations.<br /> <br /> <em>Grounding </em>refers to any technique which forces a generative AI model to justify its outputs with reference to some authoritative information. How grounding is managed is a matter of risk tolerance for each\xa0project.</p><p>For example\u200a—\u200aa company with a general-purpose inbox might look to identify action items. Usually, emails requiring actions are sent directly to account managers. A general inbox that’s full of invoices, spam, and simple replies (“thanks”, “OK”, etc.) has far too many messages for humans to check. What happens when actions are mistakenly sent to this general inbox? Actions regularly get missed. If a model makes mistakes but is generally accurate it’s already doing better than doing nothing. In this case the tolerance for mistakes/hallucinations can be\xa0high.</p><p>Other situations might warrant particularly low risk tolerance\u200a—\u200athink financial documents and “straight-through processing”. This is where extracted information is automatically added to a system without review by a human. For example, a company might not allow invoices to be automatically added to an accounting system unless (1) the payment amount exactly matches the amount in the purchase order, and (2) the payment method matches the previous payment method of the supplier.</p><p>Even when risks are low, I still err on the side of caution. Whenever I’m focused on information extraction I follow a simple\xa0rule:</p><blockquote><strong>If text is extracted from a document, then it must exactly match text found in the document.</strong></blockquote><p>This is tricky when the information is structured (e.g. a table)\u200a—\u200aespecially because PDFs don’t carry any information about the order of words on a page. For example, a description of a line-item might split across multiple lines so the aim is to draw a coherent box around the extracted text regardless of the left-to-right order of the words (or right-to-left in some languages).</p><p>Forcing the model to point to exact text in a document is “strong grounding”. Strong grounding isn’t limited to information extraction. E.g. customer service chat-bots might be required to quote (verbatim) from standardised responses in an internal knowledge base. This isn’t always ideal given that standardised responses might not actually be able to answer a customer’s question.</p><p>Another tricky situation is when information needs to be inferred from context. For example, a medical assistant AI might infer the presence of a condition based on its symptoms without the medical condition being expressly stated. Identifying where those symptoms were mentioned would be a form of “weak grounding”. The justification for a response must exist in the context but the exact output can only be synthesised from the supplied information. A further grounding step could be to force the model to lookup the medical condition and justify that those symptoms are relevant. This may still need weak grounding because symptoms can often be expressed in many\xa0ways.</p><h3>Grounding for complex\xa0problems</h3><p>Using AI to solve increasingly complex problems can make it difficult to use grounding. For example, how do you ground outputs if a model is required to perform “reasoning” or to infer information from context? Here are some considerations for adding grounding to complex problems:</p><ol><li>Identify complex decisions which could be broken down into a set of rules. Rather than having the model generate an answer to the final decision have it generate the components of that decision. Then use rules to display the result. (Caveat\u200a—\u200athis can sometimes make hallucinations worse. Asking the model multiple questions gives it multiple opportunities to hallucinate. Asking it one question could be better. But we’ve found current models are generally worse at complex multi-step reasoning.)</li><li>If something can be expressed in many ways (e.g. descriptions of symptoms), a first step could be to get the model to tag text and standardise it (usually referred to as “coding”). This might open opportunities for stronger grounding.</li><li>Set up “tools” for the model to call which constrain the output to a very specific structure. We don’t want to execute arbitrary code generated by an LLM. We want to create tools that the model can call and give restrictions for what’s in those\xa0tools.</li><li>Wherever possible, include grounding in tool use\u200a—\u200ae.g. by validating responses against the context before sending them to a downstream system.</li><li>Is there a way to validate the final output? If handcrafted rules are out of the question, could we craft a prompt for verification? (And follow the above rules for the verified model as\xa0well).</li></ol><h3>Key Takeaways</h3><ul><li>When it comes to information extraction, we don’t tolerate outputs not found in the original\xa0context.</li><li>We follow this up with verification steps that catch mistakes as well as hallucinations.</li><li>Anything we do beyond that is about risk assessment and risk minimisation.</li><li>Break complex problems down into smaller steps and identify if an LLM is even\xa0needed.</li><li>For complex problems use a systematic approach to identify verifiable task:</li></ul><p>— Strong grounding forces LLMs to quote verbatim from trusted sources. It’s always preferred to use strong grounding.</p><p>— Weak grounding forces LLMs to reference trusted sources but allows synthesis and reasoning.</p><p>— Where a problem can be broken down into smaller tasks use strong grounding on tasks where possible.</p><h3><strong>Affinda AI\xa0Platform</strong></h3><p>We’ve built a powerful <a href="https://www.affinda.com/platform">AI document processing</a> platform used by organisations around the\xa0world.</p><h3>About the\xa0Author</h3><p>I’m the Lead AI Engineer @ Affinda. I spent 10 years making <a href="https://medium.com/@TarikDzekman/my-career-change-to-ai-from-ux-b1ed6690c09a">a career change from UX to AI</a>. Looking for a more in-depth understanding of generative AI? Read my deep dive: <a href="https://medium.com/towards-data-science/what-do-large-language-models-understand-befdb4411b77">what Large Language Models actually understand</a>.</p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9fc4121295cc" width="1" /><hr /><p><a href="https://towardsdatascience.com/how-i-deal-with-hallucinations-at-an-ai-startup-9fc4121295cc">How I Deal with Hallucinations at an AI Startup</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>'
2024-09-22 14:10:22,251 - INFO - clean content - clean_content='And the difference between weak vs strong groundingImage by the authorI work as an AI Engineer in a particular niche: document automation and information extraction. In my industry using Large Language Models has presented a number of challenges when it comes to hallucinations. Imagine an AI misreading an invoice amount as $100,000 instead of $1,000, leading to a 100x overpayment. When faced with such risks, preventing hallucinations becomes a critical aspect of building robust AI solutions. These are some of the key principles I focus on when designing solutions that may be prone to hallucinations.Using validation rules and “human in the loop”There are various ways to incorporate human oversight in AI systems. Sometimes, extracted information is always presented to a human for review. For instance, a parsed resume might be shown to a user before submission to an Applicant Tracking System (ATS). More often, the extracted information is automatically added to a system and only flagged for human review if potential issues arise.A crucial part of any AI platform is determining when to include human oversight. This often involves different types of validation rules:1. Simple rules, such as ensuring line-item totals match the invoice total.2. Lookups and integrations, like validating the total amount against a purchase order in an accounting system or verifying payment details against a supplier’s previous records.An example validation error when there needs to be a human in the loop. Source: AffindaThese processes are a good thing. But we also don’t want an AI that constantly triggers safeguards and forces manual human intervention. Hallucinations can defeat the purpose of using AI if it’s constantly triggering these safeguards.Small Language ModelsOne solution to preventing hallucinations is to use Small Language Models (SLMs) which are “extractive”. This means that the model labels parts of the document and we collect these labels into structured outputs. I recommend trying to use a SLMs where possible rather than defaulting to LLMs for every problem. For example, in resume parsing for job boards, waiting 30+ seconds for an LLM to process a resume is often unacceptable. For this use case we’ve found an SLM can provide results in 2–3 seconds with higher accuracy than larger models like GPT-4o.An example from our pipelineIn our startup a document can be processed by up to 7 different models — only 2 of which might be an LLM. That’s because an LLM isn’t always the best tool for the job. Some steps such as Retrieval Augmented Generation rely on a small multimodal model to create useful embeddings for retrieval. The first step — detecting whether something is even a document — uses a small and super-fast model that achieves 99.9% accuracy. It’s vital to break a problem down into small chunks and then work out which parts LLMs are best suited for. This way, you reduce the chances of hallucinations occurring.Distinguishing Hallucinations from MistakesI make a point to differentiate between hallucinations (the model inventing information) and mistakes (the model misinterpreting existing information). For instance, selecting the wrong dollar amount as a receipt total is a mistake, while generating a non-existent amount is a hallucination. Extractive models can only make mistakes, while generative models can make both mistakes and hallucinations.Risk tolerance and GroundingWhen using generative models we need some way of eliminating hallucinations. Grounding refers to any technique which forces a generative AI model to justify its outputs with reference to some authoritative information. How grounding is managed is a matter of risk tolerance for each project.For example — a company with a general-purpose inbox might look to identify action items. Usually, emails requiring actions are sent directly to account managers. A general inbox that’s full of invoices, spam, and simple replies (“thanks”, “OK”, etc.) has far too many messages for humans to check. What happens when actions are mistakenly sent to this general inbox? Actions regularly get missed. If a model makes mistakes but is generally accurate it’s already doing better than doing nothing. In this case the tolerance for mistakes/hallucinations can be high.Other situations might warrant particularly low risk tolerance — think financial documents and “straight-through processing”. This is where extracted information is automatically added to a system without review by a human. For example, a company might not allow invoices to be automatically added to an accounting system unless (1) the payment amount exactly matches the amount in the purchase order, and (2) the payment method matches the previous payment method of the supplier.Even when risks are low, I still err on the side of caution. Whenever I’m focused on information extraction I follow a simple rule:If text is extracted from a document, then it must exactly match text found in the document.This is tricky when the information is structured (e.g. a table) — especially because PDFs don’t carry any information about the order of words on a page. For example, a description of a line-item might split across multiple lines so the aim is to draw a coherent box around the extracted text regardless of the left-to-right order of the words (or right-to-left in some languages).Forcing the model to point to exact text in a document is “strong grounding”. Strong grounding isn’t limited to information extraction. E.g. customer service chat-bots might be required to quote (verbatim) from standardised responses in an internal knowledge base. This isn’t always ideal given that standardised responses might not actually be able to answer a customer’s question.Another tricky situation is when information needs to be inferred from context. For example, a medical assistant AI might infer the presence of a condition based on its symptoms without the medical condition being expressly stated. Identifying where those symptoms were mentioned would be a form of “weak grounding”. The justification for a response must exist in the context but the exact output can only be synthesised from the supplied information. A further grounding step could be to force the model to lookup the medical condition and justify that those symptoms are relevant. This may still need weak grounding because symptoms can often be expressed in many ways.Grounding for complex problemsUsing AI to solve increasingly complex problems can make it difficult to use grounding. For example, how do you ground outputs if a model is required to perform “reasoning” or to infer information from context? Here are some considerations for adding grounding to complex problems:Identify complex decisions which could be broken down into a set of rules. Rather than having the model generate an answer to the final decision have it generate the components of that decision. Then use rules to display the result. (Caveat — this can sometimes make hallucinations worse. Asking the model multiple questions gives it multiple opportunities to hallucinate. Asking it one question could be better. But we’ve found current models are generally worse at complex multi-step reasoning.)If something can be expressed in many ways (e.g. descriptions of symptoms), a first step could be to get the model to tag text and standardise it (usually referred to as “coding”). This might open opportunities for stronger grounding.Set up “tools” for the model to call which constrain the output to a very specific structure. We don’t want to execute arbitrary code generated by an LLM. We want to create tools that the model can call and give restrictions for what’s in those tools.Wherever possible, include grounding in tool use — e.g. by validating responses against the context before sending them to a downstream system.Is there a way to validate the final output? If handcrafted rules are out of the question, could we craft a prompt for verification? (And follow the above rules for the verified model as well).Key TakeawaysWhen it comes to information extraction, we don’t tolerate outputs not found in the original context.We follow this up with verification steps that catch mistakes as well as hallucinations.Anything we do beyond that is about risk assessment and risk minimisation.Break complex problems down into smaller steps and identify if an LLM is even needed.For complex problems use a systematic approach to identify verifiable task:— Strong grounding forces LLMs to quote verbatim from trusted sources. It’s always preferred to use strong grounding.— Weak grounding forces LLMs to reference trusted sources but allows synthesis and reasoning.— Where a problem can be broken down into smaller tasks use strong grounding on tasks where possible.Affinda AI PlatformWe’ve built a powerful AI document processing platform used by organisations around the world.About the AuthorI’m the Lead AI Engineer @ Affinda. I spent 10 years making a career change from UX to AI. Looking for a more in-depth understanding of generative AI? Read my deep dive: what Large Language Models actually understand.How I Deal with Hallucinations at an AI Startup was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.'
2024-09-22 14:10:22,251 - INFO - Generating summary with BART model
2024-09-22 14:10:30,008 - INFO - Generating summary with BART model
2024-09-22 14:10:37,112 - INFO - full_tweet='Using Large Language Models has presented a number of challenges when it comes to hallucinations. Imagine..[read more👇🏼] #AI #hallucinations https://towardsdatascience.com/how-i-deal-with-hallucinations-at-an-ai-startup-9fc4121295cc?source=rss----7f60cf5620c9---4'
2024-09-22 14:10:37,416 - INFO - Tweet posted successfully: Response(data={'id': '1837857033808158728', 'edit_history_tweet_ids': ['1837857033808158728'], 'text': 'Using Large Language Models has presented a number of challenges when it comes to hallucinations. Imagine..[read more👇🏼] #AI #hallucinations https://t.co/wyusyN8b6B'}, includes={}, errors=[], meta={})
2024-09-22 14:10:37,416 - INFO - Sleeping for 5 minutes and 5 seconds.
2024-09-22 14:15:42,416 - INFO - Saved posted URL: https://towardsdatascience.com/how-i-deal-with-hallucinations-at-an-ai-startup-9fc4121295cc?source=rss----7f60cf5620c9---4 at 2024-09-22 14:15:42
2024-09-22 14:15:42,416 - INFO - Input content - content="Smartphones will get much better at meeting your needs, thanks to AI. And that's just for starters."
2024-09-22 14:15:42,416 - INFO - clean content - clean_content="Smartphones will get much better at meeting your needs, thanks to AI. And that's just for starters."
2024-09-22 14:15:42,417 - INFO - Generating summary with BART model
2024-09-22 14:15:47,398 - INFO - Generating summary with BART model
2024-09-22 14:15:52,420 - INFO - full_tweet="Smartphones will get much better at meeting your needs, thanks to AI. And that's just for starters...[read more👇🏼] #AI #Smartphones https://www.cnet.com/tech/mobile/features/a-cambrian-explosion-ais-radical-reshaping-of-your-phone-coming-soon/#ftag=CAD590a51e"
2024-09-22 14:15:52,709 - INFO - Tweet posted successfully: Response(data={'text': "Smartphones will get much better at meeting your needs, thanks to AI. And that's just for starters...[read more👇🏼] #AI #Smartphones https://t.co/9fWnWgOpBt", 'id': '1837858356217655420', 'edit_history_tweet_ids': ['1837858356217655420']}, includes={}, errors=[], meta={})
2024-09-22 14:15:52,709 - INFO - Sleeping for 7 minutes and 7 seconds.
2024-09-22 14:22:59,709 - INFO - Saved posted URL: https://www.cnet.com/tech/mobile/features/a-cambrian-explosion-ais-radical-reshaping-of-your-phone-coming-soon/#ftag=CAD590a51e at 2024-09-22 14:22:59
2024-09-22 14:22:59,709 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1fmkhok/d_last_week_in_medical_ai_top_research/
2024-09-22 14:22:59,710 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1fme9af/dwhat_are_the_top_3_countries_in_development_of/
2024-09-22 14:22:59,710 - INFO - This link is already posted: https://www.theverge.com/2024/9/21/24250867/jony-ive-confirms-collaboration-openai-hardware
2024-09-22 14:22:59,710 - INFO - This link is already posted: https://techcrunch.com/2024/09/21/google-ceo-sundar-pichai-announces-120m-fund-for-global-ai-education/
2024-09-22 14:22:59,710 - INFO - This link is already posted: https://www.techradar.com/pro/obscure-ssd-vendor-is-using-revolutionary-cooling-system-in-order-to-deliver-best-in-class-performance-iodyne-s-portable-ssd-packs-airjet-iphone-connectivity-and-unique-raid-6-capabilities-in-a-tiny-footprint

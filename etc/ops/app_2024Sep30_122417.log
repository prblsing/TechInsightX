2024-09-30 12:24:17,900 - INFO - Starting configuration setup
2024-09-30 12:24:17,900 - INFO - Environment variables loaded successfully
2024-09-30 12:24:17,900 - INFO - Twitter API client initialized successfully
2024-09-30 12:24:19,843 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-09-30 12:24:27,167 - INFO - Summarization model initialized successfully
2024-09-30 12:24:27,167 - INFO - Starting tweet scheduling
2024-09-30 12:24:27,167 - INFO - Tweeting process started at 2024-09-30 12:24:27.167859!
2024-09-30 12:24:27,168 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Sep30.csv
2024-09-30 12:24:27,168 - INFO - Fetching latest tech news from RSS feeds
2024-09-30 12:24:42,139 - INFO - Total entries found: 617
2024-09-30 12:24:42,142 - INFO - Recent AI-related entries found: 15
2024-09-30 12:24:42,143 - INFO - Input content - content='<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qk4WeIf8CzaRjVc07CWFhg.jpeg" /><figcaption><em>Image credit: Adobe\xa0Stock.</em></figcaption></figure><h4><strong><em>Fundamental choices impacting integration and deployment at scale of GenAI into businesses</em></strong></h4><p>Before a company or a developer adopts generative artificial intelligence (GenAI), they often wonder how to get business value from the integration of AI into their business. With this in mind, a fundamental question arises: Which approach will deliver the best value on investment\u200a—\u200aa large all-encompassing proprietary model or an open source AI model that can be molded and fine-tuned for a company’s needs? AI adoption strategies fall within a wide spectrum, from accessing a cloud service from a large proprietary frontier model like <a href="https://openai.com/index/hello-gpt-4o/">OpenAI’s GPT-4o</a> to building an internal solution in the company’s compute environment with an open source small model using indexed company data for a targeted set of tasks. Current AI solutions go well beyond the model itself, with a whole ecosystem of retrieval systems, agents, and other functional components such as AI accelerators, which are beneficial for both large and small models. Emergence of cross-industry collaborations like the <a href="https://opea.dev/">Open Platform for Enterprise AI (OPEA)</a> further the promise of streamlining the access and structuring of end-to-end open source solutions.</p><p>This basic choice between the open source ecosystem and a proprietary setting impacts countless business and technical decisions, making it “the AI developer’s dilemma.” I believe that for most enterprise and other business deployments, it makes sense to initially use proprietary models to learn about AI’s potential and minimize early capital expenditure (CapEx). However, for broad sustained deployment, in many cases companies would use ecosystem-based open source targeted solutions, which allows for a cost-effective, adaptable strategy that aligns with evolving business needs and industry\xa0trends.</p><h4><strong>GenAI Transition from Consumer to Business Deployment</strong></h4><p>When GenAI burst onto the scene in late 2022 with Open AI’s GPT-3 and ChatGPT 3.5, it mainly garnered consumer interest. As businesses began investigating GenAI, two approaches to deploying GenAI quickly emerged in 2023\u200a—\u200ausing giant frontier models like ChatGPT vs. the newly introduced small, open source models originally inspired by Meta’s LLaMa model. By early 2024, two basic approaches have solidified, as shown in the columns in Figure 1. With the proprietary AI approach, the company relies on a large closed model to provide all the needed technology value. For example, taking GPT-4o as a proxy for the left column, AI developers would use OpenAI technology for the model, data, security, and compute. With the open source ecosystem AI approach, the company or developer may opt for the right-sized open source model, using corporate or private data, customized functionality, and the necessary compute and security.</p><p>Both directions are valid and have advantages and disadvantages. It is not an absolute partition and developers can choose components from either approach, but taking either a proprietary or ecosystem-based open source AI path provides the company with a strategy with high internal consistency. While it is expected that both approaches will be broadly deployed, I believe that after an initial learning and transition period, most companies will follow the open source approach. Depending on the usage and setting, open source internal AI may provide significant benefits, including the ability to fine-tune the model and drive deployment using the company’s current infrastructure to run the model at the edge, on the client, in the data center, or as a dedicated service. With new AI fine-tuning tools, deep expertise is less of a\xa0barrier.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GX8paB0ZR20Hsk54HE-DlA.png" /><figcaption><em>Figure 1. Base approaches to the AI developer’s dilemma. Image credit: Intel\xa0Labs.</em></figcaption></figure><p>Across all industries, AI developers are using GenAI for a variety of applications. An October 2023 <a href="https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai">poll by Gartner</a> found that 55% of organizations reported increasing investment in GenAI since early 2023, and many companies are in pilot or production mode for the growing technology. As of the time of the survey, companies were mainly investing in using GenAI for software development, followed closely by marketing and customer service functions. Clearly, the range of AI applications is growing\xa0rapidly.</p><h4><strong>Large Proprietary Models vs. Small and Large Open Source\xa0Models</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QYYLFTdMxT7yg2e6UZYSzA.png" /><figcaption><em>Figure 2: Advantages of large proprietary models, and small and large open source models. For business considerations, see Figure 7 for CapEx and OpEx aspects. Image credit: Intel\xa0Labs.</em></figcaption></figure><p>In my blog <a href="https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618">Survival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective AI at Scale</a>, I provide a detailed evaluation of large models vs. small models. In essence, following the introduction of <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">Meta’s LLaMa open source model in February 2023</a>, there has been a virtuous cycle of innovation and rapid improvement where the academia and broad-base ecosystem are creating highly effective models that are 10x to 100x smaller than the large frontier models. A crop of small models, which in 2024 were mostly less than 30 billion parameters, could <a href="https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/">closely match</a> the capabilities of ChatGPT-style large models containing well over 100B parameters, especially when targeted for particular domains. While GenAI is already being deployed throughout industries for a wide range of business usages, the use of compact models is\xa0rising.</p><p>In addition, open source models are mostly lagging <a href="https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers">only six to 12 months behind</a> the performance of proprietary models. Using the broad language benchmark MMLU, the improvement pace of the open source models is faster and the gap seems to be closing with proprietary models. For example, OpenAI’s <a href="https://openai.com/index/hello-gpt-4o/">GPT-4o</a> came out this year on May 13 with major multimodal features while Microsoft’s small open source <a href="https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/">Phi-3-vision</a> was introduced just a week later on May 21. In <a href="https://youtu.be/PZaNL6igONU?si=jCvhwvWBoZFnRG5X">rudimentary comparisons</a> done on visual recognition and understanding, the models showed some similar competencies, with several tests even favoring the Phi-3-vision model. <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Initial evaluations of Meta’s Llama 3.2 open source release</a> suggest that its “vision models are competitive with leading foundation models, Claude 3 Haiku and GPT4o-mini on image recognition and a range of visual understanding tasks.”</p><p>Large models have incredible all-in-one versatility. Developers can choose from a variety of large commercially available proprietary GenAI models, including OpenAI’s GPT-4o multimodal model. Google’s <a href="https://deepmind.google/technologies/gemini/#introduction">Gemini 1.5</a> natively multimodal model is available in four sizes: Nano for mobile device app development, Flash small model for specific tasks, Pro for a wide range of tasks, and Ultra for highly complex tasks. And Anthropic’s <a href="https://www.anthropic.com/news/claude-3-family">Claude 3 Opus</a>, rumored to have <a href="https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3">approximately 2 trillion parameters</a>, has a 200K token context window, allowing users to upload large amounts of information. There’s also another category of out-of-the-box large GenAI models that businesses can use for employee productivity and creative development. <a href="https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/">Microsoft 365 Copilot</a> integrates the Microsoft 365 Apps suite, Microsoft Graph (content and context from emails, files, meetings, chats, calendars, and contacts), and\xa0GPT-4.</p><p>Most large and small open source models are often more transparent about application frameworks, tool ecosystem, training data, and evaluation platforms. Model architecture, hyperparameters, response quality, input modalities, context window size, and inference cost are partially or fully disclosed. These models often provide information on the dataset so that developers can determine if it meets copyright or quality expectations. This transparency allows developers to easily interchange models for future versions. Among the growing number of small commercially available open source models, Meta’s <a href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3 and 3.1</a> are based on transformer architecture and available in 8B, 70B, and 405B parameters. Llama 3.2 multimodal model has 11B and 90B, with smaller versions at 1B and 3B parameters. Built in collaboration with NVIDIA, Mistral AI’s <a href="https://mistral.ai/news/mistral-nemo/">Mistral NeMo</a> is a 12B model that features a large 128k context window while Microsoft’s <a href="https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/">Phi-3</a> (3.8B, 7B, and 14B) offers Transformer models for reasoning and language understanding tasks. Microsoft highlights Phi models as an example of “<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">the surprising power of small language models</a>” while investing heavily in OpenAI’s very large models. Microsoft’s diverse interest in GenAI indicates that it’s not a one-size-fits-all market.</p><h4><strong>Model-Incorporated Data (with RAG) vs. Retrieval-Centric Generation (RCG)</strong></h4><p>The next key question that AI developers need to address is where to find the data used during inference\u200a—\u200awithin the model parametric memory or outside the model (accessible by retrieval). It might be hard to believe, but the first ChatGPT launched in November 2022 did not have any access to data outside the model. It was trained on September 21, 2022 and notoriously had no inclination of events and data past its training date. This major oversight was addressed in 2023 when retrieval plug-ins where added. Today, most models are coupled with a retrieval front-end with exceptions in cases where there is no expectation of accessing large or continuously updating information, such as dedicated programming models.</p><p>Current models have made significant progress on this issue by enhancing the solution platforms with a retrieval-augmented generation (RAG) front-end to allow for extracting information external to the model. An efficient and secure RAG is a requirement in enterprise GenAI deployment, as shown by Microsoft’s introduction of <a href="https://github.com/Azure/GPT-RAG/">GPT-RAG</a> in late 2023. Furthermore, in the blog <a href="https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8">Knowledge Retrieval Takes Center Stage</a>, I cover how in the transition from consumer to business deployment for GenAI, solutions should be built primarily around information external to the model using retrieval-centric generation (RCG).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GmCPpetb_IvrsLOYayDMig.png" /><figcaption>Figure 3. Advantage of RAG vs. RCG. Image credit: Intel\xa0Labs.</figcaption></figure><p>RCG models can be defined as a special case of RAG GenAI solutions designed for systems where the vast majority of data resides outside the model parametric memory and is mostly not seen in pre-training or fine-tuning. With RCG, the primary role of the GenAI model is to interpret rich retrieved information from a company’s indexed data corpus or other curated content. Rather than memorizing data, the model focuses on fine-tuning for targeted constructs, relationships, and functionality. The quality of data in generated output is expected to approach 100% accuracy and timeliness.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PCzPlqrtEwM6zHO5_j14QQ.png" /><figcaption><em>Figure 4. How retrieval works in GenAI platforms. Image credit: Intel\xa0Labs.</em></figcaption></figure><p><a href="https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html">OPEA</a> is a cross-ecosystem effort to ease the adoption and tuning of GenAI systems. Using this composable framework, developers can create and evaluate “open, multi-provider, robust, and composable GenAI solutions that harness the best innovation across the ecosystem.” OPEA is expected to simplify the implementation of enterprise-grade composite GenAI solutions, including RAG, agents, and memory\xa0systems.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JX0ez08uxTQ-urFbU2yxGw.png" /><figcaption><em>Figure 5. OPEA core principles for GenAI implementation.</em> <em>Image credit:\xa0OPEA.</em></figcaption></figure><h4><strong>All-in-One General Purpose vs. Targeted Customized Models</strong></h4><p>Models like GPT-4o, Claude 3, and Gemini 1.5 are general purpose all-in-one foundation models. They are designed to perform a broad range of GenAI from coding to chat to summarization. The latest models have rapidly expanded to perform vision/image tasks, changing their function from just large language models to large multimodal models or vision language models (VLMs). Open source foundation models are headed in the same direction as integrated multimodalities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*grtawDbVYcTt5YLGX-6vgA.png" /><figcaption><em>Figure 6. Advantages of general purpose vs. targeted customized models. Image credit: Intel\xa0Labs.</em></figcaption></figure><p>However, rather than adopting the first wave of consumer-oriented GenAI models in this general-purpose form, most businesses are electing to use some form of specialization. When a healthcare company deploys GenAI technology, they would not use one general model for managing the supply chain, coding in the IT department, and deep medical analytics for managing patient care. Businesses deploy more specialized versions of the technology for each use case. There are several different ways that companies can build specialized GenAI solutions, including domain-specific models, targeted models, customized models, and optimized models.</p><p><em>Domain-specific models</em> are specialized for a particular field of business or an area of interest. There are both proprietary and open source domain-specific models. For example, BloombergGPT, a 50B parameter proprietary large language model specialized for finance, <a href="https://arxiv.org/pdf/2303.17564.pdf">beats the larger GPT-3 175B parameter model</a> on various financial benchmarks. However, small open source domain-specific models can provide an excellent alternative, as demonstrated by <a href="https://arxiv.org/pdf/2306.06031.pdf">FinGPT</a>, which provides accessible and transparent resources to develop FinLLMs. FinGPT 3.3 uses Llama 2 13B as a base model targeted for the financial sector. <a href="https://github.com/AI4Finance-Foundation/FinGPT">In recent benchmarks</a>, FinGPT surpassed BloombergGPT on a variety of tasks and beat GPT-4 handily on financial benchmark tasks like FPB, FiQA-SA, and TFNS. To understand the tremendous potential of this small open source model, it should be noted that FinGPT can be fine-tuned to incorporate new data for less than $300 per fine-tuning.</p><p><em>Targeted models</em> specialize in a family of tasks or functions, such as separate targeted models for <a href="https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2">coding</a>, image generation, question answering, or sentiment analysis. A recent example of a targeted model is <a href="https://huggingface.co/blog/setfit">SetFit</a> from Intel Labs, Hugging Face, and the UKP Lab. This few-shot text classification approach for fine-tuning Sentence Transformers is faster at inference and training, achieving high accuracy with a small number of labeled training data, such as only eight labeled examples per class on the Customer Reviews (CR) sentiment dataset. This small 355M parameter model can best the GPT-3 175B parameter model on the diverse RAFT benchmark.</p><p>It’s important to note that targeted models are independent from domain-specific models. For example, a sentiment analysis solution like <a href="https://huggingface.co/blog/setfit-absa">SetFitABSA</a> has targeted functionality and can be applied to various domains like industrial, entertainment, or hospitality. However, models that are both targeted and domain specialized can be more effective.</p><p><em>Customized models</em> are further fine-tuned and refined to meet particular needs and preferences of companies, organizations, or individuals. By indexing particular content for retrieval, the resulting system becomes highly specific and effective on tasks related to this data (private or public). The open source field offers an array of options to customize the model. For example, Intel Labs used direct preference optimization (DPO) to improve on a Mistral 7B model to create the open source <a href="https://huggingface.co/Intel/neural-chat-7b-v3-1">Intel NeuralChat</a>. Developers also can fine-tune and customize models by using low-rank adaptation of large language (<a href="https://arxiv.org/abs/2106.09685">LoRA</a>) models and its more memory-efficient version,\xa0<a href="https://arxiv.org/abs/2305.14314">QLoRA</a>.</p><p><em>Optimization capabilities</em> are available for open source models. The objective of optimization is to retain the functionality and accuracy of a model while substantially reducing its execution footprint, which can significantly improve cost, latency, and optimal execution of an intended platform. Some techniques used for model optimization include distillation, pruning, compression, and quantization (to 8-bit and even 4-bit). Some methods like mixture of experts (MoE) and <a href="https://arxiv.org/pdf/2211.17192.pdf">speculative decoding</a> can be considered as forms of execution optimization. For example, <a href="https://the-decoder.com/gpt-4-has-a-trillion-parameters/">GPT-4 is reportedly comprised</a> of eight smaller MoE models with 220B parameters. The execution only activates parts of the model, allowing for much more economical inference.</p><h4><strong>Generative-as-a-Service Cloud Execution vs. Managed Execution Environment for Inference</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Eu7lv14XuCM4sBQCnGqn2g.png" /><figcaption><em>Figure 7. Advantages of GaaS vs. managed execution. Image credit: Intel\xa0Labs.</em></figcaption></figure><p>Another key choice for developers to consider is the execution environment. If the company chooses a proprietary model direction, inference execution is done through API or query calls to an abstracted and obscured image of the model running in the cloud. The size of the model and other implementation details are insignificant, except when translated to availability and the cost charged by some key (per token, per query, or unlimited compute license). This approach, sometimes referred to as a <a href="https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7">generative-as-a-service (GaaS)</a> cloud offering, is the principle way for companies to consume very large proprietary models like GPT-4o, Gemini Ultra, and Claude 3. However, GaaS can also be offered for smaller models like Llama\xa03.2.</p><p>There are clear positive aspects to using GaaS for the outsourced intelligence approach. For example, the access is usually instantaneous and easy to use out-of-the-box, alleviating in-house development efforts. There is also the implied promise that when the models or their environment get upgraded, the AI solution developers have access to the latest updates without substantial effort or changes to their setup. Also, the costs are almost entirely operational expenditures (OpEx), which is preferred if the workload is initial or limited. For early-stage adoption and intermittent use, GaaS offers more\xa0support.</p><p>In contrast, when companies choose an internal intelligence approach, the model inference cycle is incorporated and managed within the compute environment and the existing business software setting. This is a viable solution for relatively small models (approximately 30B parameters or less in 2024) and potentially even medium models (50B to 70B parameters in 2024) on a client device, network, on-prem data center, or on-cloud cycles in an environment set with a service provider such as a virtual private cloud\xa0(VPC).</p><p>Models like Llama 3.1 8B or similar can run on the <a href="https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7">developer’s local machine</a> (Mac or PC). Using optimization techniques like <a href="https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk">quantization</a>, the needed user experience can be achieved while operating within the local setting. Using a tool and framework like <a href="https://ollama.ai/">Ollama</a>, developers can manage inference execution locally. Inference cycles can be run on legacy GPUs, <a href="https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html">Intel Xeon</a>, or <a href="https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html">Intel Gaudi AI accelerators</a> in the company’s data center. If inference is run on the model at a service provider, it will be billed as infrastructure-as-a-service (IaaS), using the company’s own setting and execution choices.</p><p>When inference execution is done in the company compute environment (client, edge, on-prem, or IaaS), there is a higher requirement for CapEx for ownership of the computer equipment if it goes beyond adding a workload to existing hardware. While the comparison of OpEx vs. CapEx is complex and depends on many variables, CapEx is preferable when deployment requires broad, continuous, stable usage. This is especially true as smaller models and optimization technologies allow for running advanced open source models on mainstream devices and processors and even local notebooks/desktops.</p><p>Running inference in the company compute environment allows for tighter control over aspects of security and privacy. Reducing data movement and exposure can be valuable in preserving privacy. Furthermore, a retrieval-based AI solution run in a local setting can be supported with fine controls to address potential privacy concerns by giving user-controlled access to information. Security is frequently mentioned as one of the top concerns of companies deploying GenAI and <a href="https://www.intel.com/content/dam/www/public/us/en/documents/solution-briefs/intro-to-confidential-computing-solution-brief.pdf">confidential computing</a> is a primary ask. Confidential computing protects data in use by computing in an attested hardware-based <a href="https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html">Trusted Execution Environment (TEE)</a>.</p><p>Smaller, open source models can run within a company’s most secure application setting. For example, a model running on Xeon can be fully executed within a TEE with limited overhead. As shown in Figure 8, encrypted data remains protected while not in compute. The model is checked for provenance and integrity to protect against tampering. The actual execution is protected from any breach, including by the operating system or other applications, preventing viewing or alteration by untrusted entities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iESqm-hxcVZYQJL-CRoGxg.png" /><figcaption>Figure 8. Security requirements for GenAI. Image credit: Intel\xa0Labs.</figcaption></figure><h4><strong>Summary</strong></h4><p>Generative AI is a transformative technology now under evaluation or active adoption by most companies across all industries and sectors. As AI developers consider their options for the best solution, one of the most important questions they need to address is whether to use external proprietary models or rely on the open source ecosystem. One path is to rely on a large proprietary black-box GaaS solution using RAG, such as GPT-4o or Gemini Ultra. The other path uses a more adaptive and integrative approach\u200a—\u200asmall, selected, and exchanged as needed from a large open source model pool, mainly utilizing company information, customized and optimized based on particular needs, and executed within the existing infrastructure of the company. As mentioned, there could be a combination of choices within these two base strategies.</p><p>I believe that as numerous AI solution developers face this essential dilemma, most will eventually (after a learning period) choose to embed open source GenAI models in their internal compute environment, data, and business setting. They will ride the incredible advancement of the open source and broad ecosystem virtuous cycle of AI innovation, while maintaining control over their costs and\xa0destiny.</p><p>Let’s give AI the final word in solving the AI developer’s dilemma. In a <a href="https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba">staged AI debate</a>, OpenAI’s GPT-4 argued with Microsoft’s open source Orca 2 13B on the merits of using proprietary vs. open source GenAI for future development. Using GPT-4 Turbo as the judge, open source GenAI won the debate. The <a href="https://youtu.be/JuwJLeVlB-w?t=774">winning argument</a>? Orca 2 called for a “more distributed, open, collaborative future of AI development that leverages worldwide talent and aims for collective advancements. This model promises to accelerate innovation and democratize access to AI, and ensure ethical and transparent practices through community governance.”</p><h4><strong>Learn More: GenAI\xa0Series</strong></h4><p><a href="https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8">Knowledge Retrieval Takes Center Stage: GenAI Architecture Shifting from RAG Toward Interpretive Retrieval-Centric Generation (RCG)\xa0Models</a></p><p><a href="https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618">Survival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective AI at\xa0Scale</a></p><p><a href="https://towardsdatascience.com/have-machines-just-made-an-evolutionary-leap-to-speak-in-human-language-319237593aa4">Have Machines Just Made an Evolutionary Leap to Speak in Human Language?</a></p><h4><strong>References</strong></h4><ol><li>Hello GPT-4o. (2024, May 13). <a href="https://openai.com/index/hello-gpt-4o/">https://openai.com/index/hello-gpt-4o/</a></li><li>Open platform for enterprise AI. (n.d.). Open Platform for Enterprise AI (OPEA). <a href="https://opea.dev/">https://opea.dev/</a></li><li>Gartner Poll Finds 55% of Organizations are in Piloting or Production. (2023, October 3). Gartner. <a href="https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai">https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai</a></li><li>Singer, G. (2023, July 28). Survival of the fittest: Compact generative AI models are the future for Cost-Effective AI at scale. <em>Medium</em>. <a href="https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618">https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618</a></li><li>Introducing LLaMA: A foundational, 65-billion-parameter language model. (n.d.). <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">https://ai.meta.com/blog/large-language-model-llama-meta-ai/</a></li><li>#392: OpenAI’s improved ChatGPT should delight both expert and novice developers, &amp; more\u200a—\u200aARK Invest. (n.d.). Ark Invest. <a href="https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers">https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers</a></li><li>Bilenko, M. (2024, May 22). New models added to the Phi-3 family, available on Microsoft Azure. Microsoft Azure Blog. <a href="https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/">https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/</a></li><li>Matthew Berman. (2024, June 2). Open-Source Vision AI\u200a—\u200aSurprising Results! (Phi3 Vision vs LLaMA 3 Vision vs GPT4o) [Video]. YouTube. <a href="https://www.youtube.com/watch?v=PZaNL6igONU">https://www.youtube.com/watch?v=PZaNL6igONU</a></li><li>Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. (n.d.). <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/</a></li><li>Gemini\u200a—\u200aGoogle DeepMind. (n.d.). <a href="https://deepmind.google/technologies/gemini/#introduction">https://deepmind.google/technologies/gemini/#introduction</a></li><li>Introducing the next generation of Claude \\ Anthropic. (n.d.). <a href="https://www.anthropic.com/news/claude-3-family">https://www.anthropic.com/news/claude-3-family</a></li><li>Thompson, A. D. (2024, March 4). The Memo\u200a—\u200aSpecial edition: Claude 3 Opus. The Memo by LifeArchitect.ai. <a href="https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3">https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3</a></li><li>Spataro, J. (2023, May 16). Introducing Microsoft 365 Copilot\u200a—\u200ayour copilot for work\u200a—\u200aThe Official Microsoft Blog. The Official Microsoft Blog. <a href="https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/">https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/</a></li><li>Introducing Llama 3.1: Our most capable models to date. (n.d.). <a href="https://ai.meta.com/blog/meta-llama-3-1/">https://ai.meta.com/blog/meta-llama-3-1/</a></li><li>Mistral AI. (2024, March 4). Mistral Nemo. Mistral AI | Frontier AI in Your Hands. <a href="https://mistral.ai/news/mistral-nemo/">https://mistral.ai/news/mistral-nemo/</a></li><li>Beatty, S. (2024, April 29). Tiny but mighty: The Phi-3 small language models with big potential. Microsoft Research. <a href="https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/">https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/</a></li><li>Hughes, A. (2023, December 16). Phi-2: The surprising power of small language models. Microsoft Research. <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</a></li><li>Azure. (n.d.). GitHub\u200a—\u200aAzure/GPT-RAG. GitHub. <a href="https://github.com/Azure/GPT-RAG/">https://github.com/Azure/GPT-RAG/</a></li><li>Singer, G. (2023, November 16). Knowledge Retrieval Takes Center Stage\u200a—\u200aTowards Data Science. Medium. <a href="https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8">https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8</a></li><li>Introducing the open platform for enterprise AI. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html">https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html</a></li><li>Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., &amp; Mann, G. (2023, March 30). BloombergGPT: A large language model for finance. arXiv.org. <a href="https://arxiv.org/abs/2303.17564">https://arxiv.org/abs/2303.17564</a></li><li>Yang, H., Liu, X., &amp; Wang, C. D. (2023, June 9). FINGPT: Open-Source Financial Large Language Models. arXiv.org. <a href="https://arxiv.org/abs/2306.06031">https://arxiv.org/abs/2306.06031</a></li><li>AI4Finance-Foundation. (n.d.). FinGPT. GitHub. <a href="https://github.com/AI4Finance-Foundation/FinGPT">https://github.com/AI4Finance-Foundation/FinGPT</a></li><li>Starcoder2. (n.d.). GitHub. <a href="https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2">https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2</a></li><li>SetFit: Efficient Few-Shot Learning Without Prompts. (n.d.). <a href="https://huggingface.co/blog/setfit">https://huggingface.co/blog/setfit</a></li><li>SetFitABSA: Few-Shot Aspect Based Sentiment Analysis Using SetFit. (n.d.). <a href="https://huggingface.co/blog/setfit-absa">https://huggingface.co/blog/setfit-absa</a></li><li>Intel/neural-chat-7b-v3–1. Hugging Face. (2023, October 12). <a href="https://huggingface.co/Intel/neural-chat-7b-v3-1">https://huggingface.co/Intel/neural-chat-7b-v3-1</a></li><li>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., &amp; Chen, W. (2021, June 17). LORA: Low-Rank adaptation of Large Language Models. arXiv.org. <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></li><li>Dettmers, T., Pagnoni, A., Holtzman, A., &amp; Zettlemoyer, L. (2023, May 23). QLORA: Efficient Finetuning of Quantized LLMS. arXiv.org. <a href="https://arxiv.org/abs/2305.14314">https://arxiv.org/abs/2305.14314</a></li><li>Leviathan, Y., Kalman, M., &amp; Matias, Y. (2022, November 30). Fast Inference from Transformers via Speculative Decoding. arXiv.org. <a href="https://arxiv.org/abs/2211.17192">https://arxiv.org/abs/2211.17192</a></li><li>Bastian, M. (2023, July 3). GPT-4 has more than a trillion parameters\u200a—\u200aReport. THE DECODER. <a href="https://the-decoder.com/gpt-4-has-a-trillion-parameters/">https://the-decoder.com/gpt-4-has-a-trillion-parameters/</a></li><li>Andriole, S. (2023, September 12). LLAMA, ChatGPT, Bard, Co-Pilot &amp; all the rest. How large language models will become huge cloud services with massive ecosystems. Forbes. <a href="https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7">https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7</a></li><li>Q8-Chat LLM: An efficient generative AI experience on Intel® CPUs. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk">https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk</a></li><li>Ollama. (n.d.). Ollama. <a href="https://ollama.com/">https://ollama.com/</a></li><li>AI Accelerated Intel® Xeon® Scalable Processors Product Brief. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html">https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html</a></li><li>Intel® Gaudi® AI Accelerator products. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html">https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html</a></li><li>Confidential Computing Solutions\u200a—\u200aIntel. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/security/confidential-computing.html">https://www.intel.com/content/www/us/en/security/confidential-computing.html</a></li><li>What is a Trusted Execution Environment? (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html">https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html</a></li><li>Adeojo, J. (2023, December 3). GPT-4 Debates Open Orca-2–13B with Surprising Results! Medium. <a href="https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba">https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba</a></li><li>Data Centric. (2023, November 30). Surprising Debate Showdown: GPT-4 Turbo vs. Orca-2–13B\u200a—\u200aProgrammed with AutoGen! [Video]. YouTube. <a href="https://www.youtube.com/watch?v=JuwJLeVlB-w">https://www.youtube.com/watch?v=JuwJLeVlB-w</a></li></ol><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=453ac735b760" width="1" /><hr /><p><a href="https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760">The AI Developer’s Dilemma: Proprietary AI vs. Open Source Ecosystem</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>'
2024-09-30 12:24:42,151 - INFO - clean content - clean_content='Image credit: Adobe Stock.Fundamental choices impacting integration and deployment at scale of GenAI into businessesBefore a company or a developer adopts generative artificial intelligence (GenAI), they often wonder how to get business value from the integration of AI into their business. With this in mind, a fundamental question arises: Which approach will deliver the best value on investment — a large all-encompassing proprietary model or an open source AI model that can be molded and fine-tuned for a company’s needs? AI adoption strategies fall within a wide spectrum, from accessing a cloud service from a large proprietary frontier model like OpenAI’s GPT-4o to building an internal solution in the company’s compute environment with an open source small model using indexed company data for a targeted set of tasks. Current AI solutions go well beyond the model itself, with a whole ecosystem of retrieval systems, agents, and other functional components such as AI accelerators, which are beneficial for both large and small models. Emergence of cross-industry collaborations like the Open Platform for Enterprise AI (OPEA) further the promise of streamlining the access and structuring of end-to-end open source solutions.This basic choice between the open source ecosystem and a proprietary setting impacts countless business and technical decisions, making it “the AI developer’s dilemma.” I believe that for most enterprise and other business deployments, it makes sense to initially use proprietary models to learn about AI’s potential and minimize early capital expenditure (CapEx). However, for broad sustained deployment, in many cases companies would use ecosystem-based open source targeted solutions, which allows for a cost-effective, adaptable strategy that aligns with evolving business needs and industry trends.GenAI Transition from Consumer to Business DeploymentWhen GenAI burst onto the scene in late 2022 with Open AI’s GPT-3 and ChatGPT 3.5, it mainly garnered consumer interest. As businesses began investigating GenAI, two approaches to deploying GenAI quickly emerged in 2023 — using giant frontier models like ChatGPT vs. the newly introduced small, open source models originally inspired by Meta’s LLaMa model. By early 2024, two basic approaches have solidified, as shown in the columns in Figure 1. With the proprietary AI approach, the company relies on a large closed model to provide all the needed technology value. For example, taking GPT-4o as a proxy for the left column, AI developers would use OpenAI technology for the model, data, security, and compute. With the open source ecosystem AI approach, the company or developer may opt for the right-sized open source model, using corporate or private data, customized functionality, and the necessary compute and security.Both directions are valid and have advantages and disadvantages. It is not an absolute partition and developers can choose components from either approach, but taking either a proprietary or ecosystem-based open source AI path provides the company with a strategy with high internal consistency. While it is expected that both approaches will be broadly deployed, I believe that after an initial learning and transition period, most companies will follow the open source approach. Depending on the usage and setting, open source internal AI may provide significant benefits, including the ability to fine-tune the model and drive deployment using the company’s current infrastructure to run the model at the edge, on the client, in the data center, or as a dedicated service. With new AI fine-tuning tools, deep expertise is less of a barrier.Figure 1. Base approaches to the AI developer’s dilemma. Image credit: Intel Labs.Across all industries, AI developers are using GenAI for a variety of applications. An October 2023 poll by Gartner found that 55% of organizations reported increasing investment in GenAI since early 2023, and many companies are in pilot or production mode for the growing technology. As of the time of the survey, companies were mainly investing in using GenAI for software development, followed closely by marketing and customer service functions. Clearly, the range of AI applications is growing rapidly.Large Proprietary Models vs. Small and Large Open Source ModelsFigure 2: Advantages of large proprietary models, and small and large open source models. For business considerations, see Figure 7 for CapEx and OpEx aspects. Image credit: Intel Labs.In my blog Survival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective AI at Scale, I provide a detailed evaluation of large models vs. small models. In essence, following the introduction of Meta’s LLaMa open source model in February 2023, there has been a virtuous cycle of innovation and rapid improvement where the academia and broad-base ecosystem are creating highly effective models that are 10x to 100x smaller than the large frontier models. A crop of small models, which in 2024 were mostly less than 30 billion parameters, could closely match the capabilities of ChatGPT-style large models containing well over 100B parameters, especially when targeted for particular domains. While GenAI is already being deployed throughout industries for a wide range of business usages, the use of compact models is rising.In addition, open source models are mostly lagging only six to 12 months behind the performance of proprietary models. Using the broad language benchmark MMLU, the improvement pace of the open source models is faster and the gap seems to be closing with proprietary models. For example, OpenAI’s GPT-4o came out this year on May 13 with major multimodal features while Microsoft’s small open source Phi-3-vision was introduced just a week later on May 21. In rudimentary comparisons done on visual recognition and understanding, the models showed some similar competencies, with several tests even favoring the Phi-3-vision model. Initial evaluations of Meta’s Llama 3.2 open source release suggest that its “vision models are competitive with leading foundation models, Claude 3 Haiku and GPT4o-mini on image recognition and a range of visual understanding tasks.”Large models have incredible all-in-one versatility. Developers can choose from a variety of large commercially available proprietary GenAI models, including OpenAI’s GPT-4o multimodal model. Google’s Gemini 1.5 natively multimodal model is available in four sizes: Nano for mobile device app development, Flash small model for specific tasks, Pro for a wide range of tasks, and Ultra for highly complex tasks. And Anthropic’s Claude 3 Opus, rumored to have approximately 2 trillion parameters, has a 200K token context window, allowing users to upload large amounts of information. There’s also another category of out-of-the-box large GenAI models that businesses can use for employee productivity and creative development. Microsoft 365 Copilot integrates the Microsoft 365 Apps suite, Microsoft Graph (content and context from emails, files, meetings, chats, calendars, and contacts), and GPT-4.Most large and small open source models are often more transparent about application frameworks, tool ecosystem, training data, and evaluation platforms. Model architecture, hyperparameters, response quality, input modalities, context window size, and inference cost are partially or fully disclosed. These models often provide information on the dataset so that developers can determine if it meets copyright or quality expectations. This transparency allows developers to easily interchange models for future versions. Among the growing number of small commercially available open source models, Meta’s Llama 3 and 3.1 are based on transformer architecture and available in 8B, 70B, and 405B parameters. Llama 3.2 multimodal model has 11B and 90B, with smaller versions at 1B and 3B parameters. Built in collaboration with NVIDIA, Mistral AI’s Mistral NeMo is a 12B model that features a large 128k context window while Microsoft’s Phi-3 (3.8B, 7B, and 14B) offers Transformer models for reasoning and language understanding tasks. Microsoft highlights Phi models as an example of “the surprising power of small language models” while investing heavily in OpenAI’s very large models. Microsoft’s diverse interest in GenAI indicates that it’s not a one-size-fits-all market.Model-Incorporated Data (with RAG) vs. Retrieval-Centric Generation (RCG)The next key question that AI developers need to address is where to find the data used during inference — within the model parametric memory or outside the model (accessible by retrieval). It might be hard to believe, but the first ChatGPT launched in November 2022 did not have any access to data outside the model. It was trained on September 21, 2022 and notoriously had no inclination of events and data past its training date. This major oversight was addressed in 2023 when retrieval plug-ins where added. Today, most models are coupled with a retrieval front-end with exceptions in cases where there is no expectation of accessing large or continuously updating information, such as dedicated programming models.Current models have made significant progress on this issue by enhancing the solution platforms with a retrieval-augmented generation (RAG) front-end to allow for extracting information external to the model. An efficient and secure RAG is a requirement in enterprise GenAI deployment, as shown by Microsoft’s introduction of GPT-RAG in late 2023. Furthermore, in the blog Knowledge Retrieval Takes Center Stage, I cover how in the transition from consumer to business deployment for GenAI, solutions should be built primarily around information external to the model using retrieval-centric generation (RCG).Figure 3. Advantage of RAG vs. RCG. Image credit: Intel Labs.RCG models can be defined as a special case of RAG GenAI solutions designed for systems where the vast majority of data resides outside the model parametric memory and is mostly not seen in pre-training or fine-tuning. With RCG, the primary role of the GenAI model is to interpret rich retrieved information from a company’s indexed data corpus or other curated content. Rather than memorizing data, the model focuses on fine-tuning for targeted constructs, relationships, and functionality. The quality of data in generated output is expected to approach 100% accuracy and timeliness.Figure 4. How retrieval works in GenAI platforms. Image credit: Intel Labs.OPEA is a cross-ecosystem effort to ease the adoption and tuning of GenAI systems. Using this composable framework, developers can create and evaluate “open, multi-provider, robust, and composable GenAI solutions that harness the best innovation across the ecosystem.” OPEA is expected to simplify the implementation of enterprise-grade composite GenAI solutions, including RAG, agents, and memory systems.Figure 5. OPEA core principles for GenAI implementation. Image credit: OPEA.All-in-One General Purpose vs. Targeted Customized ModelsModels like GPT-4o, Claude 3, and Gemini 1.5 are general purpose all-in-one foundation models. They are designed to perform a broad range of GenAI from coding to chat to summarization. The latest models have rapidly expanded to perform vision/image tasks, changing their function from just large language models to large multimodal models or vision language models (VLMs). Open source foundation models are headed in the same direction as integrated multimodalities.Figure 6. Advantages of general purpose vs. targeted customized models. Image credit: Intel Labs.However, rather than adopting the first wave of consumer-oriented GenAI models in this general-purpose form, most businesses are electing to use some form of specialization. When a healthcare company deploys GenAI technology, they would not use one general model for managing the supply chain, coding in the IT department, and deep medical analytics for managing patient care. Businesses deploy more specialized versions of the technology for each use case. There are several different ways that companies can build specialized GenAI solutions, including domain-specific models, targeted models, customized models, and optimized models.Domain-specific models are specialized for a particular field of business or an area of interest. There are both proprietary and open source domain-specific models. For example, BloombergGPT, a 50B parameter proprietary large language model specialized for finance, beats the larger GPT-3 175B parameter model on various financial benchmarks. However, small open source domain-specific models can provide an excellent alternative, as demonstrated by FinGPT, which provides accessible and transparent resources to develop FinLLMs. FinGPT 3.3 uses Llama 2 13B as a base model targeted for the financial sector. In recent benchmarks, FinGPT surpassed BloombergGPT on a variety of tasks and beat GPT-4 handily on financial benchmark tasks like FPB, FiQA-SA, and TFNS. To understand the tremendous potential of this small open source model, it should be noted that FinGPT can be fine-tuned to incorporate new data for less than $300 per fine-tuning.Targeted models specialize in a family of tasks or functions, such as separate targeted models for coding, image generation, question answering, or sentiment analysis. A recent example of a targeted model is SetFit from Intel Labs, Hugging Face, and the UKP Lab. This few-shot text classification approach for fine-tuning Sentence Transformers is faster at inference and training, achieving high accuracy with a small number of labeled training data, such as only eight labeled examples per class on the Customer Reviews (CR) sentiment dataset. This small 355M parameter model can best the GPT-3 175B parameter model on the diverse RAFT benchmark.It’s important to note that targeted models are independent from domain-specific models. For example, a sentiment analysis solution like SetFitABSA has targeted functionality and can be applied to various domains like industrial, entertainment, or hospitality. However, models that are both targeted and domain specialized can be more effective.Customized models are further fine-tuned and refined to meet particular needs and preferences of companies, organizations, or individuals. By indexing particular content for retrieval, the resulting system becomes highly specific and effective on tasks related to this data (private or public). The open source field offers an array of options to customize the model. For example, Intel Labs used direct preference optimization (DPO) to improve on a Mistral 7B model to create the open source Intel NeuralChat. Developers also can fine-tune and customize models by using low-rank adaptation of large language (LoRA) models and its more memory-efficient version, QLoRA.Optimization capabilities are available for open source models. The objective of optimization is to retain the functionality and accuracy of a model while substantially reducing its execution footprint, which can significantly improve cost, latency, and optimal execution of an intended platform. Some techniques used for model optimization include distillation, pruning, compression, and quantization (to 8-bit and even 4-bit). Some methods like mixture of experts (MoE) and speculative decoding can be considered as forms of execution optimization. For example, GPT-4 is reportedly comprised of eight smaller MoE models with 220B parameters. The execution only activates parts of the model, allowing for much more economical inference.Generative-as-a-Service Cloud Execution vs. Managed Execution Environment for InferenceFigure 7. Advantages of GaaS vs. managed execution. Image credit: Intel Labs.Another key choice for developers to consider is the execution environment. If the company chooses a proprietary model direction, inference execution is done through API or query calls to an abstracted and obscured image of the model running in the cloud. The size of the model and other implementation details are insignificant, except when translated to availability and the cost charged by some key (per token, per query, or unlimited compute license). This approach, sometimes referred to as a generative-as-a-service (GaaS) cloud offering, is the principle way for companies to consume very large proprietary models like GPT-4o, Gemini Ultra, and Claude 3. However, GaaS can also be offered for smaller models like Llama 3.2.There are clear positive aspects to using GaaS for the outsourced intelligence approach. For example, the access is usually instantaneous and easy to use out-of-the-box, alleviating in-house development efforts. There is also the implied promise that when the models or their environment get upgraded, the AI solution developers have access to the latest updates without substantial effort or changes to their setup. Also, the costs are almost entirely operational expenditures (OpEx), which is preferred if the workload is initial or limited. For early-stage adoption and intermittent use, GaaS offers more support.In contrast, when companies choose an internal intelligence approach, the model inference cycle is incorporated and managed within the compute environment and the existing business software setting. This is a viable solution for relatively small models (approximately 30B parameters or less in 2024) and potentially even medium models (50B to 70B parameters in 2024) on a client device, network, on-prem data center, or on-cloud cycles in an environment set with a service provider such as a virtual private cloud (VPC).Models like Llama 3.1 8B or similar can run on the developer’s local machine (Mac or PC). Using optimization techniques like quantization, the needed user experience can be achieved while operating within the local setting. Using a tool and framework like Ollama, developers can manage inference execution locally. Inference cycles can be run on legacy GPUs, Intel Xeon, or Intel Gaudi AI accelerators in the company’s data center. If inference is run on the model at a service provider, it will be billed as infrastructure-as-a-service (IaaS), using the company’s own setting and execution choices.When inference execution is done in the company compute environment (client, edge, on-prem, or IaaS), there is a higher requirement for CapEx for ownership of the computer equipment if it goes beyond adding a workload to existing hardware. While the comparison of OpEx vs. CapEx is complex and depends on many variables, CapEx is preferable when deployment requires broad, continuous, stable usage. This is especially true as smaller models and optimization technologies allow for running advanced open source models on mainstream devices and processors and even local notebooks/desktops.Running inference in the company compute environment allows for tighter control over aspects of security and privacy. Reducing data movement and exposure can be valuable in preserving privacy. Furthermore, a retrieval-based AI solution run in a local setting can be supported with fine controls to address potential privacy concerns by giving user-controlled access to information. Security is frequently mentioned as one of the top concerns of companies deploying GenAI and confidential computing is a primary ask. Confidential computing protects data in use by computing in an attested hardware-based Trusted Execution Environment (TEE).Smaller, open source models can run within a company’s most secure application setting. For example, a model running on Xeon can be fully executed within a TEE with limited overhead. As shown in Figure 8, encrypted data remains protected while not in compute. The model is checked for provenance and integrity to protect against tampering. The actual execution is protected from any breach, including by the operating system or other applications, preventing viewing or alteration by untrusted entities.Figure 8. Security requirements for GenAI. Image credit: Intel Labs.SummaryGenerative AI is a transformative technology now under evaluation or active adoption by most companies across all industries and sectors. As AI developers consider their options for the best solution, one of the most important questions they need to address is whether to use external proprietary models or rely on the open source ecosystem. One path is to rely on a large proprietary black-box GaaS solution using RAG, such as GPT-4o or Gemini Ultra. The other path uses a more adaptive and integrative approach — small, selected, and exchanged as needed from a large open source model pool, mainly utilizing company information, customized and optimized based on particular needs, and executed within the existing infrastructure of the company. As mentioned, there could be a combination of choices within these two base strategies.I believe that as numerous AI solution developers face this essential dilemma, most will eventually (after a learning period) choose to embed open source GenAI models in their internal compute environment, data, and business setting. They will ride the incredible advancement of the open source and broad ecosystem virtuous cycle of AI innovation, while maintaining control over their costs and destiny.Let’s give AI the final word in solving the AI developer’s dilemma. In a staged AI debate, OpenAI’s GPT-4 argued with Microsoft’s open source Orca 2 13B on the merits of using proprietary vs. open source GenAI for future development. Using GPT-4 Turbo as the judge, open source GenAI won the debate. The winning argument? Orca 2 called for a “more distributed, open, collaborative future of AI development that leverages worldwide talent and aims for collective advancements. This model promises to accelerate innovation and democratize access to AI, and ensure ethical and transparent practices through community governance.”Learn More: GenAI SeriesKnowledge Retrieval Takes Center Stage: GenAI Architecture Shifting from RAG Toward Interpretive Retrieval-Centric Generation (RCG) ModelsSurvival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective AI at ScaleHave Machines Just Made an Evolutionary Leap to Speak in Human Language?ReferencesHello GPT-4o. (2024, May 13). platform for enterprise AI. (n.d.). Open Platform for Enterprise AI (OPEA). Poll Finds 55% of Organizations are in Piloting or Production. (2023, October 3). Gartner. G. (2023, July 28). Survival of the fittest: Compact generative AI models are the future for Cost-Effective AI at scale. Medium. LLaMA: A foundational, 65-billion-parameter language model. (n.d.). OpenAI’s improved ChatGPT should delight both expert and novice developers, &amp; more — ARK Invest. (n.d.). Ark Invest. M. (2024, May 22). New models added to the Phi-3 family, available on Microsoft Azure. Microsoft Azure Blog. Berman. (2024, June 2). Open-Source Vision AI — Surprising Results! (Phi3 Vision vs LLaMA 3 Vision vs GPT4o) [Video]. YouTube. 3.2: Revolutionizing edge AI and vision with open, customizable models. (n.d.). — Google DeepMind. (n.d.). the next generation of Claude \\ Anthropic. (n.d.). A. D. (2024, March 4). The Memo — Special edition: Claude 3 Opus. The Memo by LifeArchitect.ai. J. (2023, May 16). Introducing Microsoft 365 Copilot — your copilot for work — The Official Microsoft Blog. The Official Microsoft Blog. Llama 3.1: Our most capable models to date. (n.d.). AI. (2024, March 4). Mistral Nemo. Mistral AI | Frontier AI in Your Hands. S. (2024, April 29). Tiny but mighty: The Phi-3 small language models with big potential. Microsoft Research. A. (2023, December 16). Phi-2: The surprising power of small language models. Microsoft Research. (n.d.). GitHub — Azure/GPT-RAG. GitHub. G. (2023, November 16). Knowledge Retrieval Takes Center Stage — Towards Data Science. Medium. the open platform for enterprise AI. (n.d.). Intel. S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., &amp; Mann, G. (2023, March 30). BloombergGPT: A large language model for finance. arXiv.org. H., Liu, X., &amp; Wang, C. D. (2023, June 9). FINGPT: Open-Source Financial Large Language Models. arXiv.org. (n.d.). FinGPT. GitHub. (n.d.). GitHub. Efficient Few-Shot Learning Without Prompts. (n.d.). Few-Shot Aspect Based Sentiment Analysis Using SetFit. (n.d.). Hugging Face. (2023, October 12). E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., &amp; Chen, W. (2021, June 17). LORA: Low-Rank adaptation of Large Language Models. arXiv.org. T., Pagnoni, A., Holtzman, A., &amp; Zettlemoyer, L. (2023, May 23). QLORA: Efficient Finetuning of Quantized LLMS. arXiv.org. Y., Kalman, M., &amp; Matias, Y. (2022, November 30). Fast Inference from Transformers via Speculative Decoding. arXiv.org. M. (2023, July 3). GPT-4 has more than a trillion parameters — Report. THE DECODER. S. (2023, September 12). LLAMA, ChatGPT, Bard, Co-Pilot &amp; all the rest. How large language models will become huge cloud services with massive ecosystems. Forbes. LLM: An efficient generative AI experience on Intel® CPUs. (n.d.). Intel. (n.d.). Ollama. Accelerated Intel® Xeon® Scalable Processors Product Brief. (n.d.). Intel. Gaudi® AI Accelerator products. (n.d.). Intel. Computing Solutions — Intel. (n.d.). Intel. is a Trusted Execution Environment? (n.d.). Intel. J. (2023, December 3). GPT-4 Debates Open Orca-2–13B with Surprising Results! Medium. Centric. (2023, November 30). Surprising Debate Showdown: GPT-4 Turbo vs. Orca-2–13B — Programmed with AutoGen! [Video]. YouTube. AI Developer’s Dilemma: Proprietary AI vs. Open Source Ecosystem was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.'
2024-09-30 12:24:42,151 - INFO - Generating summary with BART model
2024-09-30 12:24:48,471 - INFO - Generating summary with BART model
2024-09-30 12:24:54,159 - INFO - full_tweet='Companies often wonder how to get business value from the integration of AI into their business. Current..[read more👇🏼] #AI #business https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760?source=rss----7f60cf5620c9---4'
2024-09-30 12:24:54,548 - INFO - Tweet posted successfully: Response(data={'text': 'Companies often wonder how to get business value from the integration of AI into their business. Current..[read more👇🏼] #AI #business https://t.co/gbC6Q3wm9s', 'edit_history_tweet_ids': ['1840729532828967052'], 'id': '1840729532828967052'}, includes={}, errors=[], meta={})
2024-09-30 12:24:54,548 - INFO - Sleeping for 9 minutes and 22 seconds.
2024-09-30 12:34:16,548 - INFO - Saved posted URL: https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760?source=rss----7f60cf5620c9---4 at 2024-09-30 12:34:16
2024-09-30 12:34:16,549 - INFO - Input content - content='I wanted help capturing the themes discussed in a virtual interview. Bluedot pleasantly surprised me.'
2024-09-30 12:34:16,549 - INFO - clean content - clean_content='I wanted help capturing the themes discussed in a virtual interview. Bluedot pleasantly surprised me.'
2024-09-30 12:34:16,549 - INFO - Generating summary with BART model
2024-09-30 12:34:20,679 - INFO - Generating summary with BART model
2024-09-30 12:34:24,766 - INFO - full_tweet='"I wanted help capturing the themes discussed in a virtual interview. Bluedot pleasantly surprised me. I..[read more👇🏼] #wanted #themes https://www.cnet.com/tech/services-and-software/google-chromes-ai-extension-does-your-mundane-prep-work-and-more/#ftag=CAD590a51e'
2024-09-30 12:34:25,096 - INFO - Tweet posted successfully: Response(data={'id': '1840731925696086251', 'text': '"I wanted help capturing the themes discussed in a virtual interview. Bluedot pleasantly surprised me. I..[read more👇🏼] #wanted #themes https://t.co/arQhBmVpbI', 'edit_history_tweet_ids': ['1840731925696086251']}, includes={}, errors=[], meta={})
2024-09-30 12:34:25,097 - INFO - Sleeping for 3 minutes and 52 seconds.
2024-09-30 12:38:17,097 - INFO - Saved posted URL: https://www.cnet.com/tech/services-and-software/google-chromes-ai-extension-does-your-mundane-prep-work-and-more/#ftag=CAD590a51e at 2024-09-30 12:38:17
2024-09-30 12:38:17,097 - INFO - Input content - content="Get up to speed on the rapidly evolving world of AI with our roundup of the week's developments."
2024-09-30 12:38:17,097 - INFO - clean content - clean_content="Get up to speed on the rapidly evolving world of AI with our roundup of the week's developments."
2024-09-30 12:38:17,097 - INFO - Generating summary with BART model
2024-09-30 12:38:22,133 - INFO - Generating summary with BART model
2024-09-30 12:38:27,112 - INFO - full_tweet="This week in AI. Get up to speed on the rapidly evolving world of AI with our roundup of the week's..[read more👇🏼] #AI #week https://www.cnet.com/tech/computing/ai-awkwafina-is-ready-to-answer-questions-while-deepak-chopra-reads-you-a-story/#ftag=CAD590a51e"
2024-09-30 12:38:27,343 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1840732941900456021'], 'text': "This week in AI. Get up to speed on the rapidly evolving world of AI with our roundup of the week's..[read more👇🏼] #AI #week https://t.co/TwhtPSeUHJ", 'id': '1840732941900456021'}, includes={}, errors=[], meta={})
2024-09-30 12:38:27,343 - INFO - Sleeping for 8 minutes and 3 seconds.
2024-09-30 12:46:30,343 - INFO - Saved posted URL: https://www.cnet.com/tech/computing/ai-awkwafina-is-ready-to-answer-questions-while-deepak-chopra-reads-you-a-story/#ftag=CAD590a51e at 2024-09-30 12:46:30
2024-09-30 12:46:30,343 - INFO - Input content - content='<!-- SC_OFF --><div class="md"><p>Hey fellow AI enthusiasts and developers! I\'ve been working on a project to analyze and visualize the most common technical challenges in AI development by looking at Reddit posts on dedicated subs.</p> <h1>Project Goal</h1> <p>The main objective of this project is to identify and track the most prevalent and trending technical challenges, implementation problems, and conceptual hurdles related to AI development. By doing this, we can:</p> <ol> <li>Help developers focus on the most relevant skills and knowledge areas</li> <li>Guide educational content creators in addressing the most pressing issues</li> <li>Provide insights for researchers on areas that need more attention or solutions</li> </ol> <h1>How It Works</h1> <ol> <li><strong>Data Collection</strong>: I fetched the hottest 200 posts from each of the followingAI-related subreddits: r/learnmachinelearning, r/ArtificialIntelligence, r/MachineLearning, <a href="https://www.reddit.com/r/artificial">r/artificial</a>.</li> <li><strong>Screening</strong>: Posts are screened using an LLM to ensure they\'re about specific technical challenges rather than general discussions or news.</li> <li><strong>Summarization and Tagging</strong>: Each relevant post is summarized and tagged with up to three categories from a predefined list of 50 technical areas (e.g., LLM-ARCH for Large Language Model Architecture, CV-OBJ for Computer Vision Object Detection).</li> <li><strong>Analysis</strong>: The system analyzes the frequency of tags, along with the associated upvotes and comments for each category.</li> <li><strong>Visualization</strong>: The results are visualized through various charts and a heatmap, showing the most common challenges and their relative importance in the community.</li> </ol> <h1>Results (here are the <a href="https://imgur.com/a/CHsQMgA">figures</a>):</h1> <ol> <li>Top 15 Tags by Combined Score (frequency + upvotes + comments)</li> <li>Normalized Tag Popularity Heatmap</li> <li>Tag analysis table with individual scores</li> </ol> <h1>Feedback</h1> <p>I\'d love to get your thoughts on this project and how I can make it more useful for the AI development community. Specifically:</p> <ol> <li>Are there any other data sources we should consider beyond Reddit?</li> <li>What additional metrics or analyses would you find valuable?</li> <li>How can I make the results more actionable for developers, educators, or researchers?</li> <li>Are there any potential biases or limitations in this approach that we should address?</li> <li>Would you be interested in a regularly updated dashboard of these trends?</li> </ol> <p>Your insights and suggestions are greatly appreciated!</p> <p><strong>TL;DR: AI Development Challenges Analyzer</strong></p> <ul> <li>Project analyzes Reddit posts to identify common AI development challenges</li> <li>Uses ML to screen, summarize, and tag posts from AI-related subreddits</li> <li>Visualizes results to show most discussed and engaging technical areas</li> <li><a href="https://imgur.com/a/CHsQMgA">View results here</a></li> <li>Seeking feedback to improve the analysis</li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Fixmyn26issue"> /u/Fixmyn26issue </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fstn9m/p_i_tried_to_map_the_most_recurrent_and_popular/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fstn9m/p_i_tried_to_map_the_most_recurrent_and_popular/">[comments]</a></span>'
2024-09-30 12:46:30,344 - INFO - clean content - clean_content="Hey fellow AI enthusiasts and developers! I've been working on a project to analyze and visualize the most common technical challenges in AI development by looking at Reddit posts on dedicated subs. Project Goal The main objective of this project is to identify and track the most prevalent and trending technical challenges, implementation problems, and conceptual hurdles related to AI development. By doing this, we can: Help developers focus on the most relevant skills and knowledge areas Guide educational content creators in addressing the most pressing issues Provide insights for researchers on areas that need more attention or solutions How It Works Data Collection: I fetched the hottest 200 posts from each of the followingAI-related subreddits: r/learnmachinelearning, r/ArtificialIntelligence, r/MachineLearning, r/artificial. Screening: Posts are screened using an LLM to ensure they're about specific technical challenges rather than general discussions or news. Summarization and Tagging: Each relevant post is summarized and tagged with up to three categories from a predefined list of 50 technical areas (e.g., LLM-ARCH for Large Language Model Architecture, CV-OBJ for Computer Vision Object Detection). Analysis: The system analyzes the frequency of tags, along with the associated upvotes and comments for each category. Visualization: The results are visualized through various charts and a heatmap, showing the most common challenges and their relative importance in the community. Results (here are the figures): Top 15 Tags by Combined Score (frequency + upvotes + comments) Normalized Tag Popularity Heatmap Tag analysis table with individual scores Feedback I'd love to get your thoughts on this project and how I can make it more useful for the AI development community. Specifically: Are there any other data sources we should consider beyond Reddit? What additional metrics or analyses would you find valuable? How can I make the results more actionable for developers, educators, or researchers? Are there any potential biases or limitations in this approach that we should address? Would you be interested in a regularly updated dashboard of these trends? Your insights and suggestions are greatly appreciated! TL;DR: AI Development Challenges Analyzer Project analyzes Reddit posts to identify common AI development challenges Uses ML to screen, summarize, and tag posts from AI-related subreddits Visualizes results to show most discussed and engaging technical areas View results here Seeking feedback to improve the analysis &#32; submitted by &#32; /u/Fixmyn26issue [link] &#32; [comments]"
2024-09-30 12:46:30,345 - INFO - Generating summary with BART model
2024-09-30 12:46:36,270 - INFO - Generating summary with BART model
2024-09-30 12:46:42,190 - INFO - full_tweet="I've been working on a project to analyze and visualize the most common technical challenges in AI..[read more👇🏼] #AI #project https://www.reddit.com/r/MachineLearning/comments/1fstn9m/p_i_tried_to_map_the_most_recurrent_and_popular/"
2024-09-30 12:46:42,495 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1840735018714300477'], 'id': '1840735018714300477', 'text': "I've been working on a project to analyze and visualize the most common technical challenges in AI..[read more👇🏼] #AI #project https://t.co/zjGDxxFpEc"}, includes={}, errors=[], meta={})
2024-09-30 12:46:42,496 - INFO - Sleeping for 5 minutes and 12 seconds.
2024-09-30 12:51:54,496 - INFO - Saved posted URL: https://www.reddit.com/r/MachineLearning/comments/1fstn9m/p_i_tried_to_map_the_most_recurrent_and_popular/ at 2024-09-30 12:51:54
2024-09-30 12:51:54,496 - INFO - This link is already posted: https://www.theverge.com/2024/9/30/24258134/raspberry-pi-ai-camera-module-sony-price-availability
2024-09-30 12:51:54,496 - INFO - This link is already posted: https://www.techradar.com/computing/artificial-intelligence/fed-up-with-unnecessary-white-space-when-printing-from-the-web-ai-is-here-to-fix-that
2024-09-30 12:51:54,496 - INFO - This link is already posted: https://www.techradar.com/pro/yoga-pro-7-is-a-reliable-and-efficient-device-for-even-the-most-demanding-creative-workflows-with-a-premium-2-8k-oled-screen-excellent-keyboard-and-a-ryzen-ai-9-365-processor
2024-09-30 12:51:54,496 - INFO - This link is already posted: https://techcrunch.com/2024/09/30/raspberry-pi-launches-camera-module-for-vision-based-ai-applications/
2024-09-30 12:51:54,496 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1fsnzl5/r_looking_for_endorsement_in_arxiv_csai/
2024-09-30 12:51:54,496 - INFO - This link is already posted: https://techcrunch.com/2024/09/29/here-is-whats-illegal-under-californias-18-and-counting-new-ai-laws/
2024-09-30 12:51:54,496 - INFO - This link is already posted: https://techcrunch.com/2024/09/29/gov-newsom-vetoes-californias-controversial-ai-bill-sb-1047/
2024-09-30 12:51:54,496 - INFO - This link is already posted: https://www.theverge.com/2024/9/29/24232172/california-ai-safety-bill-1047-vetoed-gavin-newsom
2024-09-30 12:51:54,496 - INFO - This link is already posted: https://venturebeat.com/ai/onboarding-the-ai-workforce-how-digital-agents-will-redefine-work-itself/
2024-09-30 12:51:54,497 - INFO - This link is already posted: https://towardsdatascience.com/genai-with-python-build-agents-from-scratch-complete-tutorial-4fc1e084e2ec?source=rss----7f60cf5620c9---4
2024-09-30 12:51:54,497 - INFO - This link is already posted: https://www.theverge.com/24255887/social-ai-bots-social-network-chatgpt-vergecast

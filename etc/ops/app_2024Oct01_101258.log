2024-10-01 10:12:58,890 - INFO - Starting configuration setup
2024-10-01 10:12:58,890 - INFO - Environment variables loaded successfully
2024-10-01 10:12:58,890 - INFO - Twitter API client initialized successfully
2024-10-01 10:13:00,750 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-10-01 10:13:07,677 - INFO - Summarization model initialized successfully
2024-10-01 10:13:07,677 - INFO - Starting tweet scheduling
2024-10-01 10:13:07,677 - INFO - Tweeting process started at 2024-10-01 10:13:07.677409!
2024-10-01 10:13:07,677 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Oct01.csv
2024-10-01 10:13:07,677 - INFO - Fetching latest tech news from RSS feeds
2024-10-01 10:13:22,885 - INFO - Total entries found: 617
2024-10-01 10:13:22,889 - INFO - Recent AI-related entries found: 25
2024-10-01 10:13:22,889 - INFO - Input content - content='The importance of implementing AI for now, rather than the future.'
2024-10-01 10:13:22,889 - INFO - clean content - clean_content='The importance of implementing AI for now, rather than the future.'
2024-10-01 10:13:22,889 - INFO - Generating summary with BART model
2024-10-01 10:13:28,243 - INFO - Generating summary with BART model
2024-10-01 10:13:32,961 - INFO - full_tweet='The importance of implementing AI for now, rather than the future, has been underlined by Google. The..[read moreüëáüèº] #AI #future https://www.techradar.com/pro/ai-maximizing-innovation-for-good'
2024-10-01 10:13:33,277 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841058864294191112'], 'text': 'The importance of implementing AI for now, rather than the future, has been underlined by Google. The..[read moreüëáüèº] #AI #future https://t.co/1qvwywuthJ', 'id': '1841058864294191112'}, includes={}, errors=[], meta={})
2024-10-01 10:13:33,277 - INFO - Sleeping for 6 minutes and 31 seconds.
2024-10-01 10:20:04,277 - INFO - Saved posted URL: https://www.techradar.com/pro/ai-maximizing-innovation-for-good at 2024-10-01 10:20:04
2024-10-01 10:20:04,277 - INFO - Input content - content='<!-- SC_OFF --><div class="md"><p><strong>Hey folks! üëã</strong></p> <p>I\'m excited to share <a href="https://goaladvisor.app/">GoalAdvisor</a>, a tool am developing to help you break down your goals, stay organized, and track your progress with AI-powered advisor. Whether you\'re getting into AI/ML, growing your expertise, advancing your career, or just managing personal growth, GoalAdvisor is here to help!</p> <ul> <li><strong>Personalized AI-driven roadmaps</strong>‚Äîtailored to your goals and milestones.</li> <li><strong>Breakdown &amp; task organization</strong>‚Äîwe help you split complex goals into actionable tasks.</li> <li><strong>Progress tracking</strong>‚Äîvisualize your journey and stay motivated along the way.</li> <li><strong>Built by an ML Applied Scientist</strong>‚Äîmy focus is to help more people dive deeper into AI/ML and reach their full potential.</li> <li>üöÄ <strong>Free early access</strong>‚Äîthe first 10 people who join the waitlist will get exclusive early access!</li> </ul> <p><a href="https://goaladvisor.app/">Join Waitlist</a></p> <p>I‚Äôd love for you to give it a spin and share your thoughts!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Lonely_Coffee4382"> /u/Lonely_Coffee4382 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fti0vo/p_achieve_your_goals_with_aidriven_personalized/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fti0vo/p_achieve_your_goals_with_aidriven_personalized/">[comments]</a></span>'
2024-10-01 10:20:04,278 - INFO - clean content - clean_content="Hey folks! üëã I'm excited to share GoalAdvisor, a tool am developing to help you break down your goals, stay organized, and track your progress with AI-powered advisor. Whether you're getting into AI/ML, growing your expertise, advancing your career, or just managing personal growth, GoalAdvisor is here to help! Personalized AI-driven roadmaps‚Äîtailored to your goals and milestones. Breakdown &amp; task organization‚Äîwe help you split complex goals into actionable tasks. Progress tracking‚Äîvisualize your journey and stay motivated along the way. Built by an ML Applied Scientist‚Äîmy focus is to help more people dive deeper into AI/ML and reach their full potential. üöÄ Free early access‚Äîthe first 10 people who join the waitlist will get exclusive early access! Join Waitlist I‚Äôd love for you to give it a spin and share your thoughts! &#32; submitted by &#32; /u/Lonely_Coffee4382 [link] &#32; [comments]"
2024-10-01 10:20:04,278 - INFO - Generating summary with BART model
2024-10-01 10:20:13,515 - INFO - Generating summary with BART model
2024-10-01 10:20:22,735 - INFO - full_tweet='GoalAdvisor helps you break down your goals, stay organized, and track your progress with AI-powered..[read moreüëáüèº] #AI #your https://www.reddit.com/r/MachineLearning/comments/1fti0vo/p_achieve_your_goals_with_aidriven_personalized/'
2024-10-01 10:20:23,057 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841060583019356627'], 'id': '1841060583019356627', 'text': 'GoalAdvisor helps you break down your goals, stay organized, and track your progress with AI-powered..[read moreüëáüèº] #AI #your https://t.co/BPGZMxwoXe'}, includes={}, errors=[], meta={})
2024-10-01 10:20:23,057 - INFO - Sleeping for 5 minutes and 40 seconds.
2024-10-01 10:26:03,058 - INFO - Saved posted URL: https://www.reddit.com/r/MachineLearning/comments/1fti0vo/p_achieve_your_goals_with_aidriven_personalized/ at 2024-10-01 10:26:03
2024-10-01 10:26:03,058 - INFO - Input content - content='By enabling users to chat with an older version of themselves, Future You is aimed at reducing anxiety and guiding young people to make better choices.'
2024-10-01 10:26:03,058 - INFO - clean content - clean_content='By enabling users to chat with an older version of themselves, Future You is aimed at reducing anxiety and guiding young people to make better choices.'
2024-10-01 10:26:03,058 - INFO - Generating summary with BART model
2024-10-01 10:26:07,628 - INFO - Generating summary with BART model
2024-10-01 10:26:12,192 - INFO - full_tweet='Future You is aimed at reducing anxiety and guiding young people to make better choices. By enabling..[read moreüëáüèº] #anxiety #Future https://news.mit.edu/2024/ai-simulation-gives-people-glimpse-potential-future-self-1001'
2024-10-01 10:26:12,442 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841062048509075557'], 'text': 'Future You is aimed at reducing anxiety and guiding young people to make better choices. By enabling..[read moreüëáüèº] #anxiety #Future https://t.co/IsC0iW3zXd', 'id': '1841062048509075557'}, includes={}, errors=[], meta={})
2024-10-01 10:26:12,443 - INFO - Sleeping for 9 minutes and 30 seconds.
2024-10-01 10:35:42,443 - INFO - Saved posted URL: https://news.mit.edu/2024/ai-simulation-gives-people-glimpse-potential-future-self-1001 at 2024-10-01 10:35:42
2024-10-01 10:35:42,444 - INFO - Input content - content='Raspberry Pi releases an AI camera.'
2024-10-01 10:35:42,444 - INFO - clean content - clean_content='Raspberry Pi releases an AI camera.'
2024-10-01 10:35:42,444 - INFO - Generating summary with BART model
2024-10-01 10:35:46,901 - INFO - Generating summary with BART model
2024-10-01 10:35:51,370 - INFO - full_tweet='Raspberry Pi releases an AI camera. Raspberry Pi releases a camera that can take pictures using an AI..[read moreüëáüèº] #AI #mobile https://www.techradar.com/computing/artificial-intelligence/raspberry-pi-takes-a-shot-at-ai-with-a-camera-and-on-device-processing'
2024-10-01 10:35:51,653 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841064477837472006'], 'id': '1841064477837472006', 'text': 'Raspberry Pi releases an AI camera. Raspberry Pi releases a camera that can take pictures using an AI..[read moreüëáüèº] #AI #mobile https://t.co/YdXh3doY2z'}, includes={}, errors=[], meta={})
2024-10-01 10:35:51,653 - INFO - Sleeping for 6 minutes and 57 seconds.
2024-10-01 10:42:48,654 - INFO - Saved posted URL: https://www.techradar.com/computing/artificial-intelligence/raspberry-pi-takes-a-shot-at-ai-with-a-camera-and-on-device-processing at 2024-10-01 10:42:48
2024-10-01 10:42:48,654 - INFO - Input content - content='But the senator who co-wrote the bill says the Golden State will continue to lead in AI regulation.'
2024-10-01 10:42:48,654 - INFO - clean content - clean_content='But the senator who co-wrote the bill says the Golden State will continue to lead in AI regulation.'
2024-10-01 10:42:48,654 - INFO - Generating summary with BART model
2024-10-01 10:42:52,999 - INFO - Generating summary with BART model
2024-10-01 10:42:57,375 - INFO - full_tweet='California will continue to lead in AI regulation, says senator who co-wrote the bill. Senator:..[read moreüëáüèº] #AI #California https://www.cnet.com/tech/services-and-software/california-governor-vetoes-far-reaching-ai-safety-bill/#ftag=CAD590a51e'
2024-10-01 10:42:57,689 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841066264799703253'], 'text': 'California will continue to lead in AI regulation, says senator who co-wrote the bill. Senator:..[read moreüëáüèº] #AI #California https://t.co/jBowsF9ZW1', 'id': '1841066264799703253'}, includes={}, errors=[], meta={})
2024-10-01 10:42:57,689 - INFO - Sleeping for 7 minutes and 45 seconds.
2024-10-01 10:50:42,690 - INFO - Saved posted URL: https://www.cnet.com/tech/services-and-software/california-governor-vetoes-far-reaching-ai-safety-bill/#ftag=CAD590a51e at 2024-10-01 10:50:42
2024-10-01 10:50:42,690 - INFO - This link is already posted: https://www.techradar.com/computing/artificial-intelligence/can-apple-beat-metas-smart-glasses-by-adding-cameras-and-ai-to-airpods-pro
2024-10-01 10:50:42,690 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1ftb4hk/r_synthpai_a_synthetic_dataset_for_personal/
2024-10-01 10:50:42,690 - INFO - This link is already posted: https://techcrunch.com/2024/09/30/meta-wont-say-whether-it-trains-ai-on-ray-ban-meta-smart-glasses-photos/
2024-10-01 10:50:42,690 - INFO - This link is already posted: https://techcrunch.com/2024/09/30/y-combinator-is-being-criticized-after-it-backed-an-ai-startup-that-admits-it-basically-cloned-another-ai-startup/
2024-10-01 10:50:42,690 - INFO - Input content - content="Gov. Gavin Newsom's veto of SB 1047 has been hailed by some industry veterans as good for open source innovation."
2024-10-01 10:50:42,690 - INFO - clean content - clean_content="Gov. Gavin Newsom's veto of SB 1047 has been hailed by some industry veterans as good for open source innovation."
2024-10-01 10:50:42,690 - INFO - Generating summary with BART model
2024-10-01 10:50:47,325 - INFO - Generating summary with BART model
2024-10-01 10:50:51,986 - INFO - full_tweet="Gov. Gavin Newsom's veto of SB 1047 has been hailed by some industry veterans as good for open source..[read moreüëáüèº] #innovation #industry https://venturebeat.com/ai/california-ai-bill-veto-could-allow-smaller-devs-models-to-flourish/"
2024-10-01 10:50:52,245 - INFO - Tweet posted successfully: Response(data={'id': '1841068255131115952', 'text': "Gov. Gavin Newsom's veto of SB 1047 has been hailed by some industry veterans as good for open source..[read moreüëáüèº] #innovation #industry https://t.co/zBoXd9nvfZ", 'edit_history_tweet_ids': ['1841068255131115952']}, includes={}, errors=[], meta={})
2024-10-01 10:50:52,245 - INFO - Sleeping for 4 minutes and 54 seconds.
2024-10-01 10:55:46,245 - INFO - Saved posted URL: https://venturebeat.com/ai/california-ai-bill-veto-could-allow-smaller-devs-models-to-flourish/ at 2024-10-01 10:55:46
2024-10-01 10:55:46,245 - INFO - This link is already posted: https://techcrunch.com/2024/09/30/11x-ai-a-developer-of-ai-sales-reps-has-raised-50m-series-b-led-by-a16z-sources-say/
2024-10-01 10:55:46,246 - INFO - Input content - content="The startup from MIT's CSAIL says its Liquid Foundation Models have smaller memory needs thanks to a post-transformer architecture."
2024-10-01 10:55:46,246 - INFO - clean content - clean_content="The startup from MIT's CSAIL says its Liquid Foundation Models have smaller memory needs thanks to a post-transformer architecture."
2024-10-01 10:55:46,246 - INFO - Generating summary with BART model
2024-10-01 10:55:50,589 - INFO - Generating summary with BART model
2024-10-01 10:55:54,900 - INFO - full_tweet='Liquid Foundation Models have smaller memory needs thanks to a post-transformer architecture. The startup..[read moreüëáüèº] #startup #Foundation https://venturebeat.com/ai/mit-spinoff-liquid-debuts-non-transformer-ai-models-and-theyre-already-state-of-the-art/'
2024-10-01 10:55:55,159 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841069525774524832'], 'text': 'Liquid Foundation Models have smaller memory needs thanks to a post-transformer architecture. The startup..[read moreüëáüèº] #startup #Foundation https://t.co/fKknnnwGeh', 'id': '1841069525774524832'}, includes={}, errors=[], meta={})
2024-10-01 10:55:55,159 - INFO - Sleeping for 3 minutes and 13 seconds.
2024-10-01 10:59:08,159 - INFO - Saved posted URL: https://venturebeat.com/ai/mit-spinoff-liquid-debuts-non-transformer-ai-models-and-theyre-already-state-of-the-art/ at 2024-10-01 10:59:08
2024-10-01 10:59:08,159 - INFO - Input content - content="The Raspberry Pi AI Kit is only compatible with the Raspberry Pi 5 but the new AI Camera with a Sony IMX500 image sensor works with all of the company's devices."
2024-10-01 10:59:08,159 - INFO - clean content - clean_content="The Raspberry Pi AI Kit is only compatible with the Raspberry Pi 5 but the new AI Camera with a Sony IMX500 image sensor works with all of the company's devices."
2024-10-01 10:59:08,160 - INFO - Generating summary with BART model
2024-10-01 10:59:12,497 - INFO - Generating summary with BART model
2024-10-01 10:59:16,802 - INFO - full_tweet='The Raspberry Pi AI Kit is only compatible with the Raspberry Pi 5. The new AI Camera with a Sony IMX500..[read moreüëáüèº] #AI #with https://www.pcmag.com/news/70-ai-camera-from-raspberry-pi-sony-works-with-all-pi-models'
2024-10-01 10:59:17,006 - INFO - Tweet posted successfully: Response(data={'text': 'The Raspberry Pi AI Kit is only compatible with the Raspberry Pi 5. The new AI Camera with a Sony IMX500..[read moreüëáüèº] #AI #with https://t.co/qDfJUXyWoC', 'id': '1841070372340265265', 'edit_history_tweet_ids': ['1841070372340265265']}, includes={}, errors=[], meta={})
2024-10-01 10:59:17,007 - INFO - Sleeping for 2 minutes and 32 seconds.
2024-10-01 11:01:49,007 - INFO - Saved posted URL: https://www.pcmag.com/news/70-ai-camera-from-raspberry-pi-sony-works-with-all-pi-models at 2024-10-01 11:01:49
2024-10-01 11:01:49,007 - INFO - Input content - content='<!-- SC_OFF --><div class="md"><p>If you\'re looking to cut down on download times from Hugging Face and also help reduce their server load‚Äî(Clem Delangue mentions HF handles a whopping 6PB of data daily!)</p> <p>‚Äî&gt; you might find ZipNN useful.</p> <p>ZipNN is an open-source Python library, available under the MIT license, tailored for compressing AI models without losing accuracy (similar to Zip but tailored for Neural Networks).</p> <p>It uses lossless compression to reduce model sizes by 33%, saving third of your download time.</p> <p>ZipNN has a plugin to HF so you only need to add one line of code.</p> <p>Check it out here:</p> <p><a href="https://github.com/zipnn/zipnn">https://github.com/zipnn/zipnn</a></p> <p>There are already a few compressed models with ZipNN on Hugging Face, and it\'s straightforward to upload more if you\'re interested.</p> <p>The newest one is Llama-3.2-11B-Vision-Instruct-ZipNN-Compressed</p> <p>Take a look at this Kaggle notebook:</p> <p>For a practical example of Llama-3.2 you can at this Kaggle notebook:</p> <p><a href="https://www.kaggle.com/code/royleibovitz/huggingface-llama-3-2-example">https://www.kaggle.com/code/royleibovitz/huggingface-llama-3-2-example</a></p> <p>More examples are available in the ZipNN repo:<br /> <a href="https://github.com/zipnn/zipnn/tree/main/examples">https://github.com/zipnn/zipnn/tree/main/examples</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Candid_Raccoon2102"> /u/Candid_Raccoon2102 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1ft2v10/project_a_lossless_compression_library_taliored/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1ft2v10/project_a_lossless_compression_library_taliored/">[comments]</a></span>'
2024-10-01 11:01:49,008 - INFO - clean content - clean_content="If you're looking to cut down on download times from Hugging Face and also help reduce their server load‚Äî(Clem Delangue mentions HF handles a whopping 6PB of data daily!) ‚Äî&gt; you might find ZipNN useful. ZipNN is an open-source Python library, available under the MIT license, tailored for compressing AI models without losing accuracy (similar to Zip but tailored for Neural Networks). It uses lossless compression to reduce model sizes by 33%, saving third of your download time. ZipNN has a plugin to HF so you only need to add one line of code. Check it out here: There are already a few compressed models with ZipNN on Hugging Face, and it's straightforward to upload more if you're interested. The newest one is Llama-3.2-11B-Vision-Instruct-ZipNN-Compressed Take a look at this Kaggle notebook: For a practical example of Llama-3.2 you can at this Kaggle notebook: More examples are available in the ZipNN repo: &#32; submitted by &#32; /u/Candid_Raccoon2102 [link] &#32; [comments]"
2024-10-01 11:01:49,008 - INFO - Generating summary with BART model
2024-10-01 11:01:55,523 - INFO - Generating summary with BART model
2024-10-01 11:02:01,984 - INFO - full_tweet='ZipNN is an open-source Python library tailored for compressing AI models without losing accuracy. It..[read moreüëáüèº] #AI #compressing https://www.reddit.com/r/MachineLearning/comments/1ft2v10/project_a_lossless_compression_library_taliored/'
2024-10-01 11:02:02,194 - INFO - Tweet posted successfully: Response(data={'id': '1841071065188999227', 'text': 'ZipNN is an open-source Python library tailored for compressing AI models without losing accuracy. It..[read moreüëáüèº] #AI #compressing https://t.co/H5zRnMLEEf', 'edit_history_tweet_ids': ['1841071065188999227']}, includes={}, errors=[], meta={})
2024-10-01 11:02:02,194 - INFO - Sleeping for 7 minutes and 41 seconds.
2024-10-01 11:09:43,194 - INFO - Saved posted URL: https://www.reddit.com/r/MachineLearning/comments/1ft2v10/project_a_lossless_compression_library_taliored/ at 2024-10-01 11:09:43
2024-10-01 11:09:43,194 - INFO - Input content - content="Open NotebookLM, a free open-source AI tool developed in one day, challenges Google's NotebookLM by converting PDFs to podcasts, highlighting rapid AI development and its implications."
2024-10-01 11:09:43,194 - INFO - clean content - clean_content="Open NotebookLM, a free open-source AI tool developed in one day, challenges Google's NotebookLM by converting PDFs to podcasts, highlighting rapid AI development and its implications."
2024-10-01 11:09:43,195 - INFO - Generating summary with BART model
2024-10-01 11:09:46,978 - INFO - Generating summary with BART model
2024-10-01 11:09:50,752 - INFO - full_tweet='Open NotebookLM is a free open-source AI tool developed in one day. It converts PDFs to podcasts to..[read moreüëáüèº] #AI #development https://venturebeat.com/ai/this-open-source-ai-tool-was-built-in-a-day-and-its-coming-for-googles-notebooklm/'
2024-10-01 11:09:51,020 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841073031684280320'], 'id': '1841073031684280320', 'text': 'Open NotebookLM is a free open-source AI tool developed in one day. It converts PDFs to podcasts to..[read moreüëáüèº] #AI #development https://t.co/liK32utLEF'}, includes={}, errors=[], meta={})
2024-10-01 11:09:51,020 - INFO - Sleeping for 5 minutes and 38 seconds.
2024-10-01 11:15:29,021 - INFO - Saved posted URL: https://venturebeat.com/ai/this-open-source-ai-tool-was-built-in-a-day-and-its-coming-for-googles-notebooklm/ at 2024-10-01 11:15:29
2024-10-01 11:15:29,021 - INFO - Input content - content='Google NotebookLM product leader\xa0Raiza Martin says her team will be adding new updates to allow users to control more of the Audio Overviews.'
2024-10-01 11:15:29,021 - INFO - clean content - clean_content='Google NotebookLM product leader Raiza Martin says her team will be adding new updates to allow users to control more of the Audio Overviews.'
2024-10-01 11:15:29,021 - INFO - Generating summary with BART model
2024-10-01 11:15:32,748 - INFO - Generating summary with BART model
2024-10-01 11:15:36,484 - INFO - full_tweet='Google NotebookLM product leader Raiza Martin says her team will be adding new updates to allow users to..[read moreüëáüèº] #Overviews #product https://venturebeat.com/ai/google-notebooklm-leader-says-more-controls-coming-for-ai-generated-podcasts/'
2024-10-01 11:15:36,771 - INFO - Tweet posted successfully: Response(data={'text': 'Google NotebookLM product leader Raiza Martin says her team will be adding new updates to allow users to..[read moreüëáüèº] #Overviews #product https://t.co/6XdUUqxw6I', 'edit_history_tweet_ids': ['1841074481801970022'], 'id': '1841074481801970022'}, includes={}, errors=[], meta={})
2024-10-01 11:15:36,771 - INFO - Sleeping for 2 minutes and 2 seconds.
2024-10-01 11:17:38,771 - INFO - Saved posted URL: https://venturebeat.com/ai/google-notebooklm-leader-says-more-controls-coming-for-ai-generated-podcasts/ at 2024-10-01 11:17:38
2024-10-01 11:17:38,771 - INFO - Input content - content='<p>It‚Äôs been quite the year for game industry exec Pany Haritatos.\xa0<br />\nLast month, he quietly closed an oversubscribed $28 million from Netflix, Dell a16z, and others.</p>\n<p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>'
2024-10-01 11:17:38,772 - INFO - clean content - clean_content='It‚Äôs been quite the year for game industry exec Pany Haritatos. Last month, he quietly closed an oversubscribed $28 million from Netflix, Dell a16z, and others. ¬© 2024 TechCrunch. All rights reserved. For personal use only.'
2024-10-01 11:17:38,772 - INFO - Generating summary with BART model
2024-10-01 11:17:44,101 - INFO - Generating summary with BART model
2024-10-01 11:17:49,427 - INFO - full_tweet='Last month, Pany Haritatos closed an oversubscribed $28 million from Netflix, Dell a16z, and others. It‚Äôs..[read moreüëáüèº] #oversubscribed #Panyharitatos https://techcrunch.com/2024/09/30/series-the-genai-startup-reinventing-game-development-has-quietly-raised-28m-from-netflix-dell-a16z-others/'
2024-10-01 11:17:49,620 - INFO - Tweet posted successfully: Response(data={'id': '1841075039027839454', 'text': 'Last month, Pany Haritatos closed an oversubscribed $28 million from Netflix, Dell a16z, and others. It‚Äôs..[read moreüëáüèº] #oversubscribed #Panyharitatos https://t.co/bpSwt8IFcU', 'edit_history_tweet_ids': ['1841075039027839454']}, includes={}, errors=[], meta={})
2024-10-01 11:17:49,620 - INFO - Sleeping for 3 minutes and 17 seconds.
2024-10-01 11:21:06,621 - INFO - Saved posted URL: https://techcrunch.com/2024/09/30/series-the-genai-startup-reinventing-game-development-has-quietly-raised-28m-from-netflix-dell-a16z-others/ at 2024-10-01 11:21:06
2024-10-01 11:21:06,621 - INFO - Input content - content="Gavin Newsom says the bill gives the public 'a false sense of security' by targeting Big Tech, which lobbied against the bill, and ignores the threats presented by smaller companies."
2024-10-01 11:21:06,621 - INFO - clean content - clean_content="Gavin Newsom says the bill gives the public 'a false sense of security' by targeting Big Tech, which lobbied against the bill, and ignores the threats presented by smaller companies."
2024-10-01 11:21:06,621 - INFO - Generating summary with BART model
2024-10-01 11:21:11,511 - INFO - Generating summary with BART model
2024-10-01 11:21:16,323 - INFO - full_tweet="Gavin Newsom says the bill gives the public 'a false sense of security' by targeting Big Tech, which..[read moreüëáüèº] #Tech #Newsom https://www.pcmag.com/news/california-governor-vetoes-ai-safety-bill-for-only-targeting-large-models"
2024-10-01 11:21:16,514 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841075906812448862'], 'text': "Gavin Newsom says the bill gives the public 'a false sense of security' by targeting Big Tech, which..[read moreüëáüèº] #Tech #Newsom https://t.co/9RDEaLwmaI", 'id': '1841075906812448862'}, includes={}, errors=[], meta={})
2024-10-01 11:21:16,514 - INFO - Sleeping for 1 minutes and 36 seconds.
2024-10-01 11:22:52,514 - INFO - Saved posted URL: https://www.pcmag.com/news/california-governor-vetoes-ai-safety-bill-for-only-targeting-large-models at 2024-10-01 11:22:52
2024-10-01 11:22:52,514 - INFO - Input content - content='While big platforms like Reddit have signed deals with the AI giants, YouTube leaves licensing in the hands of individual creators. The ‚ÄúLicense to Scrape‚Äù program aims to give those streaming stars proper leverage.'
2024-10-01 11:22:52,515 - INFO - clean content - clean_content='While big platforms like Reddit have signed deals with the AI giants, YouTube leaves licensing in the hands of individual creators. The ‚ÄúLicense to Scrape‚Äù program aims to give those streaming stars proper leverage.'
2024-10-01 11:22:52,515 - INFO - Generating summary with BART model
2024-10-01 11:22:56,214 - INFO - Generating summary with BART model
2024-10-01 11:22:59,899 - INFO - full_tweet='YouTube is giving streaming stars more control over their content. The ‚ÄúLicense to Scrape‚Äô program aims..[read moreüëáüèº] #streaming #stars https://www.wired.com/story/license-to-scrape-youtube-ai-data-license-creators/'
2024-10-01 11:23:00,085 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841076341212336431'], 'text': 'YouTube is giving streaming stars more control over their content. The ‚ÄúLicense to Scrape‚Äô program aims..[read moreüëáüèº] #streaming #stars https://t.co/QQ1QYvDMyo', 'id': '1841076341212336431'}, includes={}, errors=[], meta={})
2024-10-01 11:23:00,085 - INFO - Sleeping for 2 minutes and 36 seconds.
2024-10-01 11:25:36,085 - INFO - Saved posted URL: https://www.wired.com/story/license-to-scrape-youtube-ai-data-license-creators/ at 2024-10-01 11:25:36
2024-10-01 11:25:36,086 - INFO - Input content - content='<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://towardsdatascience.com/causal-ai-at-kdd-2024-why-companies-that-wont-jump-on-the-causal-train-now-will-have-a-harder-bdd0671543cf?source=rss----7f60cf5620c9---4"><img src="https://cdn-images-1.medium.com/max/2600/1*BdwEdrlke0hP3FeQUS63wA.jpeg" width="4000" /></a></p><p class="medium-feed-snippet">Building Causal Expertise is a Process, Not an Event</p><p class="medium-feed-link"><a href="https://towardsdatascience.com/causal-ai-at-kdd-2024-why-companies-that-wont-jump-on-the-causal-train-now-will-have-a-harder-bdd0671543cf?source=rss----7f60cf5620c9---4">Continue reading on Towards Data Science ¬ª</a></p></div>'
2024-10-01 11:25:36,086 - INFO - clean content - clean_content='Building Causal Expertise is a Process, Not an EventContinue reading on Towards Data Science ¬ª'
2024-10-01 11:25:36,086 - INFO - Generating summary with BART model
2024-10-01 11:25:40,358 - INFO - Generating summary with BART model
2024-10-01 11:25:44,621 - INFO - full_tweet='Building Causal Expertise is a Process, Not an Event. Data Science is a process, not an event. We need to..[read moreüëáüèº] #Data #data https://towardsdatascience.com/causal-ai-at-kdd-2024-why-companies-that-wont-jump-on-the-causal-train-now-will-have-a-harder-bdd0671543cf?source=rss----7f60cf5620c9---4'
2024-10-01 11:25:44,808 - INFO - Tweet posted successfully: Response(data={'id': '1841077032110739603', 'text': 'Building Causal Expertise is a Process, Not an Event. Data Science is a process, not an event. We need to..[read moreüëáüèº] #Data #data https://t.co/yfDqnp6rVR', 'edit_history_tweet_ids': ['1841077032110739603']}, includes={}, errors=[], meta={})
2024-10-01 11:25:44,809 - INFO - Sleeping for 5 minutes and 13 seconds.
2024-10-01 11:30:57,809 - INFO - Saved posted URL: https://towardsdatascience.com/causal-ai-at-kdd-2024-why-companies-that-wont-jump-on-the-causal-train-now-will-have-a-harder-bdd0671543cf?source=rss----7f60cf5620c9---4 at 2024-10-01 11:30:57
2024-10-01 11:30:57,809 - INFO - Input content - content='<!-- SC_OFF --><div class="md"><p>can you share them</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Internal_Complaint64"> /u/Internal_Complaint64 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fsv7js/discussion_what_are_some_the_informative_blogs_on/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fsv7js/discussion_what_are_some_the_informative_blogs_on/">[comments]</a></span>'
2024-10-01 11:30:57,809 - INFO - clean content - clean_content='can you share them &#32; submitted by &#32; /u/Internal_Complaint64 [link] &#32; [comments]'
2024-10-01 11:30:57,809 - INFO - Generating summary with BART model
2024-10-01 11:31:02,782 - INFO - Generating summary with BART model
2024-10-01 11:31:07,705 - INFO - full_tweet='can you share them &#32; submitted by & #32; /u/Internal_Complaint64 [link] &# 32; [comments] #submitted #comments https://www.reddit.com/r/MachineLearning/comments/1fsv7js/discussion_what_are_some_the_informative_blogs_on/'
2024-10-01 11:31:07,989 - INFO - Tweet posted successfully: Response(data={'text': 'can you share them &amp;#32; submitted by &amp; #32; /u/Internal_Complaint64 [link] &amp;# 32; [comments] #submitted #comments https://t.co/a2xBj0WHav', 'edit_history_tweet_ids': ['1841078387642646771'], 'id': '1841078387642646771'}, includes={}, errors=[], meta={})
2024-10-01 11:31:07,989 - INFO - Sleeping for 5 minutes and 57 seconds.
2024-10-01 11:37:04,990 - INFO - Saved posted URL: https://www.reddit.com/r/MachineLearning/comments/1fsv7js/discussion_what_are_some_the_informative_blogs_on/ at 2024-10-01 11:37:04
2024-10-01 11:37:04,990 - INFO - Input content - content='New dataset of ‚Äúillusory‚Äù faces reveals differences between human and algorithmic face detection, links to animal face recognition, and a formula predicting where people most often perceive faces.'
2024-10-01 11:37:04,990 - INFO - clean content - clean_content='New dataset of ‚Äúillusory‚Äù faces reveals differences between human and algorithmic face detection, links to animal face recognition, and a formula predicting where people most often perceive faces.'
2024-10-01 11:37:04,990 - INFO - Generating summary with BART model
2024-10-01 11:37:09,519 - INFO - Generating summary with BART model
2024-10-01 11:37:14,041 - INFO - full_tweet='New dataset of ‚Äúillusory‚Äù faces reveals differences between human and algorithmic face detection, links..[read moreüëáüèº] #faces #face https://news.mit.edu/2024/ai-pareidolia-can-machines-spot-faces-in-inanimate-objects-0930'
2024-10-01 11:37:14,349 - INFO - Tweet posted successfully: Response(data={'text': 'New dataset of ‚Äúillusory‚Äù faces reveals differences between human and algorithmic face detection, links..[read moreüëáüèº] #faces #face https://t.co/WI9oSneq8l', 'id': '1841079924179837175', 'edit_history_tweet_ids': ['1841079924179837175']}, includes={}, errors=[], meta={})
2024-10-01 11:37:14,349 - INFO - Sleeping for 9 minutes and 9 seconds.
2024-10-01 11:46:23,350 - INFO - Saved posted URL: https://news.mit.edu/2024/ai-pareidolia-can-machines-spot-faces-in-inanimate-objects-0930 at 2024-10-01 11:46:23
2024-10-01 11:46:23,350 - INFO - Input content - content='<p>Artisan, a startup aiming to replace traditional sales software with AI-powered virtual employees, announced on Monday that it has raised $11.5 million in seed funding. The company&#8217;s first AI assistant, named Ava, automates many tasks typically handled by business development representatives, like researching leads and crafting personalized outreach emails. Founded just last year, Artisan has&#160;[&#8230;]\n</p>'
2024-10-01 11:46:23,350 - INFO - clean content - clean_content='Artisan, a startup aiming to replace traditional sales software with AI-powered virtual employees, announced on Monday that it has raised $11.5 million in seed funding. The company&#8217;s first AI assistant, named Ava, automates many tasks typically handled by business development representatives, like researching leads and crafting personalized outreach emails. Founded just last year, Artisan has&#160;[&#8230;]'
2024-10-01 11:46:23,350 - INFO - Generating summary with BART model
2024-10-01 11:46:27,613 - INFO - Generating summary with BART model
2024-10-01 11:46:31,877 - INFO - full_tweet="Artisan has raised $11.5 million in seed funding. The company's first AI assistant, Ava, automates many..[read moreüëáüèº] #AI #representatives https://venturebeat.com/business/artisan-raises-11-5m-to-deploy-ai-employees-for-sales-teams/"
2024-10-01 11:46:32,142 - INFO - Tweet posted successfully: Response(data={'text': "Artisan has raised $11.5 million in seed funding. The company's first AI assistant, Ava, automates many..[read moreüëáüèº] #AI #representatives https://t.co/0hiAHqfNsS", 'id': '1841082263779291316', 'edit_history_tweet_ids': ['1841082263779291316']}, includes={}, errors=[], meta={})
2024-10-01 11:46:32,142 - INFO - Sleeping for 7 minutes and 30 seconds.
2024-10-01 11:54:02,142 - INFO - Saved posted URL: https://venturebeat.com/business/artisan-raises-11-5m-to-deploy-ai-employees-for-sales-teams/ at 2024-10-01 11:54:02
2024-10-01 11:54:02,143 - INFO - Input content - content='<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*qk4WeIf8CzaRjVc07CWFhg.jpeg" /><figcaption><em>Image credit: Adobe\xa0Stock.</em></figcaption></figure><h4><strong><em>Fundamental choices impacting integration and deployment at scale of GenAI into businesses</em></strong></h4><p>Before a company or a developer adopts generative artificial intelligence (GenAI), they often wonder how to get business value from the integration of AI into their business. With this in mind, a fundamental question arises: Which approach will deliver the best value on investment\u200a‚Äî\u200aa large all-encompassing proprietary model or an open source AI model that can be molded and fine-tuned for a company‚Äôs needs? AI adoption strategies fall within a wide spectrum, from accessing a cloud service from a large proprietary frontier model like <a href="https://openai.com/index/hello-gpt-4o/">OpenAI‚Äôs GPT-4o</a> to building an internal solution in the company‚Äôs compute environment with an open source small model using indexed company data for a targeted set of tasks. Current AI solutions go well beyond the model itself, with a whole ecosystem of retrieval systems, agents, and other functional components such as AI accelerators, which are beneficial for both large and small models. Emergence of cross-industry collaborations like the <a href="https://opea.dev/">Open Platform for Enterprise AI (OPEA)</a> further the promise of streamlining the access and structuring of end-to-end open source solutions.</p><p>This basic choice between the open source ecosystem and a proprietary setting impacts countless business and technical decisions, making it ‚Äúthe AI developer‚Äôs dilemma.‚Äù I believe that for most enterprise and other business deployments, it makes sense to initially use proprietary models to learn about AI‚Äôs potential and minimize early capital expenditure (CapEx). However, for broad sustained deployment, in many cases companies would use ecosystem-based open source targeted solutions, which allows for a cost-effective, adaptable strategy that aligns with evolving business needs and industry\xa0trends.</p><h4><strong>GenAI Transition from Consumer to Business Deployment</strong></h4><p>When GenAI burst onto the scene in late 2022 with Open AI‚Äôs GPT-3 and ChatGPT 3.5, it mainly garnered consumer interest. As businesses began investigating GenAI, two approaches to deploying GenAI quickly emerged in 2023\u200a‚Äî\u200ausing giant frontier models like ChatGPT vs. the newly introduced small, open source models originally inspired by Meta‚Äôs LLaMa model. By early 2024, two basic approaches have solidified, as shown in the columns in Figure 1. With the proprietary AI approach, the company relies on a large closed model to provide all the needed technology value. For example, taking GPT-4o as a proxy for the left column, AI developers would use OpenAI technology for the model, data, security, and compute. With the open source ecosystem AI approach, the company or developer may opt for the right-sized open source model, using corporate or private data, customized functionality, and the necessary compute and security.</p><p>Both directions are valid and have advantages and disadvantages. It is not an absolute partition and developers can choose components from either approach, but taking either a proprietary or ecosystem-based open source AI path provides the company with a strategy with high internal consistency. While it is expected that both approaches will be broadly deployed, I believe that after an initial learning and transition period, most companies will follow the open source approach. Depending on the usage and setting, open source internal AI may provide significant benefits, including the ability to fine-tune the model and drive deployment using the company‚Äôs current infrastructure to run the model at the edge, on the client, in the data center, or as a dedicated service. With new AI fine-tuning tools, deep expertise is less of a\xa0barrier.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GX8paB0ZR20Hsk54HE-DlA.png" /><figcaption><em>Figure 1. Base approaches to the AI developer‚Äôs dilemma. Image credit: Intel\xa0Labs.</em></figcaption></figure><p>Across all industries, AI developers are using GenAI for a variety of applications. An October 2023 <a href="https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai">poll by Gartner</a> found that 55% of organizations reported increasing investment in GenAI since early 2023, and many companies are in pilot or production mode for the growing technology. As of the time of the survey, companies were mainly investing in using GenAI for software development, followed closely by marketing and customer service functions. Clearly, the range of AI applications is growing\xa0rapidly.</p><h4><strong>Large Proprietary Models vs. Small and Large Open Source\xa0Models</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QYYLFTdMxT7yg2e6UZYSzA.png" /><figcaption><em>Figure 2: Advantages of large proprietary models, and small and large open source models. For business considerations, see Figure 7 for CapEx and OpEx aspects. Image credit: Intel\xa0Labs.</em></figcaption></figure><p>In my blog <a href="https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618">Survival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective AI at Scale</a>, I provide a detailed evaluation of large models vs. small models. In essence, following the introduction of <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">Meta‚Äôs LLaMa open source model in February 2023</a>, there has been a virtuous cycle of innovation and rapid improvement where the academia and broad-base ecosystem are creating highly effective models that are 10x to 100x smaller than the large frontier models. A crop of small models, which in 2024 were mostly less than 30 billion parameters, could <a href="https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/">closely match</a> the capabilities of ChatGPT-style large models containing well over 100B parameters, especially when targeted for particular domains. While GenAI is already being deployed throughout industries for a wide range of business usages, the use of compact models is\xa0rising.</p><p>In addition, open source models are mostly lagging <a href="https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers">only six to 12 months behind</a> the performance of proprietary models. Using the broad language benchmark MMLU, the improvement pace of the open source models is faster and the gap seems to be closing with proprietary models. For example, OpenAI‚Äôs <a href="https://openai.com/index/hello-gpt-4o/">GPT-4o</a> came out this year on May 13 with major multimodal features while Microsoft‚Äôs small open source <a href="https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/">Phi-3-vision</a> was introduced just a week later on May 21. In <a href="https://youtu.be/PZaNL6igONU?si=jCvhwvWBoZFnRG5X">rudimentary comparisons</a> done on visual recognition and understanding, the models showed some similar competencies, with several tests even favoring the Phi-3-vision model. <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Initial evaluations of Meta‚Äôs Llama 3.2 open source release</a> suggest that its ‚Äúvision models are competitive with leading foundation models, Claude 3 Haiku and GPT4o-mini on image recognition and a range of visual understanding tasks.‚Äù</p><p>Large models have incredible all-in-one versatility. Developers can choose from a variety of large commercially available proprietary GenAI models, including OpenAI‚Äôs GPT-4o multimodal model. Google‚Äôs <a href="https://deepmind.google/technologies/gemini/#introduction">Gemini 1.5</a> natively multimodal model is available in four sizes: Nano for mobile device app development, Flash small model for specific tasks, Pro for a wide range of tasks, and Ultra for highly complex tasks. And Anthropic‚Äôs <a href="https://www.anthropic.com/news/claude-3-family">Claude 3 Opus</a>, rumored to have <a href="https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3">approximately 2 trillion parameters</a>, has a 200K token context window, allowing users to upload large amounts of information. There‚Äôs also another category of out-of-the-box large GenAI models that businesses can use for employee productivity and creative development. <a href="https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/">Microsoft 365 Copilot</a> integrates the Microsoft 365 Apps suite, Microsoft Graph (content and context from emails, files, meetings, chats, calendars, and contacts), and\xa0GPT-4.</p><p>Most large and small open source models are often more transparent about application frameworks, tool ecosystem, training data, and evaluation platforms. Model architecture, hyperparameters, response quality, input modalities, context window size, and inference cost are partially or fully disclosed. These models often provide information on the dataset so that developers can determine if it meets copyright or quality expectations. This transparency allows developers to easily interchange models for future versions. Among the growing number of small commercially available open source models, Meta‚Äôs <a href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3 and 3.1</a> are based on transformer architecture and available in 8B, 70B, and 405B parameters. Llama 3.2 multimodal model has 11B and 90B, with smaller versions at 1B and 3B parameters. Built in collaboration with NVIDIA, Mistral AI‚Äôs <a href="https://mistral.ai/news/mistral-nemo/">Mistral NeMo</a> is a 12B model that features a large 128k context window while Microsoft‚Äôs <a href="https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/">Phi-3</a> (3.8B, 7B, and 14B) offers Transformer models for reasoning and language understanding tasks. Microsoft highlights Phi models as an example of ‚Äú<a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">the surprising power of small language models</a>‚Äù while investing heavily in OpenAI‚Äôs very large models. Microsoft‚Äôs diverse interest in GenAI indicates that it‚Äôs not a one-size-fits-all market.</p><h4><strong>Model-Incorporated Data (with RAG) vs. Retrieval-Centric Generation (RCG)</strong></h4><p>The next key question that AI developers need to address is where to find the data used during inference\u200a‚Äî\u200awithin the model parametric memory or outside the model (accessible by retrieval). It might be hard to believe, but the first ChatGPT launched in November 2022 did not have any access to data outside the model. It was trained on September 21, 2022 and notoriously had no inclination of events and data past its training date. This major oversight was addressed in 2023 when retrieval plug-ins where added. Today, most models are coupled with a retrieval front-end with exceptions in cases where there is no expectation of accessing large or continuously updating information, such as dedicated programming models.</p><p>Current models have made significant progress on this issue by enhancing the solution platforms with a retrieval-augmented generation (RAG) front-end to allow for extracting information external to the model. An efficient and secure RAG is a requirement in enterprise GenAI deployment, as shown by Microsoft‚Äôs introduction of <a href="https://github.com/Azure/GPT-RAG/">GPT-RAG</a> in late 2023. Furthermore, in the blog <a href="https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8">Knowledge Retrieval Takes Center Stage</a>, I cover how in the transition from consumer to business deployment for GenAI, solutions should be built primarily around information external to the model using retrieval-centric generation (RCG).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GmCPpetb_IvrsLOYayDMig.png" /><figcaption>Figure 3. Advantage of RAG vs. RCG. Image credit: Intel\xa0Labs.</figcaption></figure><p>RCG models can be defined as a special case of RAG GenAI solutions designed for systems where the vast majority of data resides outside the model parametric memory and is mostly not seen in pre-training or fine-tuning. With RCG, the primary role of the GenAI model is to interpret rich retrieved information from a company‚Äôs indexed data corpus or other curated content. Rather than memorizing data, the model focuses on fine-tuning for targeted constructs, relationships, and functionality. The quality of data in generated output is expected to approach 100% accuracy and timeliness.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*PCzPlqrtEwM6zHO5_j14QQ.png" /><figcaption><em>Figure 4. How retrieval works in GenAI platforms. Image credit: Intel\xa0Labs.</em></figcaption></figure><p><a href="https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html">OPEA</a> is a cross-ecosystem effort to ease the adoption and tuning of GenAI systems. Using this composable framework, developers can create and evaluate ‚Äúopen, multi-provider, robust, and composable GenAI solutions that harness the best innovation across the ecosystem.‚Äù OPEA is expected to simplify the implementation of enterprise-grade composite GenAI solutions, including RAG, agents, and memory\xa0systems.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*JX0ez08uxTQ-urFbU2yxGw.png" /><figcaption><em>Figure 5. OPEA core principles for GenAI implementation.</em> <em>Image credit:\xa0OPEA.</em></figcaption></figure><h4><strong>All-in-One General Purpose vs. Targeted Customized Models</strong></h4><p>Models like GPT-4o, Claude 3, and Gemini 1.5 are general purpose all-in-one foundation models. They are designed to perform a broad range of GenAI from coding to chat to summarization. The latest models have rapidly expanded to perform vision/image tasks, changing their function from just large language models to large multimodal models or vision language models (VLMs). Open source foundation models are headed in the same direction as integrated multimodalities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*grtawDbVYcTt5YLGX-6vgA.png" /><figcaption><em>Figure 6. Advantages of general purpose vs. targeted customized models. Image credit: Intel\xa0Labs.</em></figcaption></figure><p>However, rather than adopting the first wave of consumer-oriented GenAI models in this general-purpose form, most businesses are electing to use some form of specialization. When a healthcare company deploys GenAI technology, they would not use one general model for managing the supply chain, coding in the IT department, and deep medical analytics for managing patient care. Businesses deploy more specialized versions of the technology for each use case. There are several different ways that companies can build specialized GenAI solutions, including domain-specific models, targeted models, customized models, and optimized models.</p><p><em>Domain-specific models</em> are specialized for a particular field of business or an area of interest. There are both proprietary and open source domain-specific models. For example, BloombergGPT, a 50B parameter proprietary large language model specialized for finance, <a href="https://arxiv.org/pdf/2303.17564.pdf">beats the larger GPT-3 175B parameter model</a> on various financial benchmarks. However, small open source domain-specific models can provide an excellent alternative, as demonstrated by <a href="https://arxiv.org/pdf/2306.06031.pdf">FinGPT</a>, which provides accessible and transparent resources to develop FinLLMs. FinGPT 3.3 uses Llama 2 13B as a base model targeted for the financial sector. <a href="https://github.com/AI4Finance-Foundation/FinGPT">In recent benchmarks</a>, FinGPT surpassed BloombergGPT on a variety of tasks and beat GPT-4 handily on financial benchmark tasks like FPB, FiQA-SA, and TFNS. To understand the tremendous potential of this small open source model, it should be noted that FinGPT can be fine-tuned to incorporate new data for less than $300 per fine-tuning.</p><p><em>Targeted models</em> specialize in a family of tasks or functions, such as separate targeted models for <a href="https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2">coding</a>, image generation, question answering, or sentiment analysis. A recent example of a targeted model is <a href="https://huggingface.co/blog/setfit">SetFit</a> from Intel Labs, Hugging Face, and the UKP Lab. This few-shot text classification approach for fine-tuning Sentence Transformers is faster at inference and training, achieving high accuracy with a small number of labeled training data, such as only eight labeled examples per class on the Customer Reviews (CR) sentiment dataset. This small 355M parameter model can best the GPT-3 175B parameter model on the diverse RAFT benchmark.</p><p>It‚Äôs important to note that targeted models are independent from domain-specific models. For example, a sentiment analysis solution like <a href="https://huggingface.co/blog/setfit-absa">SetFitABSA</a> has targeted functionality and can be applied to various domains like industrial, entertainment, or hospitality. However, models that are both targeted and domain specialized can be more effective.</p><p><em>Customized models</em> are further fine-tuned and refined to meet particular needs and preferences of companies, organizations, or individuals. By indexing particular content for retrieval, the resulting system becomes highly specific and effective on tasks related to this data (private or public). The open source field offers an array of options to customize the model. For example, Intel Labs used direct preference optimization (DPO) to improve on a Mistral 7B model to create the open source <a href="https://huggingface.co/Intel/neural-chat-7b-v3-1">Intel NeuralChat</a>. Developers also can fine-tune and customize models by using low-rank adaptation of large language (<a href="https://arxiv.org/abs/2106.09685">LoRA</a>) models and its more memory-efficient version,\xa0<a href="https://arxiv.org/abs/2305.14314">QLoRA</a>.</p><p><em>Optimization capabilities</em> are available for open source models. The objective of optimization is to retain the functionality and accuracy of a model while substantially reducing its execution footprint, which can significantly improve cost, latency, and optimal execution of an intended platform. Some techniques used for model optimization include distillation, pruning, compression, and quantization (to 8-bit and even 4-bit). Some methods like mixture of experts (MoE) and <a href="https://arxiv.org/pdf/2211.17192.pdf">speculative decoding</a> can be considered as forms of execution optimization. For example, <a href="https://the-decoder.com/gpt-4-has-a-trillion-parameters/">GPT-4 is reportedly comprised</a> of eight smaller MoE models with 220B parameters. The execution only activates parts of the model, allowing for much more economical inference.</p><h4><strong>Generative-as-a-Service Cloud Execution vs. Managed Execution Environment for Inference</strong></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Eu7lv14XuCM4sBQCnGqn2g.png" /><figcaption><em>Figure 7. Advantages of GaaS vs. managed execution. Image credit: Intel\xa0Labs.</em></figcaption></figure><p>Another key choice for developers to consider is the execution environment. If the company chooses a proprietary model direction, inference execution is done through API or query calls to an abstracted and obscured image of the model running in the cloud. The size of the model and other implementation details are insignificant, except when translated to availability and the cost charged by some key (per token, per query, or unlimited compute license). This approach, sometimes referred to as a <a href="https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7">generative-as-a-service (GaaS)</a> cloud offering, is the principle way for companies to consume very large proprietary models like GPT-4o, Gemini Ultra, and Claude 3. However, GaaS can also be offered for smaller models like Llama\xa03.2.</p><p>There are clear positive aspects to using GaaS for the outsourced intelligence approach. For example, the access is usually instantaneous and easy to use out-of-the-box, alleviating in-house development efforts. There is also the implied promise that when the models or their environment get upgraded, the AI solution developers have access to the latest updates without substantial effort or changes to their setup. Also, the costs are almost entirely operational expenditures (OpEx), which is preferred if the workload is initial or limited. For early-stage adoption and intermittent use, GaaS offers more\xa0support.</p><p>In contrast, when companies choose an internal intelligence approach, the model inference cycle is incorporated and managed within the compute environment and the existing business software setting. This is a viable solution for relatively small models (approximately 30B parameters or less in 2024) and potentially even medium models (50B to 70B parameters in 2024) on a client device, network, on-prem data center, or on-cloud cycles in an environment set with a service provider such as a virtual private cloud\xa0(VPC).</p><p>Models like Llama 3.1 8B or similar can run on the <a href="https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7">developer‚Äôs local machine</a> (Mac or PC). Using optimization techniques like <a href="https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk">quantization</a>, the needed user experience can be achieved while operating within the local setting. Using a tool and framework like <a href="https://ollama.ai/">Ollama</a>, developers can manage inference execution locally. Inference cycles can be run on legacy GPUs, <a href="https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html">Intel Xeon</a>, or <a href="https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html">Intel Gaudi AI accelerators</a> in the company‚Äôs data center. If inference is run on the model at a service provider, it will be billed as infrastructure-as-a-service (IaaS), using the company‚Äôs own setting and execution choices.</p><p>When inference execution is done in the company compute environment (client, edge, on-prem, or IaaS), there is a higher requirement for CapEx for ownership of the computer equipment if it goes beyond adding a workload to existing hardware. While the comparison of OpEx vs. CapEx is complex and depends on many variables, CapEx is preferable when deployment requires broad, continuous, stable usage. This is especially true as smaller models and optimization technologies allow for running advanced open source models on mainstream devices and processors and even local notebooks/desktops.</p><p>Running inference in the company compute environment allows for tighter control over aspects of security and privacy. Reducing data movement and exposure can be valuable in preserving privacy. Furthermore, a retrieval-based AI solution run in a local setting can be supported with fine controls to address potential privacy concerns by giving user-controlled access to information. Security is frequently mentioned as one of the top concerns of companies deploying GenAI and <a href="https://www.intel.com/content/dam/www/public/us/en/documents/solution-briefs/intro-to-confidential-computing-solution-brief.pdf">confidential computing</a> is a primary ask. Confidential computing protects data in use by computing in an attested hardware-based <a href="https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html">Trusted Execution Environment (TEE)</a>.</p><p>Smaller, open source models can run within a company‚Äôs most secure application setting. For example, a model running on Xeon can be fully executed within a TEE with limited overhead. As shown in Figure 8, encrypted data remains protected while not in compute. The model is checked for provenance and integrity to protect against tampering. The actual execution is protected from any breach, including by the operating system or other applications, preventing viewing or alteration by untrusted entities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iESqm-hxcVZYQJL-CRoGxg.png" /><figcaption>Figure 8. Security requirements for GenAI. Image credit: Intel\xa0Labs.</figcaption></figure><h4><strong>Summary</strong></h4><p>Generative AI is a transformative technology now under evaluation or active adoption by most companies across all industries and sectors. As AI developers consider their options for the best solution, one of the most important questions they need to address is whether to use external proprietary models or rely on the open source ecosystem. One path is to rely on a large proprietary black-box GaaS solution using RAG, such as GPT-4o or Gemini Ultra. The other path uses a more adaptive and integrative approach\u200a‚Äî\u200asmall, selected, and exchanged as needed from a large open source model pool, mainly utilizing company information, customized and optimized based on particular needs, and executed within the existing infrastructure of the company. As mentioned, there could be a combination of choices within these two base strategies.</p><p>I believe that as numerous AI solution developers face this essential dilemma, most will eventually (after a learning period) choose to embed open source GenAI models in their internal compute environment, data, and business setting. They will ride the incredible advancement of the open source and broad ecosystem virtuous cycle of AI innovation, while maintaining control over their costs and\xa0destiny.</p><p>Let‚Äôs give AI the final word in solving the AI developer‚Äôs dilemma. In a <a href="https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba">staged AI debate</a>, OpenAI‚Äôs GPT-4 argued with Microsoft‚Äôs open source Orca 2 13B on the merits of using proprietary vs. open source GenAI for future development. Using GPT-4 Turbo as the judge, open source GenAI won the debate. The <a href="https://youtu.be/JuwJLeVlB-w?t=774">winning argument</a>? Orca 2 called for a ‚Äúmore distributed, open, collaborative future of AI development that leverages worldwide talent and aims for collective advancements. This model promises to accelerate innovation and democratize access to AI, and ensure ethical and transparent practices through community governance.‚Äù</p><h4><strong>Learn More: GenAI\xa0Series</strong></h4><p><a href="https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8">Knowledge Retrieval Takes Center Stage: GenAI Architecture Shifting from RAG Toward Interpretive Retrieval-Centric Generation (RCG)\xa0Models</a></p><p><a href="https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618">Survival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective AI at\xa0Scale</a></p><p><a href="https://towardsdatascience.com/have-machines-just-made-an-evolutionary-leap-to-speak-in-human-language-319237593aa4">Have Machines Just Made an Evolutionary Leap to Speak in Human Language?</a></p><h4><strong>References</strong></h4><ol><li>Hello GPT-4o. (2024, May 13). <a href="https://openai.com/index/hello-gpt-4o/">https://openai.com/index/hello-gpt-4o/</a></li><li>Open platform for enterprise AI. (n.d.). Open Platform for Enterprise AI (OPEA). <a href="https://opea.dev/">https://opea.dev/</a></li><li>Gartner Poll Finds 55% of Organizations are in Piloting or Production. (2023, October 3). Gartner. <a href="https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai">https://www.gartner.com/en/newsroom/press-releases/2023-10-03-gartner-poll-finds-55-percent-of-organizations-are-in-piloting-or-production-mode-with-generative-ai</a></li><li>Singer, G. (2023, July 28). Survival of the fittest: Compact generative AI models are the future for Cost-Effective AI at scale. <em>Medium</em>. <a href="https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618">https://towardsdatascience.com/survival-of-the-fittest-compact-generative-ai-models-are-the-future-for-cost-effective-ai-at-scale-6bbdc138f618</a></li><li>Introducing LLaMA: A foundational, 65-billion-parameter language model. (n.d.). <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">https://ai.meta.com/blog/large-language-model-llama-meta-ai/</a></li><li>#392: OpenAI‚Äôs improved ChatGPT should delight both expert and novice developers, &amp; more\u200a‚Äî\u200aARK Invest. (n.d.). Ark Invest. <a href="https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers">https://ark-invest.com/newsletter_item/1-openais-improved-chatgpt-should-delight-both-expert-and-novice-developers</a></li><li>Bilenko, M. (2024, May 22). New models added to the Phi-3 family, available on Microsoft Azure. Microsoft Azure Blog. <a href="https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/">https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/</a></li><li>Matthew Berman. (2024, June 2). Open-Source Vision AI\u200a‚Äî\u200aSurprising Results! (Phi3 Vision vs LLaMA 3 Vision vs GPT4o) [Video]. YouTube. <a href="https://www.youtube.com/watch?v=PZaNL6igONU">https://www.youtube.com/watch?v=PZaNL6igONU</a></li><li>Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. (n.d.). <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/</a></li><li>Gemini\u200a‚Äî\u200aGoogle DeepMind. (n.d.). <a href="https://deepmind.google/technologies/gemini/#introduction">https://deepmind.google/technologies/gemini/#introduction</a></li><li>Introducing the next generation of Claude \\ Anthropic. (n.d.). <a href="https://www.anthropic.com/news/claude-3-family">https://www.anthropic.com/news/claude-3-family</a></li><li>Thompson, A. D. (2024, March 4). The Memo\u200a‚Äî\u200aSpecial edition: Claude 3 Opus. The Memo by LifeArchitect.ai. <a href="https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3">https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3</a></li><li>Spataro, J. (2023, May 16). Introducing Microsoft 365 Copilot\u200a‚Äî\u200ayour copilot for work\u200a‚Äî\u200aThe Official Microsoft Blog. The Official Microsoft Blog. <a href="https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/">https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/</a></li><li>Introducing Llama 3.1: Our most capable models to date. (n.d.). <a href="https://ai.meta.com/blog/meta-llama-3-1/">https://ai.meta.com/blog/meta-llama-3-1/</a></li><li>Mistral AI. (2024, March 4). Mistral Nemo. Mistral AI | Frontier AI in Your Hands. <a href="https://mistral.ai/news/mistral-nemo/">https://mistral.ai/news/mistral-nemo/</a></li><li>Beatty, S. (2024, April 29). Tiny but mighty: The Phi-3 small language models with big potential. Microsoft Research. <a href="https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/">https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/</a></li><li>Hughes, A. (2023, December 16). Phi-2: The surprising power of small language models. Microsoft Research. <a href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/</a></li><li>Azure. (n.d.). GitHub\u200a‚Äî\u200aAzure/GPT-RAG. GitHub. <a href="https://github.com/Azure/GPT-RAG/">https://github.com/Azure/GPT-RAG/</a></li><li>Singer, G. (2023, November 16). Knowledge Retrieval Takes Center Stage\u200a‚Äî\u200aTowards Data Science. Medium. <a href="https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8">https://towardsdatascience.com/knowledge-retrieval-takes-center-stage-183be733c6e8</a></li><li>Introducing the open platform for enterprise AI. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html">https://www.intel.com/content/www/us/en/developer/articles/news/introducing-the-open-platform-for-enterprise-ai.html</a></li><li>Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., &amp; Mann, G. (2023, March 30). BloombergGPT: A large language model for finance. arXiv.org. <a href="https://arxiv.org/abs/2303.17564">https://arxiv.org/abs/2303.17564</a></li><li>Yang, H., Liu, X., &amp; Wang, C. D. (2023, June 9). FINGPT: Open-Source Financial Large Language Models. arXiv.org. <a href="https://arxiv.org/abs/2306.06031">https://arxiv.org/abs/2306.06031</a></li><li>AI4Finance-Foundation. (n.d.). FinGPT. GitHub. <a href="https://github.com/AI4Finance-Foundation/FinGPT">https://github.com/AI4Finance-Foundation/FinGPT</a></li><li>Starcoder2. (n.d.). GitHub. <a href="https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2">https://huggingface.co/docs/transformers/v4.39.0/en/model_doc/starcoder2</a></li><li>SetFit: Efficient Few-Shot Learning Without Prompts. (n.d.). <a href="https://huggingface.co/blog/setfit">https://huggingface.co/blog/setfit</a></li><li>SetFitABSA: Few-Shot Aspect Based Sentiment Analysis Using SetFit. (n.d.). <a href="https://huggingface.co/blog/setfit-absa">https://huggingface.co/blog/setfit-absa</a></li><li>Intel/neural-chat-7b-v3‚Äì1. Hugging Face. (2023, October 12). <a href="https://huggingface.co/Intel/neural-chat-7b-v3-1">https://huggingface.co/Intel/neural-chat-7b-v3-1</a></li><li>Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., &amp; Chen, W. (2021, June 17). LORA: Low-Rank adaptation of Large Language Models. arXiv.org. <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></li><li>Dettmers, T., Pagnoni, A., Holtzman, A., &amp; Zettlemoyer, L. (2023, May 23). QLORA: Efficient Finetuning of Quantized LLMS. arXiv.org. <a href="https://arxiv.org/abs/2305.14314">https://arxiv.org/abs/2305.14314</a></li><li>Leviathan, Y., Kalman, M., &amp; Matias, Y. (2022, November 30). Fast Inference from Transformers via Speculative Decoding. arXiv.org. <a href="https://arxiv.org/abs/2211.17192">https://arxiv.org/abs/2211.17192</a></li><li>Bastian, M. (2023, July 3). GPT-4 has more than a trillion parameters\u200a‚Äî\u200aReport. THE DECODER. <a href="https://the-decoder.com/gpt-4-has-a-trillion-parameters/">https://the-decoder.com/gpt-4-has-a-trillion-parameters/</a></li><li>Andriole, S. (2023, September 12). LLAMA, ChatGPT, Bard, Co-Pilot &amp; all the rest. How large language models will become huge cloud services with massive ecosystems. Forbes. <a href="https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7">https://www.forbes.com/sites/steveandriole/2023/07/26/llama-chatgpt-bard-co-pilot--all-the-rest--how-large-language-models-will-become-huge-cloud-services-with-massive-ecosystems/?sh=78764e1175b7</a></li><li>Q8-Chat LLM: An efficient generative AI experience on Intel¬Æ CPUs. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk">https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html#gs.36q4lk</a></li><li>Ollama. (n.d.). Ollama. <a href="https://ollama.com/">https://ollama.com/</a></li><li>AI Accelerated Intel¬Æ Xeon¬Æ Scalable Processors Product Brief. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html">https://www.intel.com/content/www/us/en/products/docs/processors/xeon-accelerated/ai-accelerators-product-brief.html</a></li><li>Intel¬Æ Gaudi¬Æ AI Accelerator products. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html">https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html</a></li><li>Confidential Computing Solutions\u200a‚Äî\u200aIntel. (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/security/confidential-computing.html">https://www.intel.com/content/www/us/en/security/confidential-computing.html</a></li><li>What is a Trusted Execution Environment? (n.d.). Intel. <a href="https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html">https://www.intel.com/content/www/us/en/content-details/788130/what-is-a-trusted-execution-environment.html</a></li><li>Adeojo, J. (2023, December 3). GPT-4 Debates Open Orca-2‚Äì13B with Surprising Results! Medium. <a href="https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba">https://pub.aimind.so/gpt-4-debates-open-orca-2-13b-with-surprising-results-b4ada53845ba</a></li><li>Data Centric. (2023, November 30). Surprising Debate Showdown: GPT-4 Turbo vs. Orca-2‚Äì13B\u200a‚Äî\u200aProgrammed with AutoGen! [Video]. YouTube. <a href="https://www.youtube.com/watch?v=JuwJLeVlB-w">https://www.youtube.com/watch?v=JuwJLeVlB-w</a></li></ol><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=453ac735b760" width="1" /><hr /><p><a href="https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760">The AI Developer‚Äôs Dilemma: Proprietary AI vs. Open Source Ecosystem</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>'
2024-10-01 11:54:02,147 - INFO - clean content - clean_content='Image credit: Adobe Stock.Fundamental choices impacting integration and deployment at scale of GenAI into businessesBefore a company or a developer adopts generative artificial intelligence (GenAI), they often wonder how to get business value from the integration of AI into their business. With this in mind, a fundamental question arises: Which approach will deliver the best value on investment ‚Äî a large all-encompassing proprietary model or an open source AI model that can be molded and fine-tuned for a company‚Äôs needs? AI adoption strategies fall within a wide spectrum, from accessing a cloud service from a large proprietary frontier model like OpenAI‚Äôs GPT-4o to building an internal solution in the company‚Äôs compute environment with an open source small model using indexed company data for a targeted set of tasks. Current AI solutions go well beyond the model itself, with a whole ecosystem of retrieval systems, agents, and other functional components such as AI accelerators, which are beneficial for both large and small models. Emergence of cross-industry collaborations like the Open Platform for Enterprise AI (OPEA) further the promise of streamlining the access and structuring of end-to-end open source solutions.This basic choice between the open source ecosystem and a proprietary setting impacts countless business and technical decisions, making it ‚Äúthe AI developer‚Äôs dilemma.‚Äù I believe that for most enterprise and other business deployments, it makes sense to initially use proprietary models to learn about AI‚Äôs potential and minimize early capital expenditure (CapEx). However, for broad sustained deployment, in many cases companies would use ecosystem-based open source targeted solutions, which allows for a cost-effective, adaptable strategy that aligns with evolving business needs and industry trends.GenAI Transition from Consumer to Business DeploymentWhen GenAI burst onto the scene in late 2022 with Open AI‚Äôs GPT-3 and ChatGPT 3.5, it mainly garnered consumer interest. As businesses began investigating GenAI, two approaches to deploying GenAI quickly emerged in 2023 ‚Äî using giant frontier models like ChatGPT vs. the newly introduced small, open source models originally inspired by Meta‚Äôs LLaMa model. By early 2024, two basic approaches have solidified, as shown in the columns in Figure 1. With the proprietary AI approach, the company relies on a large closed model to provide all the needed technology value. For example, taking GPT-4o as a proxy for the left column, AI developers would use OpenAI technology for the model, data, security, and compute. With the open source ecosystem AI approach, the company or developer may opt for the right-sized open source model, using corporate or private data, customized functionality, and the necessary compute and security.Both directions are valid and have advantages and disadvantages. It is not an absolute partition and developers can choose components from either approach, but taking either a proprietary or ecosystem-based open source AI path provides the company with a strategy with high internal consistency. While it is expected that both approaches will be broadly deployed, I believe that after an initial learning and transition period, most companies will follow the open source approach. Depending on the usage and setting, open source internal AI may provide significant benefits, including the ability to fine-tune the model and drive deployment using the company‚Äôs current infrastructure to run the model at the edge, on the client, in the data center, or as a dedicated service. With new AI fine-tuning tools, deep expertise is less of a barrier.Figure 1. Base approaches to the AI developer‚Äôs dilemma. Image credit: Intel Labs.Across all industries, AI developers are using GenAI for a variety of applications. An October 2023 poll by Gartner found that 55% of organizations reported increasing investment in GenAI since early 2023, and many companies are in pilot or production mode for the growing technology. As of the time of the survey, companies were mainly investing in using GenAI for software development, followed closely by marketing and customer service functions. Clearly, the range of AI applications is growing rapidly.Large Proprietary Models vs. Small and Large Open Source ModelsFigure 2: Advantages of large proprietary models, and small and large open source models. For business considerations, see Figure 7 for CapEx and OpEx aspects. Image credit: Intel Labs.In my blog Survival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective AI at Scale, I provide a detailed evaluation of large models vs. small models. In essence, following the introduction of Meta‚Äôs LLaMa open source model in February 2023, there has been a virtuous cycle of innovation and rapid improvement where the academia and broad-base ecosystem are creating highly effective models that are 10x to 100x smaller than the large frontier models. A crop of small models, which in 2024 were mostly less than 30 billion parameters, could closely match the capabilities of ChatGPT-style large models containing well over 100B parameters, especially when targeted for particular domains. While GenAI is already being deployed throughout industries for a wide range of business usages, the use of compact models is rising.In addition, open source models are mostly lagging only six to 12 months behind the performance of proprietary models. Using the broad language benchmark MMLU, the improvement pace of the open source models is faster and the gap seems to be closing with proprietary models. For example, OpenAI‚Äôs GPT-4o came out this year on May 13 with major multimodal features while Microsoft‚Äôs small open source Phi-3-vision was introduced just a week later on May 21. In rudimentary comparisons done on visual recognition and understanding, the models showed some similar competencies, with several tests even favoring the Phi-3-vision model. Initial evaluations of Meta‚Äôs Llama 3.2 open source release suggest that its ‚Äúvision models are competitive with leading foundation models, Claude 3 Haiku and GPT4o-mini on image recognition and a range of visual understanding tasks.‚ÄùLarge models have incredible all-in-one versatility. Developers can choose from a variety of large commercially available proprietary GenAI models, including OpenAI‚Äôs GPT-4o multimodal model. Google‚Äôs Gemini 1.5 natively multimodal model is available in four sizes: Nano for mobile device app development, Flash small model for specific tasks, Pro for a wide range of tasks, and Ultra for highly complex tasks. And Anthropic‚Äôs Claude 3 Opus, rumored to have approximately 2 trillion parameters, has a 200K token context window, allowing users to upload large amounts of information. There‚Äôs also another category of out-of-the-box large GenAI models that businesses can use for employee productivity and creative development. Microsoft 365 Copilot integrates the Microsoft 365 Apps suite, Microsoft Graph (content and context from emails, files, meetings, chats, calendars, and contacts), and GPT-4.Most large and small open source models are often more transparent about application frameworks, tool ecosystem, training data, and evaluation platforms. Model architecture, hyperparameters, response quality, input modalities, context window size, and inference cost are partially or fully disclosed. These models often provide information on the dataset so that developers can determine if it meets copyright or quality expectations. This transparency allows developers to easily interchange models for future versions. Among the growing number of small commercially available open source models, Meta‚Äôs Llama 3 and 3.1 are based on transformer architecture and available in 8B, 70B, and 405B parameters. Llama 3.2 multimodal model has 11B and 90B, with smaller versions at 1B and 3B parameters. Built in collaboration with NVIDIA, Mistral AI‚Äôs Mistral NeMo is a 12B model that features a large 128k context window while Microsoft‚Äôs Phi-3 (3.8B, 7B, and 14B) offers Transformer models for reasoning and language understanding tasks. Microsoft highlights Phi models as an example of ‚Äúthe surprising power of small language models‚Äù while investing heavily in OpenAI‚Äôs very large models. Microsoft‚Äôs diverse interest in GenAI indicates that it‚Äôs not a one-size-fits-all market.Model-Incorporated Data (with RAG) vs. Retrieval-Centric Generation (RCG)The next key question that AI developers need to address is where to find the data used during inference ‚Äî within the model parametric memory or outside the model (accessible by retrieval). It might be hard to believe, but the first ChatGPT launched in November 2022 did not have any access to data outside the model. It was trained on September 21, 2022 and notoriously had no inclination of events and data past its training date. This major oversight was addressed in 2023 when retrieval plug-ins where added. Today, most models are coupled with a retrieval front-end with exceptions in cases where there is no expectation of accessing large or continuously updating information, such as dedicated programming models.Current models have made significant progress on this issue by enhancing the solution platforms with a retrieval-augmented generation (RAG) front-end to allow for extracting information external to the model. An efficient and secure RAG is a requirement in enterprise GenAI deployment, as shown by Microsoft‚Äôs introduction of GPT-RAG in late 2023. Furthermore, in the blog Knowledge Retrieval Takes Center Stage, I cover how in the transition from consumer to business deployment for GenAI, solutions should be built primarily around information external to the model using retrieval-centric generation (RCG).Figure 3. Advantage of RAG vs. RCG. Image credit: Intel Labs.RCG models can be defined as a special case of RAG GenAI solutions designed for systems where the vast majority of data resides outside the model parametric memory and is mostly not seen in pre-training or fine-tuning. With RCG, the primary role of the GenAI model is to interpret rich retrieved information from a company‚Äôs indexed data corpus or other curated content. Rather than memorizing data, the model focuses on fine-tuning for targeted constructs, relationships, and functionality. The quality of data in generated output is expected to approach 100% accuracy and timeliness.Figure 4. How retrieval works in GenAI platforms. Image credit: Intel Labs.OPEA is a cross-ecosystem effort to ease the adoption and tuning of GenAI systems. Using this composable framework, developers can create and evaluate ‚Äúopen, multi-provider, robust, and composable GenAI solutions that harness the best innovation across the ecosystem.‚Äù OPEA is expected to simplify the implementation of enterprise-grade composite GenAI solutions, including RAG, agents, and memory systems.Figure 5. OPEA core principles for GenAI implementation. Image credit: OPEA.All-in-One General Purpose vs. Targeted Customized ModelsModels like GPT-4o, Claude 3, and Gemini 1.5 are general purpose all-in-one foundation models. They are designed to perform a broad range of GenAI from coding to chat to summarization. The latest models have rapidly expanded to perform vision/image tasks, changing their function from just large language models to large multimodal models or vision language models (VLMs). Open source foundation models are headed in the same direction as integrated multimodalities.Figure 6. Advantages of general purpose vs. targeted customized models. Image credit: Intel Labs.However, rather than adopting the first wave of consumer-oriented GenAI models in this general-purpose form, most businesses are electing to use some form of specialization. When a healthcare company deploys GenAI technology, they would not use one general model for managing the supply chain, coding in the IT department, and deep medical analytics for managing patient care. Businesses deploy more specialized versions of the technology for each use case. There are several different ways that companies can build specialized GenAI solutions, including domain-specific models, targeted models, customized models, and optimized models.Domain-specific models are specialized for a particular field of business or an area of interest. There are both proprietary and open source domain-specific models. For example, BloombergGPT, a 50B parameter proprietary large language model specialized for finance, beats the larger GPT-3 175B parameter model on various financial benchmarks. However, small open source domain-specific models can provide an excellent alternative, as demonstrated by FinGPT, which provides accessible and transparent resources to develop FinLLMs. FinGPT 3.3 uses Llama 2 13B as a base model targeted for the financial sector. In recent benchmarks, FinGPT surpassed BloombergGPT on a variety of tasks and beat GPT-4 handily on financial benchmark tasks like FPB, FiQA-SA, and TFNS. To understand the tremendous potential of this small open source model, it should be noted that FinGPT can be fine-tuned to incorporate new data for less than $300 per fine-tuning.Targeted models specialize in a family of tasks or functions, such as separate targeted models for coding, image generation, question answering, or sentiment analysis. A recent example of a targeted model is SetFit from Intel Labs, Hugging Face, and the UKP Lab. This few-shot text classification approach for fine-tuning Sentence Transformers is faster at inference and training, achieving high accuracy with a small number of labeled training data, such as only eight labeled examples per class on the Customer Reviews (CR) sentiment dataset. This small 355M parameter model can best the GPT-3 175B parameter model on the diverse RAFT benchmark.It‚Äôs important to note that targeted models are independent from domain-specific models. For example, a sentiment analysis solution like SetFitABSA has targeted functionality and can be applied to various domains like industrial, entertainment, or hospitality. However, models that are both targeted and domain specialized can be more effective.Customized models are further fine-tuned and refined to meet particular needs and preferences of companies, organizations, or individuals. By indexing particular content for retrieval, the resulting system becomes highly specific and effective on tasks related to this data (private or public). The open source field offers an array of options to customize the model. For example, Intel Labs used direct preference optimization (DPO) to improve on a Mistral 7B model to create the open source Intel NeuralChat. Developers also can fine-tune and customize models by using low-rank adaptation of large language (LoRA) models and its more memory-efficient version, QLoRA.Optimization capabilities are available for open source models. The objective of optimization is to retain the functionality and accuracy of a model while substantially reducing its execution footprint, which can significantly improve cost, latency, and optimal execution of an intended platform. Some techniques used for model optimization include distillation, pruning, compression, and quantization (to 8-bit and even 4-bit). Some methods like mixture of experts (MoE) and speculative decoding can be considered as forms of execution optimization. For example, GPT-4 is reportedly comprised of eight smaller MoE models with 220B parameters. The execution only activates parts of the model, allowing for much more economical inference.Generative-as-a-Service Cloud Execution vs. Managed Execution Environment for InferenceFigure 7. Advantages of GaaS vs. managed execution. Image credit: Intel Labs.Another key choice for developers to consider is the execution environment. If the company chooses a proprietary model direction, inference execution is done through API or query calls to an abstracted and obscured image of the model running in the cloud. The size of the model and other implementation details are insignificant, except when translated to availability and the cost charged by some key (per token, per query, or unlimited compute license). This approach, sometimes referred to as a generative-as-a-service (GaaS) cloud offering, is the principle way for companies to consume very large proprietary models like GPT-4o, Gemini Ultra, and Claude 3. However, GaaS can also be offered for smaller models like Llama 3.2.There are clear positive aspects to using GaaS for the outsourced intelligence approach. For example, the access is usually instantaneous and easy to use out-of-the-box, alleviating in-house development efforts. There is also the implied promise that when the models or their environment get upgraded, the AI solution developers have access to the latest updates without substantial effort or changes to their setup. Also, the costs are almost entirely operational expenditures (OpEx), which is preferred if the workload is initial or limited. For early-stage adoption and intermittent use, GaaS offers more support.In contrast, when companies choose an internal intelligence approach, the model inference cycle is incorporated and managed within the compute environment and the existing business software setting. This is a viable solution for relatively small models (approximately 30B parameters or less in 2024) and potentially even medium models (50B to 70B parameters in 2024) on a client device, network, on-prem data center, or on-cloud cycles in an environment set with a service provider such as a virtual private cloud (VPC).Models like Llama 3.1 8B or similar can run on the developer‚Äôs local machine (Mac or PC). Using optimization techniques like quantization, the needed user experience can be achieved while operating within the local setting. Using a tool and framework like Ollama, developers can manage inference execution locally. Inference cycles can be run on legacy GPUs, Intel Xeon, or Intel Gaudi AI accelerators in the company‚Äôs data center. If inference is run on the model at a service provider, it will be billed as infrastructure-as-a-service (IaaS), using the company‚Äôs own setting and execution choices.When inference execution is done in the company compute environment (client, edge, on-prem, or IaaS), there is a higher requirement for CapEx for ownership of the computer equipment if it goes beyond adding a workload to existing hardware. While the comparison of OpEx vs. CapEx is complex and depends on many variables, CapEx is preferable when deployment requires broad, continuous, stable usage. This is especially true as smaller models and optimization technologies allow for running advanced open source models on mainstream devices and processors and even local notebooks/desktops.Running inference in the company compute environment allows for tighter control over aspects of security and privacy. Reducing data movement and exposure can be valuable in preserving privacy. Furthermore, a retrieval-based AI solution run in a local setting can be supported with fine controls to address potential privacy concerns by giving user-controlled access to information. Security is frequently mentioned as one of the top concerns of companies deploying GenAI and confidential computing is a primary ask. Confidential computing protects data in use by computing in an attested hardware-based Trusted Execution Environment (TEE).Smaller, open source models can run within a company‚Äôs most secure application setting. For example, a model running on Xeon can be fully executed within a TEE with limited overhead. As shown in Figure 8, encrypted data remains protected while not in compute. The model is checked for provenance and integrity to protect against tampering. The actual execution is protected from any breach, including by the operating system or other applications, preventing viewing or alteration by untrusted entities.Figure 8. Security requirements for GenAI. Image credit: Intel Labs.SummaryGenerative AI is a transformative technology now under evaluation or active adoption by most companies across all industries and sectors. As AI developers consider their options for the best solution, one of the most important questions they need to address is whether to use external proprietary models or rely on the open source ecosystem. One path is to rely on a large proprietary black-box GaaS solution using RAG, such as GPT-4o or Gemini Ultra. The other path uses a more adaptive and integrative approach ‚Äî small, selected, and exchanged as needed from a large open source model pool, mainly utilizing company information, customized and optimized based on particular needs, and executed within the existing infrastructure of the company. As mentioned, there could be a combination of choices within these two base strategies.I believe that as numerous AI solution developers face this essential dilemma, most will eventually (after a learning period) choose to embed open source GenAI models in their internal compute environment, data, and business setting. They will ride the incredible advancement of the open source and broad ecosystem virtuous cycle of AI innovation, while maintaining control over their costs and destiny.Let‚Äôs give AI the final word in solving the AI developer‚Äôs dilemma. In a staged AI debate, OpenAI‚Äôs GPT-4 argued with Microsoft‚Äôs open source Orca 2 13B on the merits of using proprietary vs. open source GenAI for future development. Using GPT-4 Turbo as the judge, open source GenAI won the debate. The winning argument? Orca 2 called for a ‚Äúmore distributed, open, collaborative future of AI development that leverages worldwide talent and aims for collective advancements. This model promises to accelerate innovation and democratize access to AI, and ensure ethical and transparent practices through community governance.‚ÄùLearn More: GenAI SeriesKnowledge Retrieval Takes Center Stage: GenAI Architecture Shifting from RAG Toward Interpretive Retrieval-Centric Generation (RCG) ModelsSurvival of the Fittest: Compact Generative AI Models Are the Future for Cost-Effective AI at ScaleHave Machines Just Made an Evolutionary Leap to Speak in Human Language?ReferencesHello GPT-4o. (2024, May 13). platform for enterprise AI. (n.d.). Open Platform for Enterprise AI (OPEA). Poll Finds 55% of Organizations are in Piloting or Production. (2023, October 3). Gartner. G. (2023, July 28). Survival of the fittest: Compact generative AI models are the future for Cost-Effective AI at scale. Medium. LLaMA: A foundational, 65-billion-parameter language model. (n.d.). OpenAI‚Äôs improved ChatGPT should delight both expert and novice developers, &amp; more ‚Äî ARK Invest. (n.d.). Ark Invest. M. (2024, May 22). New models added to the Phi-3 family, available on Microsoft Azure. Microsoft Azure Blog. Berman. (2024, June 2). Open-Source Vision AI ‚Äî Surprising Results! (Phi3 Vision vs LLaMA 3 Vision vs GPT4o) [Video]. YouTube. 3.2: Revolutionizing edge AI and vision with open, customizable models. (n.d.). ‚Äî Google DeepMind. (n.d.). the next generation of Claude \\ Anthropic. (n.d.). A. D. (2024, March 4). The Memo ‚Äî Special edition: Claude 3 Opus. The Memo by LifeArchitect.ai. J. (2023, May 16). Introducing Microsoft 365 Copilot ‚Äî your copilot for work ‚Äî The Official Microsoft Blog. The Official Microsoft Blog. Llama 3.1: Our most capable models to date. (n.d.). AI. (2024, March 4). Mistral Nemo. Mistral AI | Frontier AI in Your Hands. S. (2024, April 29). Tiny but mighty: The Phi-3 small language models with big potential. Microsoft Research. A. (2023, December 16). Phi-2: The surprising power of small language models. Microsoft Research. (n.d.). GitHub ‚Äî Azure/GPT-RAG. GitHub. G. (2023, November 16). Knowledge Retrieval Takes Center Stage ‚Äî Towards Data Science. Medium. the open platform for enterprise AI. (n.d.). Intel. S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., &amp; Mann, G. (2023, March 30). BloombergGPT: A large language model for finance. arXiv.org. H., Liu, X., &amp; Wang, C. D. (2023, June 9). FINGPT: Open-Source Financial Large Language Models. arXiv.org. (n.d.). FinGPT. GitHub. (n.d.). GitHub. Efficient Few-Shot Learning Without Prompts. (n.d.). Few-Shot Aspect Based Sentiment Analysis Using SetFit. (n.d.). Hugging Face. (2023, October 12). E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., &amp; Chen, W. (2021, June 17). LORA: Low-Rank adaptation of Large Language Models. arXiv.org. T., Pagnoni, A., Holtzman, A., &amp; Zettlemoyer, L. (2023, May 23). QLORA: Efficient Finetuning of Quantized LLMS. arXiv.org. Y., Kalman, M., &amp; Matias, Y. (2022, November 30). Fast Inference from Transformers via Speculative Decoding. arXiv.org. M. (2023, July 3). GPT-4 has more than a trillion parameters ‚Äî Report. THE DECODER. S. (2023, September 12). LLAMA, ChatGPT, Bard, Co-Pilot &amp; all the rest. How large language models will become huge cloud services with massive ecosystems. Forbes. LLM: An efficient generative AI experience on Intel¬Æ CPUs. (n.d.). Intel. (n.d.). Ollama. Accelerated Intel¬Æ Xeon¬Æ Scalable Processors Product Brief. (n.d.). Intel. Gaudi¬Æ AI Accelerator products. (n.d.). Intel. Computing Solutions ‚Äî Intel. (n.d.). Intel. is a Trusted Execution Environment? (n.d.). Intel. J. (2023, December 3). GPT-4 Debates Open Orca-2‚Äì13B with Surprising Results! Medium. Centric. (2023, November 30). Surprising Debate Showdown: GPT-4 Turbo vs. Orca-2‚Äì13B ‚Äî Programmed with AutoGen! [Video]. YouTube. AI Developer‚Äôs Dilemma: Proprietary AI vs. Open Source Ecosystem was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.'
2024-10-01 11:54:02,148 - INFO - Generating summary with BART model
2024-10-01 11:54:07,634 - INFO - Generating summary with BART model
2024-10-01 11:54:13,055 - INFO - full_tweet='Companies often wonder how to get business value from the integration of AI into their business. Current..[read moreüëáüèº] #AI #business https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760?source=rss----7f60cf5620c9---4'
2024-10-01 11:54:13,343 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841084198217461905'], 'text': 'Companies often wonder how to get business value from the integration of AI into their business. Current..[read moreüëáüèº] #AI #business https://t.co/gbC6Q3wm9s', 'id': '1841084198217461905'}, includes={}, errors=[], meta={})
2024-10-01 11:54:13,343 - INFO - Sleeping for 1 minutes and 53 seconds.
2024-10-01 11:56:06,344 - INFO - Saved posted URL: https://towardsdatascience.com/the-ai-developers-dilemma-proprietary-ai-vs-open-source-ecosystem-453ac735b760?source=rss----7f60cf5620c9---4 at 2024-10-01 11:56:06
2024-10-01 11:56:06,344 - INFO - Input content - content='<!-- SC_OFF --><div class="md"><p>Hey fellow AI enthusiasts and developers! I\'ve been working on a project to analyze and visualize the most common technical challenges in AI development by looking at Reddit posts on dedicated subs.</p> <h1>Project Goal</h1> <p>The main objective of this project is to identify and track the most prevalent and trending technical challenges, implementation problems, and conceptual hurdles related to AI development. By doing this, we can:</p> <ol> <li>Help developers focus on the most relevant skills and knowledge areas</li> <li>Guide educational content creators in addressing the most pressing issues</li> <li>Provide insights for researchers on areas that need more attention or solutions</li> </ol> <h1>How It Works</h1> <ol> <li><strong>Data Collection</strong>: I fetched the hottest 200 posts from each of the followingAI-related subreddits: r/learnmachinelearning, r/ArtificialIntelligence, r/MachineLearning, <a href="https://www.reddit.com/r/artificial">r/artificial</a>.</li> <li><strong>Screening</strong>: Posts are screened using an LLM to ensure they\'re about specific technical challenges rather than general discussions or news.</li> <li><strong>Summarization and Tagging</strong>: Each relevant post is summarized and tagged with up to three categories from a predefined list of 50 technical areas (e.g., LLM-ARCH for Large Language Model Architecture, CV-OBJ for Computer Vision Object Detection).</li> <li><strong>Analysis</strong>: The system analyzes the frequency of tags, along with the associated upvotes and comments for each category.</li> <li><strong>Visualization</strong>: The results are visualized through various charts and a heatmap, showing the most common challenges and their relative importance in the community.</li> </ol> <h1>Results (here are the <a href="https://imgur.com/a/CHsQMgA">figures</a>):</h1> <ol> <li>Top 15 Tags by Combined Score (frequency + upvotes + comments)</li> <li>Normalized Tag Popularity Heatmap</li> <li>Tag analysis table with individual scores</li> </ol> <h1>Feedback</h1> <p>I\'d love to get your thoughts on this project and how I can make it more useful for the AI development community. Specifically:</p> <ol> <li>Are there any other data sources we should consider beyond Reddit?</li> <li>What additional metrics or analyses would you find valuable?</li> <li>How can I make the results more actionable for developers, educators, or researchers?</li> <li>Are there any potential biases or limitations in this approach that we should address?</li> <li>Would you be interested in a regularly updated dashboard of these trends?</li> </ol> <p>Your insights and suggestions are greatly appreciated!</p> <p><strong>TL;DR: AI Development Challenges Analyzer</strong></p> <ul> <li>Project analyzes Reddit posts to identify common AI development challenges</li> <li>Uses ML to screen, summarize, and tag posts from AI-related subreddits</li> <li>Visualizes results to show most discussed and engaging technical areas</li> <li><a href="https://imgur.com/a/CHsQMgA">View results here</a></li> <li>Seeking feedback to improve the analysis</li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Fixmyn26issue"> /u/Fixmyn26issue </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fstn9m/p_i_tried_to_map_the_most_recurrent_and_popular/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fstn9m/p_i_tried_to_map_the_most_recurrent_and_popular/">[comments]</a></span>'
2024-10-01 11:56:06,344 - INFO - clean content - clean_content="Hey fellow AI enthusiasts and developers! I've been working on a project to analyze and visualize the most common technical challenges in AI development by looking at Reddit posts on dedicated subs. Project Goal The main objective of this project is to identify and track the most prevalent and trending technical challenges, implementation problems, and conceptual hurdles related to AI development. By doing this, we can: Help developers focus on the most relevant skills and knowledge areas Guide educational content creators in addressing the most pressing issues Provide insights for researchers on areas that need more attention or solutions How It Works Data Collection: I fetched the hottest 200 posts from each of the followingAI-related subreddits: r/learnmachinelearning, r/ArtificialIntelligence, r/MachineLearning, r/artificial. Screening: Posts are screened using an LLM to ensure they're about specific technical challenges rather than general discussions or news. Summarization and Tagging: Each relevant post is summarized and tagged with up to three categories from a predefined list of 50 technical areas (e.g., LLM-ARCH for Large Language Model Architecture, CV-OBJ for Computer Vision Object Detection). Analysis: The system analyzes the frequency of tags, along with the associated upvotes and comments for each category. Visualization: The results are visualized through various charts and a heatmap, showing the most common challenges and their relative importance in the community. Results (here are the figures): Top 15 Tags by Combined Score (frequency + upvotes + comments) Normalized Tag Popularity Heatmap Tag analysis table with individual scores Feedback I'd love to get your thoughts on this project and how I can make it more useful for the AI development community. Specifically: Are there any other data sources we should consider beyond Reddit? What additional metrics or analyses would you find valuable? How can I make the results more actionable for developers, educators, or researchers? Are there any potential biases or limitations in this approach that we should address? Would you be interested in a regularly updated dashboard of these trends? Your insights and suggestions are greatly appreciated! TL;DR: AI Development Challenges Analyzer Project analyzes Reddit posts to identify common AI development challenges Uses ML to screen, summarize, and tag posts from AI-related subreddits Visualizes results to show most discussed and engaging technical areas View results here Seeking feedback to improve the analysis &#32; submitted by &#32; /u/Fixmyn26issue [link] &#32; [comments]"
2024-10-01 11:56:06,344 - INFO - Generating summary with BART model
2024-10-01 11:56:12,450 - INFO - Generating summary with BART model
2024-10-01 11:56:18,502 - INFO - full_tweet="I've been working on a project to analyze and visualize the most common technical challenges in AI..[read moreüëáüèº] #AI #project https://www.reddit.com/r/MachineLearning/comments/1fstn9m/p_i_tried_to_map_the_most_recurrent_and_popular/"
2024-10-01 11:56:18,720 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1841084724023849141'], 'id': '1841084724023849141', 'text': "I've been working on a project to analyze and visualize the most common technical challenges in AI..[read moreüëáüèº] #AI #project https://t.co/zjGDxxFpEc"}, includes={}, errors=[], meta={})
2024-10-01 11:56:18,720 - INFO - Sleeping for 1 minutes and 26 seconds.
2024-10-01 11:57:44,720 - INFO - Saved posted URL: https://www.reddit.com/r/MachineLearning/comments/1fstn9m/p_i_tried_to_map_the_most_recurrent_and_popular/ at 2024-10-01 11:57:44

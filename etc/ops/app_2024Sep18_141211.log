2024-09-18 14:12:11,782 - INFO - Starting configuration setup
2024-09-18 14:12:11,782 - INFO - Environment variables loaded successfully
2024-09-18 14:12:11,782 - INFO - Twitter API client initialized successfully
2024-09-18 14:12:14,595 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-09-18 14:12:22,716 - INFO - Summarization model initialized successfully
2024-09-18 14:12:22,716 - INFO - Starting tweet scheduling
2024-09-18 14:12:22,716 - INFO - Tweeting process started at 2024-09-18 14:12:22.716957!
2024-09-18 14:12:22,717 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Sep18.csv
2024-09-18 14:12:22,717 - INFO - Fetching latest tech news from RSS feeds
2024-09-18 14:12:44,010 - INFO - Total entries found: 622
2024-09-18 14:12:44,013 - INFO - Recent AI-related entries found: 18
2024-09-18 14:12:44,013 - INFO - Input content - content='<p>Hey HN<p>I wanted to create a funny SaaS directory that\'s unlike the others and make it easy to submit your SaaS without needing to fill in a lot of info. Just enter your SaaS/product URL, and it will be sarcastically roasted and published.<p>Enjoy the roasts!</p>\n<hr />\n<p>Comments URL: <a href="https://news.ycombinator.com/item?id=41579784">https://news.ycombinator.com/item?id=41579784</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>'
2024-09-18 14:12:44,014 - INFO - clean content - clean_content="Hey HNI wanted to create a funny SaaS directory that's unlike the others and make it easy to submit your SaaS without needing to fill in a lot of info. Just enter your SaaS/product URL, and it will be sarcastically roasted and published.Enjoy the roasts! Comments URL: Points: 1 # Comments: 0"
2024-09-18 14:12:44,014 - INFO - Generating summary with BART model
2024-09-18 14:12:50,448 - INFO - Generating summary with BART model
2024-09-18 14:12:55,084 - INFO - full_tweet='Just enter your SaaS/product URL, and it will be sarcastically roasted and published. Enjoy the roasts!..[read moreüëáüèº] #Comments #URL https://roastcraze.com'
2024-09-18 14:12:55,390 - INFO - Tweet posted successfully: Response(data={'text': 'Just enter your SaaS/product URL, and it will be sarcastically roasted and published. Enjoy the roasts!..[read moreüëáüèº] #Comments #URL https://t.co/cbK8sNiFDj', 'edit_history_tweet_ids': ['1836408061168120077'], 'id': '1836408061168120077'}, includes={}, errors=[], meta={})
2024-09-18 14:12:55,390 - INFO - Sleeping for 3 minutes and 43 seconds.
2024-09-18 14:16:38,391 - INFO - Saved posted URL: https://roastcraze.com at 2024-09-18 14:16:38
2024-09-18 14:16:38,391 - INFO - Input content - content='<p>Hi HN,<p>We are Michael & Jono, and we are building Cerebrium (<a href="https://www.cerebrium.ai">https://www.cerebrium.ai</a>), a serverless infrastructure platform for ML/AI applications - we make it easy for engineers to build, deploy and scale AI applications.<p>Initially, we‚Äôve been hyper-focused on the inference side of applications, but we‚Äôre working on expanding our functionality to support training and data processing use cases‚Äîeventually covering the full AI development lifecycle.<p>You can watch a quick loom video of us deploying: <a href="https://www.loom.com/share/06947794b3bf4bb1bb21c87066dfcc66?sid=acd66bbe-2549-493c-bea4-552acfd1163d" rel="nofollow">https://www.loom.com/share/06947794b3bf4bb1bb21c87066dfcc66?...</a><p>How we got here:<p>Jono and I led the technical team at our previous e-commerce startup, which grew rapidly over a few years. As we scaled, we were tasked with building out ML applications to make the business more efficient. It was tough‚Äîevery day felt like a defeat. We found ourselves stitching together AWS Lambda, Sagemaker, and Prefect jobs (this stack alone was enough to make me want to give up). By the time we reached production, the costs were too high to maintain. Getting these applications live required a significant upfront investment of both time and money, making it inaccessible for most startups and scale-ups to attempt. We wanted to create something that would help us (and others like us) implement ML/AI applications easily and cost-effectively.<p>The problem:<p>There are a ton of challenges to tackle to realize our vision, but we‚Äôve initially focused on a few key ones:<p>1. GPUs are expensive ‚Äì An A100 is 326 times the cost of a CPU, and companies are using LLMs like they‚Äôre simple APIs. Serverless instances solve this to an extent, but minimizing cold starts is difficult.<p>2. Local development ‚Äì Engineers need local development environments to iterate quickly, but production-grade GPUs aren‚Äôt available on consumer hardware. How can we make cloud deployments feel as fast as just saving a file locally and retrying?<p>3. Cost to experiment - To run experiments we had to spin up EC2 instances each day, recreate our environment and run scripts. It was difficult to monitor logs, instance usage metrics as well as run large processing jobs or scale endpoints without a significant infrastructure investment. Additionally, we often forget to switch off instances which cost us money!<p>Our Approach<p>We have three core areas that we are focused on which we believe are the most important for any infrastructure platform:<p>1. Performance:<p>We have worked hard to get our added network latency <50ms and the cold start of our average workloads to 2-4 seconds. Here are a few things we did to get our cold starts so low:<p>- Container Runtime: We built our own container runtime that splits container images into two parts‚Äîmetadata and data blobs. Metadata provides the file structure, while the actual data blobs are fetched on-demand. This allows containers to start before the full image is downloaded. In the background, we prefetch the remaining blobs.<p>- Caching: Once an image is on a machine, it‚Äôs cached for future use. This makes subsequent container startups much faster. We also intelligently route requests to machines where the image is already cached.<p>- Efficient Inference: We route requests to the optimal machines, prioritizing low-latency and high-throughput performance. If no containers are immediately available, we efficiently queue the requests through our task scheduling system.<p>- Distributed Storage Cache: One of the most resource-intensive parts of AI workloads is loading models into VRAM. We use NVME drives (which are much faster than network volumes), as close as possible to the machines and we orchestrate workloads to nodes that already contain the necessary model weights where possible.<p>2. Developer Experience<p>We built Cerebrium to help developers iterate as quickly as possible by streamlining the entire build and deployment process.<p>To get build times as low as possible, we use high-performance machines and cache layers where possible. We\'ve reduced first-time build times to an average of 2 minutes and 24 seconds, with subsequent builds completing in just 19 seconds.<p>We also offer a wide range of GPU types‚Äîover 8 different options‚Äîso you can easily test performance and cost efficiency by adjusting a single line in your configuration file.<p>To reduce friction, we‚Äôve kept things simple. There are no custom Python decorators, no Cerebrium specific syntax to learn. You just add a .toml file to define your hardware requirements and environment settings. This makes migrating onto or off our platform just as easy as migrating off. We aim to impress you enough that you will want to stay.<p>3. Stability<p>This is arguably more important than the first two areas - no one wants to get an email at 11pm at night or on a Saturday that their application is down or degraded. Since April, we‚Äôve maintained 99.999% uptime. We have redundancies in place, monitoring, alerts, and a team that covers all time zones to resolve any issues quickly.<p>Why Is This Hard?<p>Building Cerebrium has been challenging because it involves solving multiple interconnected problems. It requires optimization at every step‚Äîfrom efficiently splitting images to fetching data on-demand without introducing latency, handling distributed caching, optimizing our network stack, and ensuring redundancies, all while holding true to the three areas mentioned above.<p>Pricing:<p>We charge you exactly for the resources you need and only charge you when your code is running ie: usage-based. For example, if you specify you need 1 A100 GPU, with 2 CPUs and 12 GB of RAM we charge you exactly for that and not a full A100 (12 CPU‚Äôs and 148GB of memory)<p>You can see more about our pricing here: <a href="http://www.cerebrium.ai/pricing">http://www.cerebrium.ai/pricing</a><p>What‚Äôs Next?<p>We\'re builders too, and we know how crucial support can be when you\'re working on something new. Here\'s what we\'ve put together to support teams like yours:<p>- $30 in free credit to start exploring. If you\'re onto something interesting but need more runway, just give us a shout - we‚Äôd be happy to extend that for compelling use-cases.<p>- We have worked hard on our docs to make onboarding easy as well as have a very elaborate Github repo covering AI voice agents, LLM optimizations and much more.<p>Docs: <a href="https://docs.cerebrium.ai/cerebrium/getting-started/introduction">https://docs.cerebrium.ai/cerebrium/getting-started/introduc...</a><p>Github Examples: <a href="https://github.com/CerebriumAI/examples/tree/master">https://github.com/CerebriumAI/examples/tree/master</a><p>If you have a question or hit a snag, you can directly reach out to the engineers who built the platform‚Äîwe‚Äôre here to help! We‚Äôve also set up Slack and Discord communities where you can connect with other creators, share experiences, ask for advice, or just chat with folks building cool things.<p>We\'re looking forward to seeing what you all build and please give us feedback on what you would like us to improve/add</p>\n<hr />\n<p>Comments URL: <a href="https://news.ycombinator.com/item?id=41579777">https://news.ycombinator.com/item?id=41579777</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>'
2024-09-18 14:16:38,392 - INFO - clean content - clean_content="Hi HN,We are Michael & Jono, and we are building Cerebrium ( a serverless infrastructure platform for ML/AI applications - we make it easy for engineers to build, deploy and scale AI applications.Initially, we‚Äôve been hyper-focused on the inference side of applications, but we‚Äôre working on expanding our functionality to support training and data processing use cases‚Äîeventually covering the full AI development lifecycle.You can watch a quick loom video of us deploying: we got here:Jono and I led the technical team at our previous e-commerce startup, which grew rapidly over a few years. As we scaled, we were tasked with building out ML applications to make the business more efficient. It was tough‚Äîevery day felt like a defeat. We found ourselves stitching together AWS Lambda, Sagemaker, and Prefect jobs (this stack alone was enough to make me want to give up). By the time we reached production, the costs were too high to maintain. Getting these applications live required a significant upfront investment of both time and money, making it inaccessible for most startups and scale-ups to attempt. We wanted to create something that would help us (and others like us) implement ML/AI applications easily and cost-effectively.The problem:There are a ton of challenges to tackle to realize our vision, but we‚Äôve initially focused on a few key ones:1. GPUs are expensive ‚Äì An A100 is 326 times the cost of a CPU, and companies are using LLMs like they‚Äôre simple APIs. Serverless instances solve this to an extent, but minimizing cold starts is difficult.2. Local development ‚Äì Engineers need local development environments to iterate quickly, but production-grade GPUs aren‚Äôt available on consumer hardware. How can we make cloud deployments feel as fast as just saving a file locally and retrying?3. Cost to experiment - To run experiments we had to spin up EC2 instances each day, recreate our environment and run scripts. It was difficult to monitor logs, instance usage metrics as well as run large processing jobs or scale endpoints without a significant infrastructure investment. Additionally, we often forget to switch off instances which cost us money!Our ApproachWe have three core areas that we are focused on which we believe are the most important for any infrastructure platform:1. Performance:We have worked hard to get our added network latency - Container Runtime: We built our own container runtime that splits container images into two parts‚Äîmetadata and data blobs. Metadata provides the file structure, while the actual data blobs are fetched on-demand. This allows containers to start before the full image is downloaded. In the background, we prefetch the remaining blobs.- Caching: Once an image is on a machine, it‚Äôs cached for future use. This makes subsequent container startups much faster. We also intelligently route requests to machines where the image is already cached.- Efficient Inference: We route requests to the optimal machines, prioritizing low-latency and high-throughput performance. If no containers are immediately available, we efficiently queue the requests through our task scheduling system.- Distributed Storage Cache: One of the most resource-intensive parts of AI workloads is loading models into VRAM. We use NVME drives (which are much faster than network volumes), as close as possible to the machines and we orchestrate workloads to nodes that already contain the necessary model weights where possible.2. Developer ExperienceWe built Cerebrium to help developers iterate as quickly as possible by streamlining the entire build and deployment process.To get build times as low as possible, we use high-performance machines and cache layers where possible. We've reduced first-time build times to an average of 2 minutes and 24 seconds, with subsequent builds completing in just 19 seconds.We also offer a wide range of GPU types‚Äîover 8 different options‚Äîso you can easily test performance and cost efficiency by adjusting a single line in your configuration file.To reduce friction, we‚Äôve kept things simple. There are no custom Python decorators, no Cerebrium specific syntax to learn. You just add a .toml file to define your hardware requirements and environment settings. This makes migrating onto or off our platform just as easy as migrating off. We aim to impress you enough that you will want to stay.3. StabilityThis is arguably more important than the first two areas - no one wants to get an email at 11pm at night or on a Saturday that their application is down or degraded. Since April, we‚Äôve maintained 99.999% uptime. We have redundancies in place, monitoring, alerts, and a team that covers all time zones to resolve any issues quickly.Why Is This Hard?Building Cerebrium has been challenging because it involves solving multiple interconnected problems. It requires optimization at every step‚Äîfrom efficiently splitting images to fetching data on-demand without introducing latency, handling distributed caching, optimizing our network stack, and ensuring redundancies, all while holding true to the three areas mentioned above.Pricing:We charge you exactly for the resources you need and only charge you when your code is running ie: usage-based. For example, if you specify you need 1 A100 GPU, with 2 CPUs and 12 GB of RAM we charge you exactly for that and not a full A100 (12 CPU‚Äôs and 148GB of memory)You can see more about our pricing here: Next?We're builders too, and we know how crucial support can be when you're working on something new. Here's what we've put together to support teams like yours:- $30 in free credit to start exploring. If you're onto something interesting but need more runway, just give us a shout - we‚Äôd be happy to extend that for compelling use-cases.- We have worked hard on our docs to make onboarding easy as well as have a very elaborate Github repo covering AI voice agents, LLM optimizations and much more.Docs: Examples: you have a question or hit a snag, you can directly reach out to the engineers who built the platform‚Äîwe‚Äôre here to help! We‚Äôve also set up Slack and Discord communities where you can connect with other creators, share experiences, ask for advice, or just chat with folks building cool things.We're looking forward to seeing what you all build and please give us feedback on what you would like us to improve/add Comments URL: Points: 1 # Comments: 0"
2024-09-18 14:16:38,393 - INFO - Generating summary with BART model
2024-09-18 14:16:42,964 - INFO - Generating summary with BART model
2024-09-18 14:16:47,565 - INFO - full_tweet='Cerebrium is a serverless infrastructure platform for ML/AI applications. It makes it easy for engineers..[read moreüëáüèº] #AI #ML https://news.ycombinator.com/item?id=41579777'
2024-09-18 14:16:47,753 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1836409035777507591'], 'id': '1836409035777507591', 'text': 'Cerebrium is a serverless infrastructure platform for ML/AI applications. It makes it easy for engineers..[read moreüëáüèº] #AI #ML https://t.co/AHXgwqXFvV'}, includes={}, errors=[], meta={})
2024-09-18 14:16:47,753 - INFO - Sleeping for 7 minutes and 55 seconds.
2024-09-18 14:24:42,753 - INFO - Saved posted URL: https://news.ycombinator.com/item?id=41579777 at 2024-09-18 14:24:42
2024-09-18 14:24:42,754 - INFO - Input content - content='<p>Article URL: <a href="https://uncertaintymindset.substack.com/p/ai-missing-middle">https://uncertaintymindset.substack.com/p/ai-missing-middle</a></p>\n<p>Comments URL: <a href="https://news.ycombinator.com/item?id=41579751">https://news.ycombinator.com/item?id=41579751</a></p>\n<p>Points: 1</p>\n<p># Comments: 0</p>'
2024-09-18 14:24:42,754 - INFO - clean content - clean_content='Article URL: Comments URL: Points: 1 # Comments: 0'
2024-09-18 14:24:42,754 - INFO - Generating summary with BART model
2024-09-18 14:24:48,031 - INFO - Generating summary with BART model
2024-09-18 14:24:53,426 - INFO - full_tweet='CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots..[read moreüëáüèº] #gallery #Travel https://uncertaintymindset.substack.com/p/ai-missing-middle'
2024-09-18 14:24:53,714 - INFO - Tweet posted successfully: Response(data={'text': 'https://t.co/O2xYLX2Kvt will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots..[read moreüëáüèº] #gallery #Travel https://t.co/tJsM2GIp9E', 'edit_history_tweet_ids': ['1836411073949233647'], 'id': '1836411073949233647'}, includes={}, errors=[], meta={})
2024-09-18 14:24:53,714 - INFO - Sleeping for 8 minutes and 47 seconds.
2024-09-18 14:33:40,715 - INFO - Saved posted URL: https://uncertaintymindset.substack.com/p/ai-missing-middle at 2024-09-18 14:33:40
2024-09-18 14:33:40,715 - INFO - Input content - content='<p>Runway, a startup developing AI video tools, including video-generating models, has partnered with Lionsgate‚Äîthe studio behind the &#8220;John Wick&#8221; and &#8220;Twilight&#8221; franchises‚Äîto train a custom video model on Lionsgate&#8217;s movie catalog. Lionsgate vice chair Michael Burns said in a statement that the studio&#8217;s &#8220;filmmakers, directors and other creative talent&#8221; will get access to the model [&#8230;]</p>\n<p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>'
2024-09-18 14:33:40,715 - INFO - clean content - clean_content='Runway, a startup developing AI video tools, including video-generating models, has partnered with Lionsgate‚Äîthe studio behind the &#8220;John Wick&#8221; and &#8220;Twilight&#8221; franchises‚Äîto train a custom video model on Lionsgate&#8217;s movie catalog. Lionsgate vice chair Michael Burns said in a statement that the studio&#8217;s &#8220;filmmakers, directors and other creative talent&#8221; will get access to the model [&#8230;] ¬© 2024 TechCrunch. All rights reserved. For personal use only.'
2024-09-18 14:33:40,715 - INFO - Generating summary with BART model
2024-09-18 14:33:45,431 - INFO - Generating summary with BART model
2024-09-18 14:33:50,130 - INFO - full_tweet='Runway, a startup developing AI video tools, has partnered with Lionsgate. Runway will train a custom..[read moreüëáüèº] #startup #AI https://techcrunch.com/2024/09/18/generative-ai-startup-runway-inks-deal-with-a-major-hollywood-studio/'
2024-09-18 14:33:50,408 - INFO - Tweet posted successfully: Response(data={'id': '1836413325007020100', 'text': 'Runway, a startup developing AI video tools, has partnered with Lionsgate. Runway will train a custom..[read moreüëáüèº] #startup #AI https://t.co/VVy8BsSd0m', 'edit_history_tweet_ids': ['1836413325007020100']}, includes={}, errors=[], meta={})
2024-09-18 14:33:50,408 - INFO - Sleeping for 6 minutes and 13 seconds.
2024-09-18 14:40:03,408 - INFO - Saved posted URL: https://techcrunch.com/2024/09/18/generative-ai-startup-runway-inks-deal-with-a-major-hollywood-studio/ at 2024-09-18 14:40:03
2024-09-18 14:40:03,409 - INFO - Input content - content='<!-- SC_OFF --><div class="md"><p>I recently began my journey as an implementation engineer and have just enrolled in a master‚Äôs at John Hopkins focusing on Artificial Intelligence. I‚Äôve been in cybersecurity operations for a few months now which has been fascinating. I‚Äôm eager to explore ways to integrate AI with my work. I‚Äôve conducted some research and identified the following drawbacks with AI in the context of security operations. I would love to hear thoughts and insights on the points below.</p> <p>False positives &amp; false negatives: incorrect flagging benign activity as a threat or failing to detect real time threats.</p> <p>Retraining: constantly retrain models to keep up with evolving threat landscape </p> <p>Lack of specialized cybersecurity knowledge: limit accuracy and misinterpret complex attack </p> <p>Skilled Attackers Manipulate AI Systems: attacker will mimic normal behavior which the AI will struggle to between normal and malicious activity </p> <p>Lack human intuition: miss subtle clues or connections human would spot</p> <p>Overfitting: use to specific patterns and fail to spot new or unseen threats </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Strict-Bat8273"> /u/Strict-Bat8273 </a> <br /> <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fjrvui/sec_ops_and_ai_discussion/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/MachineLearning/comments/1fjrvui/sec_ops_and_ai_discussion/">[comments]</a></span>'
2024-09-18 14:40:03,409 - INFO - clean content - clean_content='I recently began my journey as an implementation engineer and have just enrolled in a master‚Äôs at John Hopkins focusing on Artificial Intelligence. I‚Äôve been in cybersecurity operations for a few months now which has been fascinating. I‚Äôm eager to explore ways to integrate AI with my work. I‚Äôve conducted some research and identified the following drawbacks with AI in the context of security operations. I would love to hear thoughts and insights on the points below. False positives &amp; false negatives: incorrect flagging benign activity as a threat or failing to detect real time threats. Retraining: constantly retrain models to keep up with evolving threat landscape Lack of specialized cybersecurity knowledge: limit accuracy and misinterpret complex attack Skilled Attackers Manipulate AI Systems: attacker will mimic normal behavior which the AI will struggle to between normal and malicious activity Lack human intuition: miss subtle clues or connections human would spot Overfitting: use to specific patterns and fail to spot new or unseen threats &#32; submitted by &#32; /u/Strict-Bat8273 [link] &#32; [comments]'
2024-09-18 14:40:03,409 - INFO - Generating summary with BART model
2024-09-18 14:40:10,774 - INFO - Generating summary with BART model
2024-09-18 14:40:18,190 - INFO - full_tweet='I recently began my journey as an implementation engineer and have just enrolled in a master‚Äôs at John..[read moreüëáüèº] #AI #I https://www.reddit.com/r/MachineLearning/comments/1fjrvui/sec_ops_and_ai_discussion/'
2024-09-18 14:40:18,590 - INFO - Tweet posted successfully: Response(data={'edit_history_tweet_ids': ['1836414952891977866'], 'id': '1836414952891977866', 'text': 'I recently began my journey as an implementation engineer and have just enrolled in a master‚Äôs at John..[read moreüëáüèº] #AI #I https://t.co/Z4vXAv9iSD'}, includes={}, errors=[], meta={})
2024-09-18 14:40:18,590 - INFO - Sleeping for 9 minutes and 33 seconds.
2024-09-18 14:49:51,591 - INFO - Saved posted URL: https://www.reddit.com/r/MachineLearning/comments/1fjrvui/sec_ops_and_ai_discussion/ at 2024-09-18 14:49:51
2024-09-18 14:49:51,591 - INFO - This link is already posted: https://www.techradar.com/pro/security/google-cloud-document-ai-has-some-worrying-security-flaws
2024-09-18 14:49:51,591 - INFO - Input content - content='Apple and Android app Death Clock delivers a personalized health and lifestyle guide that predicts the day you will die.'
2024-09-18 14:49:51,591 - INFO - clean content - clean_content='Apple and Android app Death Clock delivers a personalized health and lifestyle guide that predicts the day you will die.'
2024-09-18 14:49:51,591 - INFO - Generating summary with BART model
2024-09-18 14:49:55,509 - INFO - Generating summary with BART model
2024-09-18 14:49:59,365 - INFO - full_tweet='Death Clock delivers a personalized health and lifestyle guide that predicts the day you will die. Apple..[read moreüëáüèº] #app #App https://www.cnet.com/tech/services-and-software/death-clock-app-uses-ai-to-predict-your-death-date/#ftag=CAD590a51e'
2024-09-18 14:49:59,689 - INFO - Tweet posted successfully: Response(data={'text': 'Death Clock delivers a personalized health and lifestyle guide that predicts the day you will die. Apple..[read moreüëáüèº] #app #App https://t.co/DbbMsTdXRK', 'edit_history_tweet_ids': ['1836417390394913222'], 'id': '1836417390394913222'}, includes={}, errors=[], meta={})
2024-09-18 14:49:59,689 - INFO - Sleeping for 6 minutes and 53 seconds.
2024-09-18 14:56:52,689 - INFO - Saved posted URL: https://www.cnet.com/tech/services-and-software/death-clock-app-uses-ai-to-predict-your-death-date/#ftag=CAD590a51e at 2024-09-18 14:56:52
2024-09-18 14:56:52,689 - INFO - Input content - content="Artificial intelligence tool Midjourney can help you create custom, themed wedding invitations on the cheap. Here's how they turned out for me."
2024-09-18 14:56:52,689 - INFO - clean content - clean_content="Artificial intelligence tool Midjourney can help you create custom, themed wedding invitations on the cheap. Here's how they turned out for me."
2024-09-18 14:56:52,689 - INFO - Generating summary with BART model
2024-09-18 14:56:56,157 - INFO - Generating summary with BART model
2024-09-18 14:56:59,591 - INFO - full_tweet='Artificial intelligence tool Midjourney can help you create custom, themed wedding invitations on the..[read moreüëáüèº] #intelligence #invitations https://www.cnet.com/tech/services-and-software/i-tried-using-ai-to-make-custom-invitations-heres-what-went-right-and-wrong/#ftag=CAD590a51e'
2024-09-18 14:56:59,713 - ERROR - Failed to post tweet: 429 Too Many Requests
Too Many Requests

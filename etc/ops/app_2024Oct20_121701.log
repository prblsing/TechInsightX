2024-10-20 12:17:01,204 - INFO - Starting configuration setup
2024-10-20 12:17:01,204 - INFO - Environment variables loaded successfully
2024-10-20 12:17:01,205 - INFO - Twitter API client initialized successfully
2024-10-20 12:17:03,114 - INFO - Initializing summarization model: facebook/bart-large-cnn
2024-10-20 12:17:10,877 - INFO - Summarization model initialized successfully
2024-10-20 12:17:10,877 - INFO - Starting tweet scheduling
2024-10-20 12:17:10,877 - INFO - Tweeting process started at 2024-10-20 12:17:10.877409!
2024-10-20 12:17:10,877 - INFO - Loaded posted URLs from /home/runner/work/TechInsightX/TechInsightX/src/../etc/ops/posted_links_2024Oct20.csv
2024-10-20 12:17:10,877 - INFO - Fetching latest tech news from RSS feeds
2024-10-20 12:17:25,854 - INFO - Total entries found: 615
2024-10-20 12:17:25,857 - INFO - Recent AI-related entries found: 15
2024-10-20 12:17:25,857 - INFO - Input content - content='<h4>Not because we’re curious. Because we need to get shit\xa0done.</h4><p>Are explanations important to AI model outputs important?</p><p>My first answer to this is: <strong>not\xa0really</strong>.</p><p>When an explanation is a rhetorical exercise to impress me you had your reasons for a decision, it’s just bells and whistles with no impact. If I’m waiting for a cancer diagnosis based on my MRI, I’m much more interested in improving accuracy from 80% to 99% than in seeing a compelling image showing where the evidence lies. It may take a highly trained expert to recognize the evidence, or the evidence might be too diffuse, spread across millions of pixels, for a human to comprehend. Chasing explanations just to feel good about trusting the AI is pointless. We should measure correctness, and if the math shows the results are reliable, explanations are unnecessary.</p><p>But, sometimes an explanation are more than a rhetorical exercise. Here’s when explanations matter:</p><ol><li>When accuracy is crucial, and the explanation lets us bring down the error levels, e.g. from 1% to\xa00.01%.</li><li>When the raw prediction isn’t really all you care about. The explanation generates useful actions. For example, saying “somewhere in this contract there’s an unfair clause”, isn’t useful as showing exactly where this unfair clause shows up, because we can take action and propose an edit to the contract.</li></ol><h3>When the explanation is more important than the\xa0answer</h3><p>Let’s double click on a concrete example from <a href="https://www.docupanda.io/">DocuPanda</a>, a service I’ve cofounded. In a nutshell, what we do is let users map complex documents into a JSON payload that contains a consistent, correct\xa0output</p><p>So maybe we scan an entire rental lease, and emit a short JSON: {“monthlyRentAmount”: 2000, “dogsAllowed”\xa0:\xa0true}.</p><p>To make it very concrete, <a href="https://docupanda-marketing-assets.s3.amazonaws.com/lease2.pdf">here’s all 51 pages of my lease</a> from my time in Berkeley, California.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DgZovohDnYupZ_KfjSEb2g.png" /><figcaption>Yeah, rent in Bay Area is insane, thanks for\xa0asking</figcaption></figure><p>If you’re not from the US, you might be shocked it takes 51 pages to spell out “You’re gonna pay $3700 a month, you get to live here in exchange”. I think it might not be necessary <a href="https://scholarship.law.tamu.edu/facscholar/302/">legally</a>, but I\xa0digress.</p><p>Now, using Docupanda, we can get to bottom line answers like\u200a—\u200awhat’s the rental amount, and can I take my dog to live there, what’s the start date,\xa0etc.</p><p>Let’s take a look at the JSON we\xa0extract</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Vamp-5IGGVLuDNly9kvKSw.png" /><figcaption>So apparently Roxy can’t come live with\xa0me</figcaption></figure><p>If you look all the way at the bottom, we have a flag to indicate that pets are disallowed, along with a description of the exception spelled out in the\xa0lease.</p><p>There are two reasons explainability would be awesome\xa0here:</p><ol><li>Maybe it’s crucial that we get this right. By reviewing the paragraph I can make sure that we understand the policy correctly.</li><li>Maybe I want to propose an edit. Just knowing that somewhere in these 51 pages there’s a pet prohibition doesn’t really help\u200a—\u200aI’ll still have to go over all pages to propose an\xa0edit.</li></ol><p>So here’s how we solve for this. Rather than just giving you a black box with a dollar amount, a true/false, etc\u200a—\u200awe’ve designed DocuPanda to ground its prediction in precise pixels. You can click on a result, and scroll to the exact page and section that justifies our prediction.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RbFXHTTm0dYXR0eHqPuzLw.gif" /><figcaption>Clicking on “pets allowed = false” immediately scrolls to the relevant page where it says “no mammal pets\xa0etc”</figcaption></figure><h3>Explanation-Driven Workflows</h3><p>At DocuPanda, we’ve observed three overall paradigms for how explainability is\xa0used.</p><h4>Explanations Drive\xa0Accuracy</h4><p>The first paradigm we predicted from the outset is that explainability can reduce errors and validate predictions. When you have an invoice for $12,000, you really want a human to ensure the number is valid and not taken out of context, because the stakes are too high if this figure feeds into accounting automation software.</p><p>The thing about document processing, though, is that we humans are exceptionally good at it. In fact, nearly 100% of document processing is still handled by humans today. As large language models become more capable and their adoption increases, that percentage will decrease\u200a—\u200abut we can still rely heavily on humans to correct AI predictions and benefit from more powerful and focused\xa0reading.</p><h4>Explanations drive high-knowledge worker productivity</h4><p>This paradigm arose naturally from our user base, and we didn’t entirely anticipate it at first. Sometimes, more than we want the raw answer to a question, we want to leverage AI to get the right information in front of our\xa0eyes.</p><p>As an example, consider a bio research company that wants to scour every biological publication to identify processes that increase sugar production in potatoes. They use DocuPanda to answer fields\xa0like:</p><p>{sugarProductionLowered: true, sugarProductionGenes: [“AP2a”,”TAGL1&quot;]}</p><p>Their goal is <strong>not </strong>to blindly trust DocuPanda and count how many papers mention a gene or something like that. The thing that makes this result useful is that researcher can click around to get right to the gist of the paper. By clicking on the gene names, a researcher can immediately jump in to context where the gene got mentioned\u200a—\u200aand reason about whether the paper is relevant. This is an example where the explanation is more important than the raw answer, and can boost the productivity of very high knowledge workers.</p><h4>Explanations for liability purposes</h4><p>There’s another reason to use explanations and leverage them to put a human in the loop. In addition to reducing error rates (often), they let you demonstrate that you have a <strong>reasonable, legally compliant process</strong> in\xa0place.</p><p>Regulators care about process. A black box that emits mistakes is not a sound process. The ability to trace every extracted data point back to the original source lets you put a human in the loop to review and approve results. Even if the human doesn’t reduce errors, having that person involved can be legally useful. It shifts the process from being blind automation, for which your company is responsible, to one driven by humans, who have an acceptable rate of clerical errors. A related example is that it looks like regulators and public opinion tolerate a far lower rate of fatal car crashes, measured per-mile, when discussing a fully automated system, vs human driving-assistance tools. I personally find this to be morally unjustifiable, but I don’t make the rules, and we have to play by\xa0them.</p><p>By giving you the ability to put a human in the loop, you move from a legally tricky minefield of full automation, with the legal exposure it entails, to the more familiar legal territory of a human analyst using a 10x speed and productivity tool (and making occasional mistakes like the rest of us sinners).</p><blockquote>all images are owned by the\xa0author</blockquote><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=840144df418e" width="1" /><hr /><p><a href="https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e">Why Explainability Matters in AI</a> was originally published in <a href="https://towardsdatascience.com">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>'
2024-10-20 12:17:25,860 - INFO - clean content - clean_content='Not because we’re curious. Because we need to get **** done.Are explanations important to AI model outputs important?My first answer to this is: not really.When an explanation is a rhetorical exercise to impress me you had your reasons for a decision, it’s just bells and whistles with no impact. If I’m waiting for a cancer diagnosis based on my MRI, I’m much more interested in improving accuracy from 80% to 99% than in seeing a compelling image showing where the evidence lies. It may take a highly trained expert to recognize the evidence, or the evidence might be too diffuse, spread across millions of pixels, for a human to comprehend. Chasing explanations just to feel good about trusting the AI is pointless. We should measure correctness, and if the math shows the results are reliable, explanations are unnecessary.But, sometimes an explanation are more than a rhetorical exercise. Here’s when explanations matter:When accuracy is crucial, and the explanation lets us bring down the error levels, e.g. from 1% to 0.01%.When the raw prediction isn’t really all you care about. The explanation generates useful actions. For example, saying “somewhere in this contract there’s an unfair clause”, isn’t useful as showing exactly where this unfair clause shows up, because we can take action and propose an edit to the contract.When the explanation is more important than the answerLet’s double click on a concrete example from DocuPanda, a service I’ve cofounded. In a nutshell, what we do is let users map complex documents into a JSON payload that contains a consistent, correct outputSo maybe we scan an entire rental lease, and emit a short JSON: {“monthlyRentAmount”: 2000, “dogsAllowed” : true}.To make it very concrete, here’s all 51 pages of my lease from my time in Berkeley, California.Yeah, rent in Bay Area is insane, thanks for askingIf you’re not from the US, you might be shocked it takes 51 pages to spell out “You’re gonna pay $3700 a month, you get to live here in exchange”. I think it might not be necessary legally, but I digress.Now, using Docupanda, we can get to bottom line answers like — what’s the rental amount, and can I take my dog to live there, what’s the start date, etc.Let’s take a look at the JSON we extractSo apparently Roxy can’t come live with meIf you look all the way at the bottom, we have a flag to indicate that pets are disallowed, along with a description of the exception spelled out in the lease.There are two reasons explainability would be awesome here:Maybe it’s crucial that we get this right. By reviewing the paragraph I can make sure that we understand the policy correctly.Maybe I want to propose an edit. Just knowing that somewhere in these 51 pages there’s a pet prohibition doesn’t really help — I’ll still have to go over all pages to propose an edit.So here’s how we solve for this. Rather than just giving you a black box with a dollar amount, a true/false, etc — we’ve designed DocuPanda to ground its prediction in precise pixels. You can click on a result, and scroll to the exact page and section that justifies our prediction.Clicking on “pets allowed = false” immediately scrolls to the relevant page where it says “no mammal pets etc”Explanation-Driven WorkflowsAt DocuPanda, we’ve observed three overall paradigms for how explainability is used.Explanations Drive AccuracyThe first paradigm we predicted from the outset is that explainability can reduce errors and validate predictions. When you have an invoice for $12,000, you really want a human to ensure the number is valid and not taken out of context, because the stakes are too high if this figure feeds into accounting automation software.The thing about document processing, though, is that we humans are exceptionally good at it. In fact, nearly 100% of document processing is still handled by humans today. As large language models become more capable and their adoption increases, that percentage will decrease — but we can still rely heavily on humans to correct AI predictions and benefit from more powerful and focused reading.Explanations drive high-knowledge worker productivityThis paradigm arose naturally from our user base, and we didn’t entirely anticipate it at first. Sometimes, more than we want the raw answer to a question, we want to leverage AI to get the right information in front of our eyes.As an example, consider a bio research company that wants to scour every biological publication to identify processes that increase sugar production in potatoes. They use DocuPanda to answer fields like:{sugarProductionLowered: true, sugarProductionGenes: [“AP2a”,”TAGL1"]}Their goal is not to blindly trust DocuPanda and count how many papers mention a gene or something like that. The thing that makes this result useful is that researcher can click around to get right to the gist of the paper. By clicking on the gene names, a researcher can immediately jump in to context where the gene got mentioned — and reason about whether the paper is relevant. This is an example where the explanation is more important than the raw answer, and can boost the productivity of very high knowledge workers.Explanations for liability purposesThere’s another reason to use explanations and leverage them to put a human in the loop. In addition to reducing error rates (often), they let you demonstrate that you have a reasonable, legally compliant process in place.Regulators care about process. A black box that emits mistakes is not a sound process. The ability to trace every extracted data point back to the original source lets you put a human in the loop to review and approve results. Even if the human doesn’t reduce errors, having that person involved can be legally useful. It shifts the process from being blind automation, for which your company is responsible, to one driven by humans, who have an acceptable rate of clerical errors. A related example is that it looks like regulators and public opinion tolerate a far lower rate of fatal car crashes, measured per-mile, when discussing a fully automated system, vs human driving-assistance tools. I personally find this to be morally unjustifiable, but I don’t make the rules, and we have to play by them.By giving you the ability to put a human in the loop, you move from a legally tricky minefield of full automation, with the legal exposure it entails, to the more familiar legal territory of a human analyst using a 10x speed and productivity tool (and making occasional mistakes like the rest of us sinners).all images are owned by the authorWhy Explainability Matters in AI was originally published in Towards Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.'
2024-10-20 12:17:25,860 - INFO - Generating summary with BART model
2024-10-20 12:17:34,647 - INFO - Generating summary with BART model
2024-10-20 12:17:42,763 - INFO - full_tweet='When an explanation is a rhetorical exercise to impress me you had your reasons for a decision, it’s just..[read more👇🏼] #AI #just https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e?source=rss----7f60cf5620c9---4'
2024-10-20 12:17:43,074 - INFO - Tweet posted successfully: Response(data={'id': '1847975480319705186', 'edit_history_tweet_ids': ['1847975480319705186'], 'text': 'When an explanation is a rhetorical exercise to impress me you had your reasons for a decision, it’s just..[read more👇🏼] #AI #just https://t.co/h0IcMRPB6b'}, includes={}, errors=[], meta={})
2024-10-20 12:17:43,074 - INFO - Saved posted URL: https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e?source=rss----7f60cf5620c9---4 at 2024-10-20 12:17:43
2024-10-20 12:17:43,074 - INFO - Tweet link saved successfully.
2024-10-20 12:17:43,074 - INFO - Sleeping for 8 minutes and 20 seconds.
2024-10-20 12:26:03,074 - INFO - This link is already posted: https://www.techradar.com/computing/artificial-intelligence/ai-marketing-is-a-con-especially-when-it-comes-to-cpus
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1g7mjgs/endorsement_for_a_ai_paper_in_arvix_r/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://www.techradar.com/pro/another-big-win-for-amd-as-lenovo-adds-epyc-9005-and-instinct-mi325x-to-its-thinksystem-server-platform-boosting-ai-capabilities
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://techcrunch.com/2024/10/19/former-openai-cto-mira-murati-is-reportedly-fundraising-for-a-new-ai-startup/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://www.pcmag.com/news/major-publisher-penguin-random-house-blocks-ai-training-on-its-books
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1g7ghwx/news_aaai_2025_workshop_on_ai_for_music/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://techcrunch.com/2024/10/19/four-takeaways-from-pony-ais-ipo-filing/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://techcrunch.com/2024/10/19/penguin-random-house-is-adding-an-ai-warning-to-its-books-copyright-pages/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1g7f0qj/discussion_sophon_ai_accelerator_accuracy_problem/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://techcrunch.com/2024/10/19/midjourney-plans-to-let-anyone-on-the-web-edit-images-with-ai/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://techcrunch.com/2024/10/19/women-in-ai-dr-rebecca-portnoff-is-protecting-children-from-harmful-deepfakes/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1g7bhb0/d_resources_for_a_course_on_neurosymbolic_ai/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://www.reddit.com/r/MachineLearning/comments/1g7barf/p_i_built_a_web_app_to_track_trending_ai_papers/
2024-10-20 12:26:03,075 - INFO - This link is already posted: https://techcrunch.com/2024/10/19/claude-everything-you-need-to-know-about-anthropics-ai/
